,"Yearly revenue (millions, USD)",Vendor headquarters,Device Description,Device,Field 1,Vendor wikipedia or cruncbase description,Library,Vendor,Vendor logo,Vendor website,ChatGPT prompt,Category description link,Device Category,Category Description,Corrected device name,Device datasheet (PDF),Device picture,Device Description Link,GitHub link to Python driver (NOT LINK TO DOCS ON GITHUB),Python docs link,Device Price,ChatGPT code,Obsolete,Notes,docstring
18,7.0,Germany,"The pco.pixelfly™ 1.3 SWIR is a high-performance machine vision camera due to its special InGaAs image sensor which is sensitive in the shortwave infrared, near infrared and visible range of the electromagnetic spectrum.
",PCO,,"**PCO** is one of the leading manufacturers of scientific **cameras**: sCMOS & Highspeed **camera** systems, developed and produced in Kelheim Bavaria Germany.

",Instrumental,PCO,"[OrderedDict([('id', 'attxVqZoiiRf18FSt'), ('width', 800), ('height', 235), ('url', 'https://v5.airtableusercontent.com/v1/18/18/1689537600000/-O8MDLWvJvPjaBTJh_cc5Q/ijCfR40Ze2km-SOxmq3VcVVr-hjy4kkKJ4t5qe7YzkIkMd8Gn17zUnOIf-lMQVQFcMkW00y5WlG6RnG4gE9b0Fw5Q6z9qdxPusyGp5teiNk/nzky1avNfIpmjUyLqDUhLOfjAHCtYVbPXV-WkaPtZXM'), ('filename', '800px-Pco_logo.png'), ('size', 77223), ('type', 'image/png'), ('thumbnails', OrderedDict([('small', OrderedDict([('url', 'https://v5.airtableusercontent.com/v1/18/18/1689537600000/XU6O5wmXvAVBINpZUZfwpg/05z1RSZMXnNXucoUcm5qT8LphOjCMNIhowNpOyU8CRHVye7e_4CRicioajsHhjUoDbRVmonvMXv1BkWgGlZ4Hg/7nn2kOS8apHvrsdqGyW7VDlEaN0ZQ3V-1Sn0JKeut98'), ('width', 123), ('height', 36)])), ('large', OrderedDict([('url', 'https://v5.airtableusercontent.com/v1/18/18/1689537600000/oyFRlY5oGgUhfGyGFYUrwQ/HzUoZVdeb9TcAsBvBjqlGwJFA0_3OC6KqUtU4gpPcmbCWkybL_ymakpZOuYxQBgnB8R_hMnMr_zIpMzmeYiZ9A/qJK9f7Rm1CKXrB8u8l6G7bduMjyG4LW7c6LvRkJiZwU'), ('width', 800), ('height', 235)])), ('full', OrderedDict([('url', 'https://v5.airtableusercontent.com/v1/18/18/1689537600000/lHWvxt47EtbkjplPYWJPSw/rP_ria2WhSdoAP7Sbuo8X5RhgCYXgmGvoNYIJiLPw4z0nIzadAVvS5D6ExSEyEREeYB1IYrhm9sa2e519kRfOg/Pi55BJJkqt_XjnkVhmzFLY6Tp7I6z0gEzAAYukIaLBc'), ('width', 3000), ('height', 3000)]))]))])]",https://www.pco-tech.com,Write a Python script that uses Instrumental to connect to a PCO Pixelfly Cameras,https://en.wikipedia.org/wiki/Machine_vision,['Cameras'],"Machine vision (MV) is the technology and methods used to provide imaging-based automatic inspection and analysis for such applications as automatic inspection, process control, and robot guidance, usually in industry",PCO Pixelfly,https://www.pco.de/fileadmin/user_upload/pco-product_sheets/FL_PCOPIXELFLY13SWIR_V101.pdf,"[OrderedDict([('id', 'att8fWvmDAnaGMGMR'), ('width', 60), ('height', 60), ('url', 'https://v5.airtableusercontent.com/v1/18/18/1689537600000/hYrs0S1KYd3Zs-TCTRn0vQ/sU3LlKMA6OMfR1zi-Q7GJjBxjskOT2LJCL-nEYZhPiZpaWoHpuNfIviu0W5ur9TpUwLqk4rs4oxJ8_Ewsc9qINunppFV2e6Ve0pN9K60FWc/lI3OJTMcTOxITFemT6C9LDWa7OgNpzgWuSYoHgxGX1E'), ('filename', '29533-2571181.webp'), ('size', 516), ('type', 'image/webp'), ('thumbnails', OrderedDict([('small', OrderedDict([('url', 'https://v5.airtableusercontent.com/v1/18/18/1689537600000/Wiqt1gWBDnOy9jPoJw1plg/Sgyprk1tp2sEaa809gRSaclltq60M8ldsD87luiQ_HkkU_hPX86ZX-yBpB_WPMg5YZiwqUwdpM7dqXlNdua9n8C4O5yZQGaEuDtAiEZGjD4/iOQC-rOFGdIuBuX5qbKKqVgtidiVvdO6xnsq7sjxMLQ'), ('width', 36), ('height', 36)])), ('large', OrderedDict([('url', 'https://v5.airtableusercontent.com/v1/18/18/1689537600000/UYLvelEOJ28t-hpWe3irwA/Sf4Sb0gVJgRdYboPJDNMgHTe5dSy31nj7LtmWdQP8-Etn-5LFXmoCcjdapfmozQho6iA4qLxIOuvPbhe1m_T2_q6CgW1RCy7bRRDLdNIkrA/BcWl9WkKJNuzuRogsH0qWgFCxaiMZygFBuR6hWs_jxU'), ('width', 60), ('height', 60)])), ('full', OrderedDict([('url', 'https://v5.airtableusercontent.com/v1/18/18/1689537600000/ZW0Tmq_xdNbWaQBJIsd3Qw/qunyXCEDyE6Sq082EbZ86spWxA5izmI8uDwVEBg5jNG_RLX_VJWw4FJw9VtpRqT6gvI5Mu11RDC1JUzsY1YLL0qBgjb3MiuFtt2mSeuZdGM/10pYvopJNDXagaaB5K1PrHNojO_08wl6zyaLjDI9WsI'), ('width', 3000), ('height', 3000)]))]))])]",https://www.pco.de/scientific-cameras/pcopixelflytm-13-swir/,https://github.com/patapisp/PCO_PixelFly/blob/master/pco_app.py,https://instrumental-lib.readthedocs.io/en/0.7/cameras-pixelfly.html,,,,,"__author__ = 'Polychronis Patapis'
from PyQt4 import QtGui
from core.pco_gui import CameraWidget
import sys

if __name__ == ""__main__"":
    app = QtGui.QApplication(sys.argv)
    window = QtGui.QMainWindow()
    window.setWindowTitle('PCO.PixelFly                    -ETH Zurich- ')
    try:
        icon = QtGui.QIcon('App.ico')
        window.setWindowIcon(icon)
    except:
        pass
    pco_ui = CameraWidget(parent=None)
    pco_ui.create_gui(window)
    window.show()
    sys.exit(app.exec_())

"
26,5420.0,USA,"Keysight 81110A
Pulse Pattern Generator / 165/330 MHz",Keysight 81110 A,,"Keysight Technologies, or Keysight, is an American company that manufactures electronics test and measurement equipment and software

",Instrumental,Agilent,"[OrderedDict([('id', 'attc6xrG2EsYbFAYl'), ('width', 212), ('height', 75), ('url', 'https://v5.airtableusercontent.com/v1/18/18/1689537600000/fQUxMyvye7LkRNrW_9LpDw/a1a72T_ybNDngFNeEthdujK-3J411WCU7p5oSDUdZQKBnyItPISoB5kGTdXbBoZHMWn8OVlfLkqUORBw0cWhS3FAreUOnbjBjTSb7AkMQD0/aWKCcUbxs_atz78f_o0gyPdiOT19I4BOaetlO7zRc7s'), ('filename', 'keysight-logo.svg'), ('size', 2285), ('type', 'image/svg+xml'), ('thumbnails', OrderedDict([('small', OrderedDict([('url', 'https://v5.airtableusercontent.com/v1/18/18/1689537600000/JMLOejSs0sNQqebza0_O4g/ptnoLe_aT143RHRw7XWGMF2Rbl_oJh18VP8XlO3pEzBdEpOleXMyRBWqTWt9h3Cvnv1NVgGzBSewM34fGtz6_G2zfORynLBJVLQttq4CERA/6RykqSNQV6z5GqlyNFAHFATyDxsZtr_dkrebBGCHQqo'), ('width', 102), ('height', 36)])), ('large', OrderedDict([('url', 'https://v5.airtableusercontent.com/v1/18/18/1689537600000/55V5uSdPbh0zNz0Drmxo7A/nDDnQrgWyg3uGnPFSTEbFZFZg8lh6PmAa0gqcuhRBMy8ux1D1qEeynNZZfXB5RCYzdvd11mgkZDaWWL4zMTDdVK3jm-U_9HlUEEdEIRTjf8/-82nBzlZVRi4Izj7vfypYX3HzbD-Usw5_Owv_MfRxgU'), ('width', 212), ('height', 75)])), ('full', OrderedDict([('url', 'https://v5.airtableusercontent.com/v1/18/18/1689537600000/RWWIIc79xTTZegoK7wkprg/VCK9-LQrz8PtqcOTSmaZiNVa5dpQ2eq5sIELnqJMdDY0-qksJXtd4reH5o2nSEl2SeAVkYTwBrC4ZfMl2h_DMHsN8FrQO1wuGgXs-cNLh60/mZHEAeKa58qt39N5aX3_5o_zlOkrLoir5OMzhvImp8k'), ('width', 3000), ('height', 3000)]))]))])]",https://www.keysight.com/us/en/home.html,Write a Python script that uses Instrumental to connect to a KEYSIGHT 81110A Function Generators,https://saving.em.keysight.com/en/used/function-aw-signal-generators,['Function Generators'],"Signal generators are routinely used when designing, manufacturing, testing, troubleshooting, servicing, calibrating or repairing electronic devices. Depending on the frequency range of the signal generator and electronic device, this may range from mechanical- and electro-accoustic applications to high and ultra-high frequencies in the digital, wireless, radar spectrum",KEYSIGHT 81110A,https://www.keysight.com/us/en/assets/3121-1445/data-sheets-archived/5980-1215.pdf,"[OrderedDict([('id', 'attXDU3MLQIOvGG3t'), ('width', 5472), ('height', 3648), ('url', 'https://v5.airtableusercontent.com/v1/18/18/1689537600000/01PCMk7VZbBfnegR3bEfrg/tqMfduB5wWg0CAwsJhCWLgGMJ1xXyFezGTgiZ40vKDkCmbdFr68k6EK90BdrwjRRLRJ0Bj-R743pClA6iRVEj_fhp-obVAc1Fm2n1PcmO_0/WGzIbHz3fLdQf7jXs4Mn4Sg534t5EpWN8j2slc6nexA'), ('filename', '81110.jpeg'), ('size', 2392802), ('type', 'image/jpeg'), ('thumbnails', OrderedDict([('small', OrderedDict([('url', 'https://v5.airtableusercontent.com/v1/18/18/1689537600000/IQqNwO1smvmw1aR-HbEoug/giWv3i0ow-hhOpc4sjr9NaRScwQIB8O4_HFD_zkcu8UqErMQ8oNmiunHIrGngu64qjUQOqG-QsuyRRE3nBLCyA/o9HlHmmHVtSz74jxZjPoDBW5OeLmud4n3RO67c5SNu4'), ('width', 54), ('height', 36)])), ('large', OrderedDict([('url', 'https://v5.airtableusercontent.com/v1/18/18/1689537600000/EhYGVez-J_6lZnrHhDw7ng/ajQHsKizhhk8aFR4jCakIz_OQLOIj4jRR1UZ84-1zmy2n2H2b85gcfmypberCVmlG1C5nwnj5vrnqf7FMj-4gA/N0-1CvN7ZcszYPxW0snqiwMzuJUiv7LJG3oPSlbZSBQ'), ('width', 768), ('height', 512)])), ('full', OrderedDict([('url', 'https://v5.airtableusercontent.com/v1/18/18/1689537600000/jVhWkVK5m-RYOZCZLCFazA/cvmQnplZ0CYBzkSaxSL-eRzLmQKp4EvdcPoD8bb_rVGpLMsW7wqGAceTR2CiBiYxf9Wa5sJQ3efAv670MGvljg/R8IAnQCu1mO9SKqY87WY8nRK0d1UnmHbih2Lc1xvpK8'), ('width', 3000), ('height', 3000)]))]))])]",https://saving.em.keysight.com/en/used/function-aw-signal-generators/81110a-e2504113?channel=CP_DSA_MLP&gclid=Cj0KCQjw7uSkBhDGARIsAMCZNJswNSak-h4YCvdtOEGCPo8AxKoSOofAXGHH6SjIfDn3NmD_VvY5EEUaAkOGEALw_wcB,https://github.com/heeres/qtlab/blob/master/instrument_plugins/HP_81110A.py,,6500.0,"
",,,"# HP_81110A.py class, to perform the communication between the Wrapper and the device
# Pieter de Groot <pieterdegroot@gmail.com>, 2008
# Martijn Schaafsma <qtlab@mcschaafsma.nl>, 2008
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA  02110-1301  USA

from instrument import Instrument
import visa
import types
import logging
import time

class HP_81110A(Instrument):
    '''
    This is the python driver for the HP 81110A
    pulse generator
    Also works with the Agilent 81130A, the former HP 81130A.

    Usage:
    Initialize with
    <name> = instruments.create('<name>', 'HP_81110A', address='<GPIB address>',
        reset=<bool>)
    '''

    def __init__(self, name, address, reset=False):
        '''
        Initializes the HP_81110A, and communicates with the wrapper.

        Input:
            name (string)    : name of the instrument
            address (string) : GPIB address
            reset (bool)     : resets to default values, default=false

        Output:
            None
        '''

        Instrument.__init__(self, name, tags=['physical'])

        self._address = address
        self._visainstrument = visa.instrument(self._address)
        self._channels = self._get_number_of_channels()

        self.add_parameter('delay', type=types.FloatType,
            flags=Instrument.FLAG_GETSET | Instrument.FLAG_GET_AFTER_SET,
            channels=(1, self._channels), minval=0.0, maxval=999, units='sec',channel_prefix='ch%d_')
        self.add_parameter('width', type=types.FloatType,
            flags=Instrument.FLAG_GETSET | Instrument.FLAG_GET_AFTER_SET,
            channels=(1, self._channels), minval=-6.25e-9, maxval=999.5, units='sec',channel_prefix='ch%d_')
        self.add_parameter('high', type=types.FloatType,
            flags=Instrument.FLAG_GETSET | Instrument.FLAG_GET_AFTER_SET,
            channels=(1, self._channels), minval=-9.90, maxval=10.0, units='Volts',channel_prefix='ch%d_')
        self.add_parameter('low', type=types.FloatType,
            flags=Instrument.FLAG_GETSET | Instrument.FLAG_GET_AFTER_SET,
            channels=(1, self._channels), minval=-10.0, maxval=9.90, units='Volts',channel_prefix='ch%d_')
        self.add_parameter('status', type=types.StringType, channels=(1, self._channels),
            flags=Instrument.FLAG_GETSET | Instrument.FLAG_GET_AFTER_SET,channel_prefix='ch%d_')
        self.add_parameter('display', type=types.StringType,
            flags=Instrument.FLAG_GETSET | Instrument.FLAG_GET_AFTER_SET)

        self.add_function('reset')
        self.add_function('get_all')
        self.add_function('set_mode_triggered')
        self.add_function('set_mode_continuous')

        if reset:
            self.reset()
        else:
            self.get_all()



    def reset(self):
        '''
        Resets the instrument to default values

        Input:
            None

        Output:
            None
        '''
        logging.info(__name__ + ' : Resetting instrument')
        self._visainstrument.write('*RST')

        """""" Fix: The Agilent 81130A is kinda slow after a reset. """"""
        time.sleep(2)

        self.get_all()

    def get_all(self):
        '''
        Reads all implemented parameters from the instrument,
        and updates the wrapper.

        Input:
            None

        Output:
            None
        '''
        logging.info(__name__ + ' : reading all settings from instrument')

        for i in range(1,self._channels+1):
            self.get('ch%d_delay' % i)
            self.get('ch%d_width' % i)
            self.get('ch%d_low' % i)
            self.get('ch%d_high' % i)
            self.get('ch%d_status' % i)

        self.get_display()

    # communication with device
    def do_get_delay(self, channel):
        '''
        Reads the pulse delay from the specified channel

        Input:
            channel (int) : 1 or 2, the number of the designated channel

        Output:
            delay (float) : delay in seconds
        '''
        logging.debug(__name__ + ' : get delay for channel %d' % channel)
        return float(self._visainstrument.ask(':PULS:DEL' + str(channel) + ""?""))

    def do_set_delay(self, val, channel):
        '''
        Sets the delay of the pulse of the specified channel

        Input:
            val (float)   : delay in seconds
            channel (int) : 1 or 2, the number of the designated channel

        Output:
            None
        '''
        logging.debug(__name__ + ' : set delay for channel %d to %f' % (channel, val))
        self._visainstrument.write(':PULS:DEL' + str(channel) + "" "" + str(val) + ""S"")

    def do_get_width(self, channel):
        '''
        Reads the pulse width from the device for the specified channel

        Input:
            channel (int) : 1 or 2, the number of the designated channel

        Output:
            width (float) : width in seconds
        '''
        logging.debug(__name__ + ' : get width for channel %d' % channel)
        return float(self._visainstrument.ask(':PULS:WIDT' + str(channel) + ""?""))

    def do_set_width(self, val, channel):
        '''
        Sets the width of the pulse of the specified channel

        Input:
            val (float)   : width in seconds
            channel (int) : 1 or 2, the number of the designated channel

        Output:
            None
        '''
        logging.debug(__name__ + ' : set width for channel %d to %f' % (channel, val))
        self._visainstrument.write(':PULS:WIDT' + str(channel) + "" "" + str(val) + ""S"")

    def do_get_high(self, channel):
        '''
        Reads the upper value from the device for the specified channel

        Input:
            channel (int) : 1 or 2, the number of the designated channel

        Output:
            val (float) : upper bound in Volts
        '''
        logging.debug(__name__ + ' : get high for channel %d' % channel)
        return float(self._visainstrument.ask(':VOLT' + str(channel) + ':HIGH?'))

    def do_set_high(self, val, channel):
        '''
        Sets the upper value of the specified channel

        Input:
            val (float)   : high bound in Volts
            channel (int) : 1 or 2, the number of the designated channel

        Output:
            None
        '''
        logging.debug(__name__ + ' : set high for channel %d to %f' % (channel, val))
        self._visainstrument.write(':VOLT' + str(channel) + "":HIGH "" + str(val) + ""V"")

    def do_get_low(self, channel):
        '''
        Reads the lower value from the device for the specified channel

        Input:
            channel (int) : 1 or 2, the number of the designated channel

        Output:
            val (float) : lower bound in Volts
        '''
        logging.debug(__name__ + ' : get low for channel %d' % channel)
        return float(self._visainstrument.ask(':VOLT' + str(channel) + ':LOW?'))

    def do_set_low(self, val, channel):
        '''
        Sets the lower value of the specified channel

        Input:
            val (float)   : low bound in Volts
            channel (int) : 1 or 2, the number of the designated channel

        Output:
            None
        '''
        logging.debug(__name__ + ' : set low for channel %d to %f' % (channel, val))
        self._visainstrument.write(':VOLT' + str(channel) + "":LOW "" + str(val)        + ""V"")

    def do_get_status(self, channel):
        '''
        Reads the status from the device for the specified channel

        Input:
            channel (int) : 1 or 2, the number of the designated channel

        Output:
            status (string) : 'on' or 'off'
        '''
        logging.debug(__name__ + ' : getting status for channel %d' % channel)
        val = self._visainstrument.ask('OUTP' + str(channel) + '?')
        if (val=='1'):
            return 'on'
        elif (val=='0'):
            return 'off'
        return 'error'

    def do_set_status(self, val, channel):
        '''
        Sets the status of the specified channel

        Input:
            val (string)  : 'on' or 'off'
            channel (int) : 1 or 2, the number of the designated channel

        Output:
            None
        '''
        logging.debug(__name__ + ' : setting status for channel %d to %s' % (channel, val))
        if ((val.upper()=='ON') | (val.upper()=='OFF')):
            self._visainstrument.write('OUTP' + str(channel) + "" "" + val)
        else:
            logging.error('Try tot set OUTP to ' + str(val))

    def do_get_display(self):
        '''
        Reads the display status from the device

        Input:
            None

        Output:
            status (string) : 'on' or 'off'
        '''
        logging.debug(__name__ + ' : getting display status')
        val = self._visainstrument.ask('DISP?')
        if (val=='1'):
            return 'on'
        elif (val=='0'):
            return 'off'
        return 'error'

    def do_set_display(self, val):
        '''
        Sets the display status of the device

        Input:
            val (string) : 'on' or 'off'

        Output:
            None
        '''
        logging.debug(__name__ + ' : setting display status to %s' % val)
        if ((val.upper()=='ON') | (val.upper()=='OFF')):
            self._visainstrument.write('DISP ' + val)
        else:
            logging.error('Try tot set display to ' +val)

    def set_mode_triggered(self):
        '''
        Sets the instrument in 'triggered' mode

        Input:
            None

        Output:
            None
        '''
        logging.debug(__name__ + ' : setting instrument to triggered mode')
        self._visainstrument.write(':ARM:SOUR EXT')

    def set_mode_continuous(self):
        '''
        Sets the instrument in 'triggered' mode

        Input:
            None

        Output:
            None
        '''
        logging.debug(__name__ + ' : setting instrument to continuous mode')
        self._visainstrument.write(':ARM:SOUR IMM')

    def _get_number_of_channels(self):
        '''
        asks the device for the options installed and derives the number of channels
        Fixme: maybe there is a direct method to examine the number of channels.

        Input:
            None
        Output:
            Number of installed channels (int)

        '''
        opt = self._visainstrument.ask('*OPT?').split()
        return int(len(opt)-opt.count(0))
"
30,3500.0,"Irvine, California, United States",,Myfacet,,"Newport provides a wide range of photonics technology and products designed to enhance the capabilities and productivity of our customers' applications.
",Instrumental,Newport,"[OrderedDict([('id', 'attXej1yb0ZjvVVtv'), ('width', 119), ('height', 119), ('url', 'https://v5.airtableusercontent.com/v1/18/18/1689537600000/eJF3UbuXimfV1ihP9bpJbA/aUuQ3ySKJ3Ck7iVo_ttsIYwQYPIPPbW-ymksVpBwfMe-w5ld6x3Jh8N8iEcxVd2FJlnsRm4kZ8Z0AlnnXvdLsLjgt2lHWAUg5pGRgiWing0/cQ7xieJDgP0gXAdYteL8KICPVcN6GPgJHEIBNF93VoM'), ('filename', 'download (4).png'), ('size', 1554), ('type', 'image/png'), ('thumbnails', OrderedDict([('small', OrderedDict([('url', 'https://v5.airtableusercontent.com/v1/18/18/1689537600000/sUivBL_K7azt26iNB3Tshw/xhA4U9s8liX3z0U23n6TKINZ4QI7N9tsDVuO1SP0WhZc_oXoUBHeNsKyb7dZH8w-nsltXwRFd7PaocFJ-MdUcQ/EKtnn7Re9BcgDyKHCIBYI98vX1ZWnm1SKaAE33PmUXQ'), ('width', 36), ('height', 36)])), ('large', OrderedDict([('url', 'https://v5.airtableusercontent.com/v1/18/18/1689537600000/Ii7mKuM-BnbGEVA8WXszCg/hRwzRTc7Orc7Ckc-pcBWPqOG2cLXBR-VPFP40XU6keLSWkwjLcNVphU3uWS8R7GvT0DYir1pNTc1wydz5QeqbA/yOl2VlnroxlPhZM7jhR2Ndj749JLIlLNhHUsAwMzq78'), ('width', 119), ('height', 119)])), ('full', OrderedDict([('url', 'https://v5.airtableusercontent.com/v1/18/18/1689537600000/YAUQVRx7gEmmEhPpt9cbAQ/aTYL4VMJe9WxFwtJerL5-KZ6n1bszVY8CRHFuLc-7NDV2R770SqIBOLoVhDQHB1ELoEEkn2F9xJWqK6ok9DhPw/lJaUVpWFk-Sx7N9ATCoaKUz88h2gxXL3ClbGSaaq5Xc'), ('width', 3000), ('height', 3000)]))]))])]",https://www.newport.com/,Write a Python script that uses Instrumental to connect to a {Device name} Power Meters,,['Power Meters'],,,,,,,,,,,,
78,5420.0,USA,"Keysight 86100A
Infiniium DCA Wide-Bandwidth Oscilloscope / up to 50 GHz
","Agilent 86100A

",,"Keysight Technologies, or Keysight, is an American company that manufactures electronics test and measurement equipment and software

",Instrumental,Agilent,"[OrderedDict([('id', 'attc6xrG2EsYbFAYl'), ('width', 212), ('height', 75), ('url', 'https://v5.airtableusercontent.com/v1/18/18/1689537600000/fQUxMyvye7LkRNrW_9LpDw/a1a72T_ybNDngFNeEthdujK-3J411WCU7p5oSDUdZQKBnyItPISoB5kGTdXbBoZHMWn8OVlfLkqUORBw0cWhS3FAreUOnbjBjTSb7AkMQD0/aWKCcUbxs_atz78f_o0gyPdiOT19I4BOaetlO7zRc7s'), ('filename', 'keysight-logo.svg'), ('size', 2285), ('type', 'image/svg+xml'), ('thumbnails', OrderedDict([('small', OrderedDict([('url', 'https://v5.airtableusercontent.com/v1/18/18/1689537600000/JMLOejSs0sNQqebza0_O4g/ptnoLe_aT143RHRw7XWGMF2Rbl_oJh18VP8XlO3pEzBdEpOleXMyRBWqTWt9h3Cvnv1NVgGzBSewM34fGtz6_G2zfORynLBJVLQttq4CERA/6RykqSNQV6z5GqlyNFAHFATyDxsZtr_dkrebBGCHQqo'), ('width', 102), ('height', 36)])), ('large', OrderedDict([('url', 'https://v5.airtableusercontent.com/v1/18/18/1689537600000/55V5uSdPbh0zNz0Drmxo7A/nDDnQrgWyg3uGnPFSTEbFZFZg8lh6PmAa0gqcuhRBMy8ux1D1qEeynNZZfXB5RCYzdvd11mgkZDaWWL4zMTDdVK3jm-U_9HlUEEdEIRTjf8/-82nBzlZVRi4Izj7vfypYX3HzbD-Usw5_Owv_MfRxgU'), ('width', 212), ('height', 75)])), ('full', OrderedDict([('url', 'https://v5.airtableusercontent.com/v1/18/18/1689537600000/RWWIIc79xTTZegoK7wkprg/VCK9-LQrz8PtqcOTSmaZiNVa5dpQ2eq5sIELnqJMdDY0-qksJXtd4reH5o2nSEl2SeAVkYTwBrC4ZfMl2h_DMHsN8FrQO1wuGgXs-cNLh60/mZHEAeKa58qt39N5aX3_5o_zlOkrLoir5OMzhvImp8k'), ('width', 3000), ('height', 3000)]))]))])]",https://www.keysight.com/us/en/home.html,Write a Python script that uses Instrumental to connect to a KEYSIGHT 86100A Oscilloscopes,https://saving.em.keysight.com/en/used/digital-communication-analyzers/86100a-e2506142,['Oscilloscopes'],"Digital Communications Analyzers (DCA) are often modular instruments, also known as sampling oscilloscopes or equivalent-time sampling oscilloscopes",KEYSIGHT 86100A,https://www.keysight.com/us/en/assets/9018-05081/service-manuals/9018-05081.pdf,"[OrderedDict([('id', 'attaN02T4s8ifJYhU'), ('width', 5472), ('height', 3648), ('url', 'https://v5.airtableusercontent.com/v1/18/18/1689537600000/suqG07bFYlJr5XEIrr62Og/Db_Ds9tgXWE1adi5y4xCEy2_CNPcvKYxSE6a_0g8lEI8cFrkY0VM34jgiCqfaxXx3-Z9wp0byt_o-H62swIi17aUnh4qx-XVQJlnEpSi3JY/gMPSluE8WDARxkRnu48KpAVPmVnnQjBRCTlauxBpfQQ'), ('filename', '86100.jpeg'), ('size', 2851285), ('type', 'image/jpeg'), ('thumbnails', OrderedDict([('small', OrderedDict([('url', 'https://v5.airtableusercontent.com/v1/18/18/1689537600000/zxLCrDuZRxQoVp_Jax1whg/JsDqBAeKub3ztGtLWXFIivg02lTo767iubJKAkdBJUe0g1Td1qRPOHcgnPYOAWcPiXBXziQR_3CbhUmtze85TA/64KkzIyhBFeCtcoPJAUmGhfOwZn1T_KRHAPMy7WsdXg'), ('width', 54), ('height', 36)])), ('large', OrderedDict([('url', 'https://v5.airtableusercontent.com/v1/18/18/1689537600000/eKj0AXBnF565KAsQyFbhIg/IGPZriUwXVCucdQstB2fsU_vHOKL7FAEBSh2wmYxZHZWp0aPsCrcfoGBro9RvgV9TojNEmm6gQSF1gepPKd2jQ/n490ibCi-UMG0i6KpQ5Jz0ZGYO-xbYeCg7mRXvIoc2I'), ('width', 768), ('height', 512)])), ('full', OrderedDict([('url', 'https://v5.airtableusercontent.com/v1/18/18/1689537600000/CKMVcTHDG6a8Kw8A-CE4JQ/hSvWPml4XwoPq4QDOAInm_k_5XPc8BK1wqTd9yXJH3Ou1oC7KkkAKWeI_TesMWFEP447svE_dOuOhB7uEWvF9Q/9hycDjIOnU9lTVjA_vI3Zi-qu8KIQm-6JDp2yVjI46w'), ('width', 3000), ('height', 3000)]))]))])]",https://saving.em.keysight.com/en/used/digital-communication-analyzers/86100a-e2506142,,,4200.0,"
",,,
80,23.0,"Beijing, China","The 1000 Series from RIGOL include the B, D, and E series oscilloscopes. The E Series are the value leader with 50-100 MHz models that include 2 channels and 1 Million points of memory. The D Series add low speed digital capture enabling basic mixed signal analysis in a economic package. The B Series provide more speed and power including our economic 4 channel, 200 MHz DS1204B model which provides 2 GSa/sec sampling. With features including FFTs, record and replay, roll mode, alternate trigger mode, and adjustable trigger sensitivity the 1000 Series is a great entry for value oscilloscope requirements.",Rigol,,"RIGOL Technologies, Inc. specializes in development and production of test and measuring equipment and is one of the fastest growing Chinese companies in this sphere.
RIGOL’s line of products includes [digital storage oscilloscopes](https://www.tmatlantic.com/e-store/index.php?SECTION_ID=227), [function/arbitrary waveform generators](https://www.tmatlantic.com/e-store/index.php?SECTION_ID=230), [digital multimeters](https://www.tmatlantic.com/e-store/index.php?SECTION_ID=233), PC-based devices compatible with LXI standard etc.
",Instrumental,Rigol,"[OrderedDict([('id', 'attivRtPF5u9Jt8zc'), ('width', 600), ('height', 400), ('url', 'https://v5.airtableusercontent.com/v1/18/18/1689537600000/dK_mPGZZbeZp5bcFtkzbxg/5lP9IsicLc-53c57pU09lJq8btShrsIP8oUdQD8g-Riyyl7CgjgCuLxljMo3u7mSmIXTKA_1rOHlzYVZ9sSzLxtBldRCizWNkSc1c3VGOCyAsmZ8HP6hOtPBhLsT_DGV/oOwHd5lFYWPNGu0Q17nAm8QwNC1tHLy_fE5qOLj7tSs'), ('filename', 'rigol-technologies-inc-logo-vector.png'), ('size', 2793), ('type', 'image/png'), ('thumbnails', OrderedDict([('small', OrderedDict([('url', 'https://v5.airtableusercontent.com/v1/18/18/1689537600000/zLg1x4J0supwCaJ83IL-4A/V-NesMLWPnrBKKPErwXZSINLiLFF25aXtZnMo_kMyeFuWjgTj5V5zEjv57r4fGSxq3VyIPN_4rjf69NIqz8Gzw/pZHIjN5-F-C70IjpD1k3NvdeUOvruDC8BQQZecQyYN8'), ('width', 54), ('height', 36)])), ('large', OrderedDict([('url', 'https://v5.airtableusercontent.com/v1/18/18/1689537600000/TtrY8vWkD6dH3FpegMsnZg/VDR5M5gZRQkV2BrFVjDxcbTNXGQhzU42l5WvrjM-ipcjmlF-7Oq-BO3myo_SOVCkT2wiVPQAjmpQh9cV_64hPQ/5dMGpy23APettmB4rgrzyyIGA_a3a69X1naJ8wxXyDA'), ('width', 600), ('height', 400)])), ('full', OrderedDict([('url', 'https://v5.airtableusercontent.com/v1/18/18/1689537600000/BLGNhQ0TAAvOaix0UZDcpQ/7lmilKMeJr2Ofwuq_NWkkqdKkoUC_-kSB7azAErsuAjnu-0VYL8ejL_P2ioQYZxF9PzGLPVMxaKtkDVN-tY0Sg/juOPtupWPWQ8JaIQ3AdUWz3xKt51fE8OTZzs7jTwlTk'), ('width', 3000), ('height', 3000)]))]))])]",https://www.rigol.com/,Write a Python script that uses Instrumental to connect to a ?? Oscilloscopes,https://en.wikipedia.org/wiki/Oscilloscope,['Oscilloscopes'],An oscilloscope (informally scope or O-scope) is a type of electronic test instrument that graphically displays varying voltages of one or more signals as a function of time.,??,,,https://www.rigolna.com/products/digital-oscilloscopes/1000/,https://github.com/mabuchilab/Instrumental/blob/master/instrumental/drivers/scopes/rigol.py,not on docs,,,,,"# -*- coding: utf-8 -*-
""""""
Driver module for Rigol oscilloscopes. Currently supports

* DS1000Z series
""""""
import visa
from pyvisa.constants import InterfaceType
import numpy as np
from pint import UndefinedUnitError
from . import Scope
from .. import VisaMixin, SCPI_Facet, Facet
from ..util import visa_context
from ... import u, Q_
from enum import Enum
from .. import ParamSet
from visa import ResourceManager
from enum import Enum
import time
import numpy as np
from struct import unpack

_INST_PARAMS_ = ['visa_address']
_INST_VISA_INFO_ = {
    'DS1000Z': ('RIGOL TECHNOLOGIES', ['DS1054Z']),
}

MANUFACTURER_ID = 0x1AB1

class SpecTypes(Enum):
    DS1054Z = 0x04CE

def list_instruments():
    """"""Get a list of all spectrometers currently attached""""""
    paramsets = []
    model_string = ''

    for spec in SpecTypes:
        model_string += '(VI_ATTR_MODEL_CODE==0x{:04X}) || '.format(spec.value)
    model_string = model_string.rstrip(' || ')
    search_string = ""USB?*?{{VI_ATTR_MANF_ID==0x{:04X} && ({})}}"".format(MANUFACTURER_ID, model_string)

    rm = ResourceManager()
    try:
        raw_spec_list = rm.list_resources(search_string)
    except:
        return paramsets

    for spec in raw_spec_list:
        _, _, model, serial, _ = spec.split('::', 4)
        model = SpecTypes(int(model, 0))
        paramsets.append(ParamSet(DS1000Z, usb=spec, serial=serial, model=model))

    return paramsets

class OnOffState(Enum):
    ON = True
    OFF = False

class RigolScope(Scope, VisaMixin):
    """"""
    A base class for Rigol Technologies Scopes
    """"""

    yinc = SCPI_Facet(':WAVeform:YINCrement', convert=float)
    yref = SCPI_Facet(':WAVeform:YREFerence', convert=float)
    yorig = SCPI_Facet(':WAVeform:YORigin', convert=float)
    xincr = SCPI_Facet(':WAVeform:XINCrement', convert=float)
    beeper = SCPI_Facet('SYSTem:BEEPer', convert=OnOffState)

    def _initialize(self):
        self._rsrc.write_termination = '\n'
        self._rsrc.read_termination = '\n'

    @property
    def manufacturer(self):
        manufacturer, _, _, _ = self.query('*IDN?').rstrip().split(',', 4)
        return manufacturer

    @property
    def model(self):
        _, model, _, _ = self.query('*IDN?').split(',', 4)
        return model

    @property
    def serial(self):
        _, _, serial, _ = self.query('*IDN?').split(',', 4)
        return serial

    @property
    def version(self):
        _, _, _, version = self.query('*IDN?').rstrip().split(',', 4)
        return version

    @property
    def beeper(self):
        val = self.query('SYSTem:BEEPer?')
        return OnOffState[val].value

    @beeper.setter
    def beeper(self, val):
        val = int(bool(val))
        self.write('SYSTem:BEEPer %s' % OnOffState(val).name)

    @property
    def vmax_averages(self):
        return self.query(':MEASure:STATistic:ITEM? AVERages,VMAX')

    @property
    def vmax(self):
        return self.query(':MEASure:ITEM? VMAX')

    @property
    def vmin_averages(self):
        return self.query(':MEASure:STATistic:ITEM? AVERages,VMIN')

    @property
    def vmin(self):
        return self.query(':MEASure:ITEM? VMIN')

    def get_data(self):
        self.write(':WAV:SOUR CHAN1')
        time.sleep(1)
        data = self._rsrc.query_binary_values(':WAVeform:DATA?', datatype='B')

        yinc = self.yinc # Don't query multiple times
        yref = self.yref
        yorig = self.yorig
        xincr = self.xincr

        Volts = [(val - yorig - yref) * yinc for val in data]
        Time = np.arange(0, xincr * len(Volts), xincr)

        return Time, Volts

    def single_acq(self):
        self.write(""STOP"")
        self.write("":FUNCtion:WRECord:ENABle 0"")
        self.write("":FUNCtion:WRECord:ENABle 1"")
        self.write(""RUN"")
        while True:  # 1 means that the acquisition is still running
            time.sleep(0.5)
            if self.query("":FUNCtion:WRECord:OPERate?"") == ""STOP"":
                self.write("":FUNCtion:WRECord:ENABle 0"")
                break

    def local(self):
        self.write('SYSTem:LOCal')

    def remote(self):
        self.write('SYSTem:REMote')


class DS1000Z(RigolScope, VisaMixin):
    pass"
152,550.0,USA,"Kiralux 12.3 MP Monochrome CMOS Camera, Hermetically Sealed Cooled Package, USB 3.0 Interface, 1/4""-20 Taps
",Thorlabs Tsi (All Cameras),,"Thorlabs, Inc. is an American privately held optical equipment company headquartered in Newton, New Jersey. The company was founded in 1989 by Alex Cable, who serves as its current president and CEO. As of 2018, Thorlabs has annual sales of approximately $500 million.
",Instrumental,Thorlabs,"[OrderedDict([('id', 'attUZyCLkkmIuEgLg'), ('width', 2560), ('height', 398), ('url', 'https://v5.airtableusercontent.com/v1/18/18/1689537600000/BmNIeFHbhni_Wx2fsTQ7xA/7KGqzexbwgJQF4Glvxuce4xqbhQEHXktvcuNjxXhlvAhkK8CMrzKBeVv_-PHP7tmn-ApzOCstqQ78s4C7I55B6lFLXiYxxm-LRTgXLyuUC0/lrbUj5K4TTuMnassRPaSVDuk_3XZ5ajRvKcgWES7DOs'), ('filename', 'Image.png'), ('size', 70770), ('type', 'image/png'), ('thumbnails', OrderedDict([('small', OrderedDict([('url', 'https://v5.airtableusercontent.com/v1/18/18/1689537600000/ZZWxVhiPn1kMizuxLLk0Fg/0ebbYsrz7yPvC0KBBzNXH_YPxfCVcdt1ddf0aE3R7HCJoloiw8p80VHX4oB56xrqE7mAvyArGgfM18_lSXRRgQ/_9jZYvkMYMGOsykN6AlnWrE4VLLRcgITDBJiyCGI1Jk'), ('width', 232), ('height', 36)])), ('large', OrderedDict([('url', 'https://v5.airtableusercontent.com/v1/18/18/1689537600000/zIACzGEswy_VqAPI41RE0w/p9fZRVc7zXxmA-O88-PkMCrGiAgfV6ZhiPuJIdJges4pm8vK0kRS4RVlbvX3ISNKHhkpjNxGWTHFMS-Z1i6ltw/Q8t43BvydtWeEdRBXsOXk-0v2Fk3iEIno3tA1RqZfYA'), ('width', 2560), ('height', 398)])), ('full', OrderedDict([('url', 'https://v5.airtableusercontent.com/v1/18/18/1689537600000/g4og31dOk-TKjerxqVIoQg/QGIUp1sDPwfwfv7ZVi9tuQgRcWx7g3ITLMgEtKB1jNCQNc21T5YWxgXMcBFvV1ABa0s_2hn56K_GXiQvwAKfzg/1Q3db6fhAbr_EEtoThHWZoCVFkSliLdsG_PPrvp8Zeo'), ('width', 3000), ('height', 3000)]))]))])]",https://www.thorlabs.com/,Write a Python script that uses Instrumental to connect to a CC126Mu (Example) Cameras,https://en.wikipedia.org/wiki/Camera_module,['Cameras'],"A camera module is an image sensor integrated with a lens, control electronics, and an interface like CSI, Ethernet or plain raw low-voltage differential signaling.",CC126Mu (Example),https://www.thorlabs.com/_sd.cfm?fileName=ITN004586-D02.pdf&partNumber=CC126MU,"[OrderedDict([('id', 'attQB4KqLwtljUVvu'), ('width', 150), ('height', 150), ('url', 'https://v5.airtableusercontent.com/v1/18/18/1689537600000/5feJaci6lFXTCdqgWQdyUg/ZH_31gLdzXk1aqFwphSEGt3uGzJN_G40CDcFgKXPjJw9sShzSiBnJkxu2uTRj9rojTVikAP0NxoOSqtD41KViwJtuKJHMgGpm4Igqqe3IeM/7EDWqYXgmm9mY1yLuLjgbfsC-mUa2p48Hd0P3BhhOOY'), ('filename', 'ITN004855-std.jpg'), ('size', 12030), ('type', 'image/jpeg'), ('thumbnails', OrderedDict([('small', OrderedDict([('url', 'https://v5.airtableusercontent.com/v1/18/18/1689537600000/dTrwsai_qWUaJCBuNWYgbg/zurJYpLghDX9XwOGd_-LGYncNgNV18gP6Qe6n71vwrnEO20ucfltxY3dnqYhuJvdbTqtTMQl_bmdvblDuwaHOQ/E6aN9tbLzs8oUGtEqNwB1_i8KgptPbQ7t1uSNpOy7dE'), ('width', 36), ('height', 36)])), ('large', OrderedDict([('url', 'https://v5.airtableusercontent.com/v1/18/18/1689537600000/qvPv7Q8AqE6HpPWMfjVk8g/z0qFGj8IBRb7lcUjbPnzkx3qMDS3pb22hVNgOB77SEa0K4hqXVwDFJ0EjUqxDVEOHHmYkWPtX8D7iJ8uU8Wk0w/N4AdQmB8Ld93DO7Q1aHNjfCL_bVE8bjQL4dYgWO2Rm4'), ('width', 150), ('height', 150)])), ('full', OrderedDict([('url', 'https://v5.airtableusercontent.com/v1/18/18/1689537600000/HHZpYVr9H3upIGE7c84ggw/pCGaT6ddmpIkBQkhnY8z6BiScXy84cKMMX97eQEf2xsczHBElOFeOdjCa5dznOlZazqGiI942KsI0muu8N0y_w/VLJnVds_J51-j6KOrEs8ge9fxpGgIE0hoQ6mVMKjbAY'), ('width', 3000), ('height', 3000)]))]))])]",https://www.thorlabs.com/newgrouppage9.cfm?objectgroup_id=13243,https://github.com/OregonIons/TSICam/blob/master/TSICam/driver.py,https://instrumental-lib.readthedocs.io/en/stable/cameras-tsi.html,5200.0,,,TSI Driver is made for all scientific Camera from ThorLABS,"from TSICam import tsi_library
import numpy as np
import collections
import time
import functools
import logging
from threading import Thread

logger = logging.getLogger(__name__)

class TSICam:
    """"""
    Class for interfacing with a single Thorlabs Scientific Imaging (TSI) camera.

    A TSICam instance can be initialized using the desired serial number.
    If no serial number is specified, it connects to first listed camera.

    The `get_image` function returns images from a frame buffer; in order to
    to have images in the buffer, either `start_acquisition` (which acquires
    images continuously) or `single_acquisition` must be called.

    Callbacks can be registered such that for every new frame acquired, the
    function is called, with the numpy array forming the image as the sole
    argument.

    :param sn: camera serial number
    :sn type: str
    :param framebuffer_len: length of frame buffer
    :framebuffer_len type: int
    """"""

    # Servo polling period in seconds
    _POLL_PERIOD = 0.05

    def __init__(self, sn=None, framebuffer_len=100):
        """"""Constructor method
        """"""
        self.quit = False
        self.dead = False
        self.connected = False

        self.acquisition_enabled = False

        self.auto_exposure = False
        self.exposure = None
        self.exposure_min = None
        self.exposure_max = None
        self.exposure_inc = None

        self.frame_rate = 3.0 # set frame rate to 3fps

        self._frame_buffer = collections.deque([], framebuffer_len)
        self._frame_call_list = []

        # connect to camera library
        self._library_init()
        if sn is None:
            try:
                self._connect(self.c._cam_list[0])
            except Exception as e:
                self._library_cleanup()
                raise
        else:
            try:
                self._connect(sn)
            except Exception as e:
                self._library_cleanup()
                raise

        logger.debug(""Starting image acquisition thread"")
        t = Thread(target=self._acquisition_thread, daemon=True)
        t.start()

    def _library_init(self):
        self.c = tsi_library.TSI_Library()

    def _library_cleanup(self):
        del self.c

    def _connect(self, sn=None):
        id_ = 0
        for i in range(len(self.c._cam_list)):
            camera = self.c._cam_list[i]
            serial_no = camera
            if str(sn) in serial_no:
                if id_:
                    raise ValueError(
                        ""Multiple substring matches for {}"".format(sn))
                id_ = camera
        if sn is not None and id_ == 0:
            raise ValueError(""Camera {} not found"".format(sn))
        self.c.connect(id_)
        self._get_sensor_info()
        self._get_exposure_params()
        self._get_aoi()
        self._get_aoi_absolute()
        self.get_serial_no()

        self.connected = True # a camera is connected

    def _disconnect(self):
        self.c.disconnect()
        self.connected = False

    def _reconnect(self):
        self._disconnect()
        self._library_cleanup()
        self._library_init()
        self._connect(self._serial_no)

    def _acquisition_thread(self):
        while self.quit is False:
            if self.acquisition_enabled:
                try:
                    # print(""waiting"", flush=True)
                    im = self.c.acquire(native=True)
                    im = np.transpose(im)
                    # from here on in, the first axis of im is the x axis
                    # print(""acquired!"", flush=True)
                except:
                    logger.exception(""Exception occurred, reconnecting"")
                    self._reconnect()
                else:
                    im = self._crop_to_aoi(im)
                    self._frame_buffer.append(im.copy(order=""C""))
                    for f in self._frame_call_list:
                        f(im)

                    # print(""enabled {}"".format(self.acquisition_enabled), flush=True)
                    # Update the exposure if necessary
                    if self.auto_exposure:
                        # print(""try auto"", flush=True)
                        self.handle_auto_exposure(im)

            # print(""sleeping"", flush=True)
            time.sleep(self._POLL_PERIOD)
            # print(""slept"", flush=True)
                
        self.dead = True

    def start_acquisition(self, single=False):
        """"""Turn on auto acquire""""""

        def acquire_single_cb(_):
            """"""image is passed to callback and not used""""""
            self.stop_acquisition()
            self.deregister_callback(acquire_single_cb)

        if single:
            self.register_callback(acquire_single_cb)

        self.acquisition_enabled = True

    def single_acquisition(self):
        """"""Single acquisition. Turns on auto acquire, and then turns it off after one acquisition.""""""
        self.start_acquisition(single=True)

    def stop_acquisition(self):
        """"""Turn off auto acquire""""""
        self.acquisition_enabled = False

    def get_serial_no(self):
        """"""Return camera serial number.

        :return: camera serial number
        :rtype: str
        """"""
        info = self._get_cam_info()
        self._serial_no = info[""SerNo""]
        return self._serial_no

    def _get_cam_info(self):
        cam_info = {""SerNo"": self.c._sn, ""Model"": self.c._model}
        return cam_info

    def get_pixel_width(self):
        """"""Returns pixel width in microns.

        :return: pixel width in microns
        :rtype: float
        """"""
        return self.c.get_pixel_width()

    def _get_frame_rate_params(self):
        self.frame_rate_min, self.frame_rate_max, \
            self.frame_rate_inc =  self.c.get_frame_rate_limits()

    def get_frame_rate_params(self):
        """"""Return current frame rate, minimum, maximum and increment values.
        Only works if frame rate control is enabled.

        :return: (frame rate, frame rate min, frame rate max, frame rate inc)
        :rtype: tuple
        """"""
        self._get_frame_rate_params()
        return self.frame_rate, self.frame_rate_min, self.frame_rate_max, self.frame_rate_inc

    def _get_exposure_params(self):
        self.exposure = self.c.get_exposure()
        self.exposure_min, self.exposure_max, \
            self.exposure_inc = self.c.get_exposure_limits()

    def get_exposure_params(self):
        """"""Return current exposure, minimum, maximum and increment values.

        :return: (exposure, exposure min, exposure max, exposure inc)
        :rtype: tuple
        """"""
        self._get_exposure_params()
        return self.exposure, self.exposure_min, self.exposure_max, self.exposure_inc

    def set_exposure_time(self, exposure_time):
        """"""Set the CCD exposure time in seconds.

        :param exposure_time: New exposure time in seconds.
        :exposure_time type: float
        """"""
        time_ms = exposure_time * 1000
        self.set_exposure_ms(time_ms)

    def set_exposure_ms(self, exposure_time):
        """"""Set the camera exposure time in ms.
        
        :param exposure_time: New exposure time in ms.
        :exposure_time type: float
        """"""
        self.c.set_exposure(exposure_time)
        self.exposure = exposure_time

    def set_auto_exposure(self, auto_exposure):
        """"""Enable/disable auto exposure.

        :param auto_exposure: True to enable auto exposure, False to disable
        :auto_exposure type: bool
        """"""
        self.auto_exposure = auto_exposure

    def handle_auto_exposure(self, image):
        """"""Reads the most recent image and updates exposure

        :param image: most recent image
        :image type: numpy array
        """"""
        # print(""got to exposure handler"", flush=True)
        pixel_max = np.amax(image)

        # print(""autoexpose"", flush=True)
        if pixel_max > auto_exposure_max_threshold:
            exposure = np.maximum(self.exposure*0.9, self.exposure_min)
            # if exposure == self.exposure:
                # print(""warning: min exposure reached"", flush=True)
            # print(""reducing exposure"", flush=True)
            self.set_exposure_ms(exposure)

        elif pixel_max < auto_exposure_min_threshold:
            exposure = np.minimum(self.exposure*1.1, self.exposure_max)
            # if exposure == self.exposure:
                # print(""warning: max exposure reached"", flush=True)
            # print(""increasing exposure"", flush=True)
            self.set_exposure_ms(exposure)

    def set_image_region(self, hStart, hEnd, vStart, vEnd, **kwargs):
        """"""Set the CCD region to read out.
        The region is 0 indexed and inclusive, so the valid ranges for hStart
        is 0 ... self.ccd_width - 1 etc.

        :param hStart: starting point for horizontal axis
        :hStart type: float
        :param hEnd: ending point for horizontal axis
        :hEnd type: float
        :param vStart: starting point for vertical axis
        :vStart type: float
        :param vEnd: ending point for vertical axis
        :vEnd type: float
        """"""
        if kwargs:
            print(""binning is not supported"")
        self.aoi_x = self._clip_to_grid(int(hStart), 0, self.ccd_width, 4)
        self.aoi_y = self._clip_to_grid(int(vStart), 0, self.ccd_height, 2)
        self.aoi_width = self._clip_to_grid(int(1+hEnd-hStart), 32,
                                            int(self.ccd_width-hStart), 4)
        self.aoi_height = self._clip_to_grid(int(1+vEnd-vStart), 4,
                                             int(self.ccd_height-vStart), 2)

        self._set_aoi(self.aoi_x, self.aoi_y, self.aoi_height, self.aoi_width)
        # exposure settings will change
        self._get_exposure_params()

    def _clip_to_grid(self, in_val, min_, max_, grid):
        out_val = np.clip(in_val, min_, max_)
        # integer maths does the rounding
        out_val = out_val // grid * grid
        return out_val

    def _set_aoi(self, posx, posy, width, height, absolute=True):
        """"""Set Area Of Interest according to Thorlabs spec
        If absolute is True, the memory location of the aoi within the image
        matches its actual position
        """"""
        self.c.set_roi(posx, posy, width, height)

    def _get_aoi(self):
        """"""Gets the aoi parameters, sets internal values and returns them""""""
        rectAOI = self.c.get_roi()
        self.aoi_x = rectAOI.upper_left_x_pixels
        self.aoi_y = rectAOI.upper_left_y_pixels
        self.aoi_width = rectAOI.lower_right_x_pixels - rectAOI.upper_left_x_pixels
        self.aoi_height = rectAOI.lower_right_y_pixels - rectAOI.upper_left_y_pixels

        return self.aoi_x, self.aoi_y, self.aoi_width, self.aoi_height

    def _get_aoi_absolute(self):
        self.aoi_absolute = (self.c.get_sensor_size() == self.c.get_image_size())
        return self.aoi_absolute

    def _crop_to_aoi(self, image):
        """"""crop the image to the size of the aoi""""""
        if self.aoi_width == self.ccd_width \
                and self.aoi_height == self.ccd_height:
            # shortcut for when the aoi is the whole sensor
            return image

        x_start = self.aoi_x if self.aoi_absolute else 0
        y_start = self.aoi_y if self.aoi_absolute else 0
        x_end = self.aoi_width + x_start
        y_end = self.aoi_height + y_start
        return image[x_start:x_end, y_start:y_end]

    def _get_sensor_info(self):
        self.ccd_width, self.ccd_height = self.c.get_sensor_size()
    
    def register_callback(self, f):
        """"""Register a function to be called from the acquisition thread for each
        new image.

        :param f: function to register
        :f type: func
        """"""
        self._frame_call_list.append(f)

    def deregister_callback(self, f):
        """"""Deregister a function to be called from the acquisition thread.

        :param f: function to deregister
        :f type: func
        """"""
        if f in self._frame_call_list:
            self._frame_call_list.remove(f)

    def get_image(self):
        """"""Returns the oldest image in the buffer as a numpy array, or None if
        no new images.

        :return: image
        :rtype: numpy array, or None
        """"""
        if len(self._frame_buffer) == 0:
            return None
        return self._frame_buffer.popleft()

    def get_all_images(self):
        """"""Returns all of the images in the buffer as an array of numpy arrays,
        or None if no new images.

        :return: array of images
        :rtype: numpy array, or None
        """"""
        if len(self._frame_buffer):
            ims = []
            while len(self._frame_buffer) > 0:
                ims.append(self._frame_buffer.popleft())
        else:
            ims = None
        return ims

    def acquire_and_get_single_image(self):
        """"""Triggers a single image acquisition and returns image.

        :return: image
        :rtype: numpy array
        """"""
        logger.info(""Capturing image..."")
        self.single_acquisition()
        im = self.get_image()
        while im is None:
            time.sleep(0.1)
            im = self.get_image()
        logger.info(""Image captured"")
        return im

    def _ping(self):
        return True

    def close(self):
        """"""Disconnects from camera and cleans up library.
        """"""
        self.quit = True
        while not self.dead:
            time.sleep(0.01)
        self._disconnect()
        self._library_cleanup()

    def __del__(self):
        """"""Deletion operations
        """"""
        try:
            self.close()
        except AttributeError:
            # The library was already cleaned up
            pass
"
166,14.0,Germany,"The ECC100 is a state-ofthe-art motion controller, allowing the simultaneous operation of up to three positioners from attocube’s industrial ECS Drive series.",Attocube ECC 100,,"**Attocube** is a leading pioneer for nanotechnology solutions in precision motion and nanopositioning applications, cryogenic microscopy,

",Instrumental,Attocube,"[OrderedDict([('id', 'att3WtxPNsVQ6tRkf'), ('width', 711), ('height', 165), ('url', 'https://v5.airtableusercontent.com/v1/18/18/1689537600000/xsxIWN8M1gyROR2MzmRuUQ/jNBn5-0PU84zAvkh0orQjAmapbajHqsnW2nA3mG_mgokIwQl8l8YnZrFQojjzxPNrIDhvxsg7-PTgWxA47SFpsruf4a1f8pp5n2xYEyMhq8/Wl1o87U3Svlk7xDXFWrpt4z0T5J9QWNoyndLNLDxVP8'), ('filename', 'attocube_systems.png'), ('size', 42332), ('type', 'image/png'), ('thumbnails', OrderedDict([('small', OrderedDict([('url', 'https://v5.airtableusercontent.com/v1/18/18/1689537600000/M3IbIKRAvITPOznGTBqdpA/w7wc5Bq-pvquKMt8Iui05x_TeXFMAAbrj8CFF_giy_fhfw_a6UDUmzf8JS5XgQeEp0NQzWfQihfJvkrjqh2ibQ/9UmmKc2fJUEAeF-8b4JjFu_pG0UhEJ4bdR7Nz8sEdY8'), ('width', 155), ('height', 36)])), ('large', OrderedDict([('url', 'https://v5.airtableusercontent.com/v1/18/18/1689537600000/wto4V_QVd96QIrUpssl4Ew/sRMW72g4O4T_NXxW6jGHspX6B6c9KUjRSLICsV0y6uCjcSNwu7gkWcuPO-8dTMX5eHZxCK17sTHk06cxaO5FPA/I8UbobHUtdTPxJKo5-74I34DpGJiGRze2-UoUm-W7l8'), ('width', 711), ('height', 165)])), ('full', OrderedDict([('url', 'https://v5.airtableusercontent.com/v1/18/18/1689537600000/sMk-SiW5Bz-ET-2czXEHRA/QH1_8LPsc8sYq0mDTxZyxvaNC905UwvB8s8zN5yDq0iVwBigbgTKQ2cgQ8VzoBD2FL0ItyJTYd15Ans5N5Yo6g/L0Qlckp4wFk7hfY2GGWSg3k5Xa3-lYPMoMy3UxgFyKc'), ('width', 3000), ('height', 3000)]))]))])]",https://www.attocube.com/en,Write a Python script that uses Instrumental to connect to a {Device name} Motion,https://en.wikipedia.org/wiki/Motion_control,['Motion'],"Motion controller calculates and controls the mechanical trajectories (motion profile) an actuator must follow (i.e., motion planning) and, in closed loop systems, employs feedback to make control corrections and thus implement closed-loop control.",,https://www.attocube.com/application/files/6416/3697/3543/Specifications_ECC100.pdf,"[OrderedDict([('id', 'attT8ecDBvpxYrTlB'), ('width', 850), ('height', 550), ('url', 'https://v5.airtableusercontent.com/v1/18/18/1689537600000/DV0XI11xDUhvYZ6xCDm8BQ/7MR-9sHnKUomNtjEpX61hgVIJWM4nOnkXiUxwCzqenlxiPMU7IPJqYYxVBA6scobxOrNTpnQIoJoE_qnIOm59PbSvTFCnsGIR7at30uJoQ8/MNBEd_2CunEyTn2fqKfYWiX1X_Qg8-N3yW6jhu6oDos'), ('filename', 'ECC100_Legacy.png'), ('size', 47654), ('type', 'image/png'), ('thumbnails', OrderedDict([('small', OrderedDict([('url', 'https://v5.airtableusercontent.com/v1/18/18/1689537600000/37__N8NvFkbr-7NpQEHfNw/M8hgmuq9r5OzwVhE4es6x60Uvj20PwSJx7RSykjVFga3iwwZqVnrJvWfPDzA5z3SX45MCMefN_KnHWMOmQKweQ/-2vr8xwmBIZF5kHG4WvLlxtXZgtfgOrEU_5OgaL017Q'), ('width', 56), ('height', 36)])), ('large', OrderedDict([('url', 'https://v5.airtableusercontent.com/v1/18/18/1689537600000/YMZ8jJfrv0QO5QqZh7ruuw/nuKYJ504TNu54k7xSsBHjBqwTIbe9ymnldZODp_ByFzkvuHQSXL32TgrlOXEfGPVfFG6Yel2_XD7ZbekJuUTcg/o201gQxBzJPYuGQi4rHPbeEvV2Bbvzxau38-rAxs0lk'), ('width', 791), ('height', 512)])), ('full', OrderedDict([('url', 'https://v5.airtableusercontent.com/v1/18/18/1689537600000/hKDOZw_DCtrW5UV_lwNxcg/1DZRV97Sizohah_9m8JGrnEJU3HP2izHUlvqUXne2DAIfvrDEJ-I7Qmu6yC2lEsK094WnCxTB1_ZSZNxdBe5Zg/awIr-lfhjZDrMmtSpy-KF1qXgtx3Fld_7CDOmPXMmPw'), ('width', 3000), ('height', 3000)]))]))])]",https://www.observatorysciences.co.uk/assets/uploads/downloads/ECC100_Developers_Manual_V1-0.pdf,https://github.com/MaxIV-KitsControls/lib-maxiv-ecc100/blob/master/libecc100/libECC100.py,https://instrumental-lib.readthedocs.io/en/0.3.1/ecc100.html,,,,,"import socket
import ecc100_protocol
import ucprotocol
import time
"""""" receive buffer size (bytes) """"""
BUFFER_SIZE = 1024

"""""" line feed characters """"""
LF = '\n'

TEST_CTAG = 0x99
TIME_DELAY = 0.25

class ECC100():

    def __init__(self, host):
        self.ECC100_HOST = host
        self.ECC100_TCP_PORT = 2101

        self.connect()
        self.disable_events()
        try:
            self.AXIS0_NAME = self.get_name(0).rstrip()
            self.AXIS1_NAME = self.get_name(1).rstrip()
            self.AXIS2_NAME = self.get_name(1).rstrip()
        except Exception as ex:
            self.AXIS0_NAME = ''
            self.AXIS1_NAME = ''
            self.AXIS2_NAME = ''

	for axis in [0, 1, 2]:
	    if self.axis_connected(axis):
  		self.enable_output_relay(axis)

    def connect(self):
        self.socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)

        try:
            self.socket.connect((self.ECC100_HOST, self.ECC100_TCP_PORT))
        except:
            print 'Connection failed'
            pass

    def open(self):
        self.socket.connect((self.ECC100_HOST, self.ECC100_TCP_PORT))

    def close(self):
        self.socket.close()

    def restart(self):
        self.close()
        time.sleep(0.1)
        self.connect()

    def send(self, cmd):
        self.socket.send(cmd + LF)

    def recv(self):
        r = self.socket.recv(BUFFER_SIZE).rstrip(LF)
        return r

    def query(self, cmd):
        try:
            self.socket.sendall(cmd)  # or perhaps plain send?
            return self.recv()
        except ValueError as err:
            raise ValueError(""Command type error: %s"" % str(err))

    def parse_get_telegram(self, axis, address):
        return ucprotocol.UcGetTelegram.pack(ucprotocol.UcGetTelegram_size,
                                             ucprotocol.UC_GET,
                                             address,
                                             axis,
                                             TEST_CTAG)

    def parse_set_telegram(self, axis, address, value):
        return ucprotocol.UcSetTelegram.pack(ucprotocol.UcSetTelegram_size,
                                             ucprotocol.UC_SET,
                                             address,
                                             axis,
                                             TEST_CTAG,
                                             value)
    def disable_events(self):
        resp = self.query(self.parse_set_telegram(0, ecc100_protocol.ID_ASYNC_EN, 0))
        data = ucprotocol.UcAckTelegram.unpack(resp)

    def get_firmware(self):
        resp = self.query(self.parse_get_telegram(0, ecc100_protocol.ID_ECC_FIRMWARE_REV))
        data = ucprotocol.UcAckTelegram.unpack(resp)
        return hex(data[6])[2:]

    def get_position(self, axis):
        resp = self.query(self.parse_get_telegram(axis, ecc100_protocol.ID_ECC_POSITION))
        data = ucprotocol.UcAckTelegram.unpack(resp)
        return float(data[6])  # unit nm

    def set_position(self, axis, position):
        resp = self.query(self.parse_set_telegram(axis,
                                                  ecc100_protocol.ID_ECC_TARGET_POS,
                                                  position))
        try:
            data = ucprotocol.UcAckTelegram.unpack(resp)
            result = int(data[5])
        except Exception as ex:
            result = 1

        return result  # check protocol  for error codes, 0: ok

    def get_reference_position(self, axis):
        resp = self.query(self.parse_get_telegram(axis, ecc100_protocol.ID_ECC_REFPOSITION))
        data = ucprotocol.UcAckTelegram.unpack(resp)
        return float(data[6])  # unit nm

    def get_reference_position_valid(self, axis):
        resp = self.query(self.parse_get_telegram(axis, ecc100_protocol.ID_ECC_REFPOS_VALID))
        data = ucprotocol.UcAckTelegram.unpack(resp)
        return data[6]
    
    def enable_movement(self, axis):
        # Controls the approach of the actor to the target position. 1: enable approach, 0: Stop movement
        resp = self.query(self.parse_set_telegram(axis,
                                                  ecc100_protocol.ID_ECC_MOVE_ABS,
                                                  1))
        try:
            data = ucprotocol.UcAckTelegram.unpack(resp)
            result = int(data[5])
        except Exception as ex:
            result = 1

        return result  # check protocol  for error codes, 0: ok

    def disable_movement(self, axis):
        # Controls the approach of the actor to the target position. 1: enable approach, 0: Stop movement
        resp = self.query(self.parse_set_telegram(axis,
                                                  ecc100_protocol.ID_ECC_MOVE_ABS,
                                                  0))
        try:
            data = ucprotocol.UcAckTelegram.unpack(resp)
            result = int(data[5])
        except Exception as ex:
            result = 1

        return result  # check protocol  for error codes, 0: ok

    def enable_output_relay(self, axis):
	# Controls the output relais of the selected axis. 1: on, 0: off
        resp = self.query(self.parse_set_telegram(axis,
                                                  ecc100_protocol.ID_ECC_OUTPUT_EN,
                                                  1))
        try:
            data = ucprotocol.UcAckTelegram.unpack(resp)
            result = int(data[5])
        except Exception as ex:
            result = 1

        return result  # check protocol  for error codes, 0: ok
    
    def disable_output_relay(self, axis):
	# Controls the output relais of the selected axis. 1: on, 0: off
        resp = self.query(self.parse_set_telegram(axis,
                                                  ecc100_protocol.ID_ECC_OUTPUT_EN,
                                                  0))
        try:
            data = ucprotocol.UcAckTelegram.unpack(resp)
            result = int(data[5])
        except Exception as ex:
            result = 1

        return result  # check protocol  for error codes, 0: ok

    def trigger_single_step_backward(self, axis):
        resp = self.query(self.parse_set_telegram(axis,
                                                  ecc100_protocol.ID_ECC_SGL_STEP_BKWD,
                                                  1))
        try:
            data = ucprotocol.UcAckTelegram.unpack(resp)
            result = int(data[5])
        except Exception as ex:
            result = 1

        return result  # check protocol  for error codes, 0: ok

    def trigger_single_step_forward(self, axis):
        resp = self.query(self.parse_set_telegram(axis,
                                                  ecc100_protocol.ID_ECC_SGL_STEP_FWD,
                                                  1))
        try:
            data = ucprotocol.UcAckTelegram.unpack(resp)
            result = int(data[5])
        except Exception as ex:
            result = 1

        return result  # check protocol  for error codes, 0: ok
    
    def trigger_continous_backward(self, axis, cont=1):
        # if cont = 0, stops all movement of the axis regardless its direction.
        resp = self.query(self.parse_set_telegram(axis,
                                                  ecc100_protocol.ID_ECC_CONT_BKWD,
                                                  cont))
        try:
            data = ucprotocol.UcAckTelegram.unpack(resp)
            result = int(data[5])
        except Exception as ex:
            result = 1

        return result  # check protocol  for error codes, 0: ok
   
    def timed_continous_backward(self, axis, delay=0.1):
        self.trigger_continous_backward(axis)
        time.sleep(delay)
        self.trigger_continous_backward(axis, 0)

    def timed_continous_forward(self, axis, delay=0.1):
        self.trigger_continous_forward(axis)
        time.sleep(delay)
        self.trigger_continous_forward(axis, 0)

    def trigger_continous_forward(self, axis, cont=1):
        # if cont = 0, stops all movement of the axis regardless its direction.
        resp = self.query(self.parse_set_telegram(axis,
                                                  ecc100_protocol.ID_ECC_CONT_FWD,
                                                  cont))
        try:
            data = ucprotocol.UcAckTelegram.unpack(resp)
            result = int(data[5])
        except Exception as ex:
            result = 1

        return result  # check protocol  for error codes, 0: ok
    
    def reset_position(self, axis):
        resp = self.query(self.parse_set_telegram(axis, ecc100_protocol.ID_ECC_POSITION_RESET, 1))
        data = ucprotocol.UcAckTelegram.unpack(resp)
        return data[6]

    def axis_connected(self, axis):
        resp = self.query(self.parse_get_telegram(axis, ecc100_protocol.ID_ECC_CONNECTED))
        data = ucprotocol.UcAckTelegram.unpack(resp)
        return data[6]

    def get_moving_status(self, axis):
        resp = self.query(self.parse_get_telegram(axis, ecc100_protocol.ID_ECC_MOVING))
        data = ucprotocol.UcAckTelegram.unpack(resp)
        return data[6]

    def get_end_of_travel_forward(self, axis):
        resp = self.query(self.parse_get_telegram(axis, ecc100_protocol.ID_ECC_EOT_FWD))
        data = ucprotocol.UcAckTelegram.unpack(resp)
        return data[6]

    def get_end_of_travel_backward(self, axis):
        resp = self.query(self.parse_get_telegram(axis, ecc100_protocol.ID_ECC_EOT_BKWD))
        data = ucprotocol.UcAckTelegram.unpack(resp)
        return data[6]

    def get_on_target_status(self, axis):
        resp = self.query(self.parse_get_telegram(axis, ecc100_protocol.ID_ECC_TARGET_STATUS))
        data = ucprotocol.UcAckTelegram.unpack(resp)
        return data[6]

    def get_name(self, axis):
        resp = self.query(self.parse_get_telegram(axis, ecc100_protocol.ID_ECC_AXIS_MOTORNAME_0))
        name0 = hex(ucprotocol.UcAckTelegram.unpack(resp)[6])[2:].decode('hex')

        resp = self.query(self.parse_get_telegram(axis, ecc100_protocol.ID_ECC_AXIS_MOTORNAME_1))
        name1 = hex(ucprotocol.UcAckTelegram.unpack(resp)[6])[2:].decode('hex')

        resp = self.query(self.parse_get_telegram(axis, ecc100_protocol.ID_ECC_AXIS_MOTORNAME_2))
        name2 = hex(ucprotocol.UcAckTelegram.unpack(resp)[6])[2:].decode('hex')

        resp = self.query(self.parse_get_telegram(axis, ecc100_protocol.ID_ECC_AXIS_MOTORNAME_3))
        name3 = hex(ucprotocol.UcAckTelegram.unpack(resp)[6])[2:].decode('hex')

        resp = self.query(self.parse_get_telegram(axis, ecc100_protocol.ID_ECC_AXIS_MOTORNAME_4))
        name4 = hex(ucprotocol.UcAckTelegram.unpack(resp)[6])[2:].decode('hex')
        return name4+name3+name2+name1+name0  # this is wonderful...


def test():
    host_list = ['172.16.118.51', '172.16.118.53', '172.16.118.52']

    positions = [600, 1200, 1800]  # [nm]

    print 'Starting test program...'
    for host in host_list:
        print 'Controller host: ', host
        ecc100 = ECC100(host=host)

        print 'connected axis names: ', ecc100.AXIS0_NAME, ecc100.AXIS1_NAME, ecc100.AXIS2_NAME
        print 'AXIS 0 >>> Position: ', ecc100.get_position(0), '| Connected: ', ecc100.axis_connected(0)
        print 'AXIS 1 >>> Position: ', ecc100.get_position(1), '| Connected: ', ecc100.axis_connected(1)
        print 'AXIS 2 >>> Position: ', ecc100.get_position(2), '| Connected: ', ecc100.axis_connected(2)

        ecc100.close()
        print 'Connection closed'
        print '#################\n'
        time.sleep(0.5)

    print '$$$$$$$$$$$\n'
    print 'Moving to new positions...'
    for host in host_list:
        try:
            print 'Controller host: ', host
            ecc100 = ECC100(host=host)
            try:
                ecc100.set_position(0, positions[0])
            except:
                print 'error axis 0'
            try:
                ecc100.set_position(1, positions[1])
            except:
                print 'error axis 1'
            try:
                ecc100.set_position(2, positions[2])
            except:
                print 'error axis 2'

            time.sleep(0.1)

            try:
                print 'AXIS 0 >>> Position: ', ecc100.get_position(0)
            except:
                print 'error getting axis'
            time.sleep(0.1)

            try:
                print 'AXIS 1 >>> Position: ', ecc100.get_position(1)
            except:
                print 'error getting axis'
            time.sleep(0.1)

            try:
                print 'AXIS 2 >>> Position: ', ecc100.get_position(2)
            except:
                print 'error getting axis'
            
            ecc100.close()
            print 'Connection closed'
            print '#################\n'

            time.sleep(0.5)

        except Exception as ex:
            ecc100.close()
            print 'Setting error!! ',ex
            time.sleep(0.5)

    print 'Test finished'
    print '#################\n'

if __name__ == '__main__':
    test()
"
212,550.0,USA,"DCU224C - CCD Camera, 1280 x 1024 Resolution, Color, USB 2.0 ","DCx Series
",,"Thorlabs, Inc. is an American privately held optical equipment company headquartered in Newton, New Jersey. The company was founded in 1989 by Alex Cable, who serves as its current president and CEO. As of 2018, Thorlabs has annual sales of approximately $500 million.
",Instrumental,Thorlabs,"[OrderedDict([('id', 'attUZyCLkkmIuEgLg'), ('width', 2560), ('height', 398), ('url', 'https://v5.airtableusercontent.com/v1/18/18/1689537600000/BmNIeFHbhni_Wx2fsTQ7xA/7KGqzexbwgJQF4Glvxuce4xqbhQEHXktvcuNjxXhlvAhkK8CMrzKBeVv_-PHP7tmn-ApzOCstqQ78s4C7I55B6lFLXiYxxm-LRTgXLyuUC0/lrbUj5K4TTuMnassRPaSVDuk_3XZ5ajRvKcgWES7DOs'), ('filename', 'Image.png'), ('size', 70770), ('type', 'image/png'), ('thumbnails', OrderedDict([('small', OrderedDict([('url', 'https://v5.airtableusercontent.com/v1/18/18/1689537600000/ZZWxVhiPn1kMizuxLLk0Fg/0ebbYsrz7yPvC0KBBzNXH_YPxfCVcdt1ddf0aE3R7HCJoloiw8p80VHX4oB56xrqE7mAvyArGgfM18_lSXRRgQ/_9jZYvkMYMGOsykN6AlnWrE4VLLRcgITDBJiyCGI1Jk'), ('width', 232), ('height', 36)])), ('large', OrderedDict([('url', 'https://v5.airtableusercontent.com/v1/18/18/1689537600000/zIACzGEswy_VqAPI41RE0w/p9fZRVc7zXxmA-O88-PkMCrGiAgfV6ZhiPuJIdJges4pm8vK0kRS4RVlbvX3ISNKHhkpjNxGWTHFMS-Z1i6ltw/Q8t43BvydtWeEdRBXsOXk-0v2Fk3iEIno3tA1RqZfYA'), ('width', 2560), ('height', 398)])), ('full', OrderedDict([('url', 'https://v5.airtableusercontent.com/v1/18/18/1689537600000/g4og31dOk-TKjerxqVIoQg/QGIUp1sDPwfwfv7ZVi9tuQgRcWx7g3ITLMgEtKB1jNCQNc21T5YWxgXMcBFvV1ABa0s_2hn56K_GXiQvwAKfzg/1Q3db6fhAbr_EEtoThHWZoCVFkSliLdsG_PPrvp8Zeo'), ('width', 3000), ('height', 3000)]))]))])]",https://www.thorlabs.com/,Write a Python script that uses Instrumental to connect to a Thorlab DCU224C (Example) Cameras,https://en.wikipedia.org/wiki/Camera_module,['Cameras'],"A camera module is an image sensor integrated with a lens, control electronics, and an interface like CSI, Ethernet or plain raw low-voltage differential signaling.",Thorlab DCU224C (Example),https://www.thorlabs.com/_sd.cfm?fileName=ITN000493-D02.pdf&partNumber=DCU224C,"[OrderedDict([('id', 'attcWK1mi1vITqbL4'), ('width', 200), ('height', 200), ('url', 'https://v5.airtableusercontent.com/v1/18/18/1689537600000/hsFpKKpLh4PWtK998vUFbg/AqFWpYmw61amo28MO2QZDL1vE2UILkrDmYa5jzSlA7PqiIECfeHjFOi1pHgKXDRBrFT0OI7yuTy4hEzqa_Qt9mtTFEAd0WyzWVLL95oMwMQ/9iyV1S6zFDJ1Um_dXKgm9jeB6YR1oeIvGjuTlfRsH4k'), ('filename', '15985-std.jpg'), ('size', 6487), ('type', 'image/jpeg'), ('thumbnails', OrderedDict([('small', OrderedDict([('url', 'https://v5.airtableusercontent.com/v1/18/18/1689537600000/FqpNXwi_Om3ZAdMmYTvBgw/ZYhAqxH6pH6oI0lvHh2KhuHnIrACnnHSYnyv1BKB5s-cBEwHSN5vFRGN2nVo7gEetjg11Jfylo3igIlbqAqw0A/GeWJ8FYDjQT8zbKDoWr1ou6hVbX0-5Lt8a98U2M8oNs'), ('width', 36), ('height', 36)])), ('large', OrderedDict([('url', 'https://v5.airtableusercontent.com/v1/18/18/1689537600000/XKbJaJU_OT1nhXh9WVutoQ/pql0D3iGpUwbYNSvBGW8XK6jE1tl97oE42nQA-Tj6kx0umOSrzlR7MHXN6tSXFI70eanLidkKeDyBxOlWyjnFQ/wkg1XLGU3TeoMSmmoBz5KK3RzTAJopjjNZRSLaTZsnM'), ('width', 200), ('height', 200)])), ('full', OrderedDict([('url', 'https://v5.airtableusercontent.com/v1/18/18/1689537600000/JZgiJyNt11hnVXAwkL3OlA/8l3qmMXJ34iReAx0eDrV_wRxRPJWprQ0-Wt5WfdQC6UPFDWQLkeoVbwXZy4DfMGuLTzOUreyV7lY-mUXR4WryQ/KN1VcWiMyN8NotqiSlj2s3Xc1cLmC5lJS47lcREojAg'), ('width', 3000), ('height', 3000)]))]))])]",https://www.thorlabs.com/thorproduct.cfm?partnumber=DCU224C,https://github.com/bernardokyotoku/pydcu/blob/master/tests/test_uc480.py,https://instrumental-lib.readthedocs.io/en/stable/uc480-cameras.html,,,True,,"import uc480
import sys

class Testuc480:
	def setUp(self):
		self.camera = uc480.camera()

	def tearDown(self):
		self.camera.ExitCamera()

	def test_ReadEEPROM(self):
		self.camera.WriteEEPROM(""test"")
		content = self.camera.ReadEEPROM(count=len(""test""))
		assert content == ""test""

	def test_WriteEEPROM(self):
		self.camera.WriteEEPROM(""test"")

	def test_AllocImageMem(self):
		assert self.camera.AllocImageMem() == 0

	def test_FreeImageMem(self):
		self.camera.AllocImageMem()
		assert self.camera.FreeImageMem() == 0

	def test_SetImageMem(self):
		self.camera.AllocImageMem()
		assert self.camera.SetImageMem() == 0

	def test_SetImageSize(self):
		cam = self.camera
		cam.AllocImageMem()
		cam.SetImageMem()
		assert cam.SetImageSize(x = 300,y = 200) == 0
		assert cam.SetImageSize(x = uc480.IS.GET_IMAGE_SIZE_X) == 300
		assert cam.SetImageSize(x = uc480.IS.GET_IMAGE_SIZE_Y) == 200

	def test_CaptureVideo(self):
		self.camera.AllocImageMem()
		self.camera.SetImageMem()
		assert self.camera.CaptureVideo() == 0

	def test_StopLiveVideo(self):
		self.camera.AllocImageMem()
		self.camera.SetImageMem()
		self.camera.CaptureVideo() 
		assert self.camera.StopLiveVideo() == 0

	def test_GetError(self):
		try:
			self.camera.CaptureVideo()
		except Exception:
			self.camera.GetError()
			error_message = self.camera.error_message.value 
		assert error_message == ""There is no activated image memory""

	def test_CopyImageMem(self):
		import numpy 
		self.camera.AllocImageMem()
		self.camera.SetImageMem()
		self.camera.CaptureVideo(uc480.IS.WAIT) 
		self.camera.StopLiveVideo(uc480.IS.WAIT)
		assert self.camera.CopyImageMem() == 0
		assert numpy.sum(self.camera.data) > 0

	def test_AddToSequence(self):
		self.camera.AllocImageMem()
		assert self.camera.AddToSequence() == 0

	def test_CopyImageMem_from_Sequence(self):
		import numpy 
		self.camera.AllocImageMem()
		self.camera.AddToSequence()
		self.camera.SetImageMem()
		self.camera.CaptureVideo(uc480.IS.WAIT) 
		self.camera.StopLiveVideo(uc480.IS.WAIT)
		assert self.camera.CopyImageMem() == 0
		assert numpy.sum(self.camera.data) > 0

	def test_ClearSequence(self):
		assert self.camera.ClearSequence() == 0

	def test_LockSeqBuf(self):
		self.camera.AllocImageMem()
		self.camera.AddToSequence()
		assert self.camera.LockSeqBuf(1) == 0

	def test_UnlockSeqBuf(self):
		self.camera.AllocImageMem()
		assert self.camera.UnlockSeqBuf(1) == 0
		#test with number = 0 done

#	def test_GetLastMemorySequence(self):
#		self.camera.AllocImageMem()
#		self.camera.SetImageMem()
#		self.camera.AddToSequence()
#		self.camera.CaptureVideo(uc480.IS.WAIT) 
#		self.camera.StopLiveVideo(uc480.IS.WAIT)
#		assert self.camera.GetLastMemorySequence() == 0

	def test_GetActSeqBuf(self):
		self.camera.AllocImageMem()
		self.camera.AddToSequence()
		assert self.camera.GetActSeqBuf() == 0

#	def test_GetNumberOfMemoryImages(self):
#		self.camera.AllocImageMem()
#		self.camera.SetImageMem()
#		self.camera.AddToSequence()
#		assert self.camera.GetNumberOfMemoryImages() == 0

	def test_SetAllocatedImageMem(self):
		assert self.camera.SetAllocatedImageMem() == 0

	def test_SetAllocatedImageMem_captures(self):
		import numpy
		self.camera.SetAllocatedImageMem()
		self.camera.SetImageMem()
		self.camera.CaptureVideo(uc480.IS.WAIT) 
		self.camera.StopLiveVideo(uc480.IS.WAIT)
		assert numpy.sum(self.camera.data) > 0

def test_init():
	camera = uc480.camera()
	camera.ExitCamera()
"
239,4614.0,USA,The Kinetix sCMOS delivers the highest speed and largest field of view with near-perfect 96% quantum efficiency,"Photometrics - Cameras
",,"**Teledyne** provides enabling technologies to sense, transmit and analyze information for industrial growth markets.
",Instrumental,Pvcam,"[OrderedDict([('id', 'attUlYZ9niDgdYtKl'), ('width', 2560), ('height', 599), ('url', 'https://v5.airtableusercontent.com/v1/18/18/1689537600000/UA1iji_p-4WoTjTD0nlbaA/Odrt7MRZt3m0jPKRP0dCTAKEepgVQmVL0GJBTsprysOjTnfbiZixlOeD-viGW3kn3NHc3ExZ7pvhI5eJGkPo-fXwx4KKaK0apc-8k4ZaGQ0/4hbNVzDqWh5pxfVFc47kRxMNWdtgGnOiiH7ImH6ebBw'), ('filename', 'Teledyne_logo.svg.png'), ('size', 40989), ('type', 'image/png'), ('thumbnails', OrderedDict([('small', OrderedDict([('url', 'https://v5.airtableusercontent.com/v1/18/18/1689537600000/BR_1pebiRYDzNwinIBKoqA/uJFq58rS57Pgvank6pm7qOqOESXfb1WrAAXwOZ6ApOjZWN--mloek9LS4AuG6S3DHOPQIrAd7G9oishCRWxR6Q/UtmxKgPDiur3i3o0bpv2zMHDyRlavFz0Q92vK-jOogQ'), ('width', 154), ('height', 36)])), ('large', OrderedDict([('url', 'https://v5.airtableusercontent.com/v1/18/18/1689537600000/88ZimlEReqN1i30ktX8VNQ/pZfeJgNao7sz-XR1BrDxssujv8TVrQg1lccmPejH5Ly2J5vAUSk0M8HhDAHAVqfuNU_VuEzZ96NzvEZEviilxA/nwnTB-zFLn-a5uzOqvCo84ewNcdkAHFlHxDirGArNYE'), ('width', 2188), ('height', 512)])), ('full', OrderedDict([('url', 'https://v5.airtableusercontent.com/v1/18/18/1689537600000/L1EGHsMiKRVJAskjT2-Jbg/Jd30P-DrV3x_9SiN4OPmxif59kj7k768zhmOTn6YDde_F4iZLOKJ98sv_LQR16ZbnHabFMaVC6tVna10DJ3QrQ/MgqRQQ9UP95FRX16ymEgOjBG5ucNrhQNBcF37e-q6RY'), ('width', 3000), ('height', 3000)]))]))])]",https://www.teledyne.com,Write a Python script that uses Instrumental to connect to a Kinetix sCMOS (And All the Cameras)) Cameras,https://en.wikipedia.org/wiki/Camera_module,['Cameras'],"A camera module is an image sensor integrated with a lens, control electronics, and an interface like CSI, Ethernet or plain raw low-voltage differential signaling.",Kinetix sCMOS (And All the Cameras)),https://www.photometrics.com/wp-content/uploads/2023/03/Kinetix-Datasheet-Rev-A3-08032023.pdf,"[OrderedDict([('id', 'attVX5StrhkYMHgb7'), ('width', 323), ('height', 366), ('url', 'https://v5.airtableusercontent.com/v1/18/18/1689537600000/B6LWKz3wPLDzQ6MMTmOckA/L88yLY306qe-Ys8p_aL7SSEQJyLbKznrk__98J6U5wSNrTiKR5WrBIjtuCV2GWOyvTVhfP3DnpGoG6nFPXM_0cTdmbFSjZURFxUJDtYs_V5j_Sch-Yni0YaKkITFxO_T/VpMQsW063m_G0HyFBe7j7JedLWtyk8ftQdLSC8KL-BI'), ('filename', 'Kinetix-mock-up-v4-copy.png'), ('size', 168825), ('type', 'image/png'), ('thumbnails', OrderedDict([('small', OrderedDict([('url', 'https://v5.airtableusercontent.com/v1/18/18/1689537600000/3gHsiCYUgMgtYgbc8Ic2Ww/7kKd2i5yrhWzhls6fLp9UM_zz4fdmqL0J-iExdO78K6rkHGsTuLdW627PU5aACCcTy_PW_9fuTmj53g7ckA6Sw/xx2WDNsJSPCHfzID1qQntmo70PFSYsm-lRsx92GEruk'), ('width', 32), ('height', 36)])), ('large', OrderedDict([('url', 'https://v5.airtableusercontent.com/v1/18/18/1689537600000/9dwP9Ch71aCYC-we2V6axA/JRItUsIu4-s_wl2kUFqQT2_msg748Zkgi3wP2DqvCD6QKpYZ8jYjrvC1y9jq2k3N1lu9NkVLPliFzhx8ZkI3CA/CEiyOo-kUNkyVOrTFr5ljSQpNfpaCHDGTWzZB0N_hFQ'), ('width', 323), ('height', 366)])), ('full', OrderedDict([('url', 'https://v5.airtableusercontent.com/v1/18/18/1689537600000/SEEnp1VBb9ourP_bRCnoFg/xzRhIuWz02bibCf04FWCw6IlZY4SJNbvlYFgujlgxyiqJ8CDdxfp1bYgFmS94pgZXT6JZDZD-DNBeqdytf9b_A/3kaETcIy5L51x3_xsRxurM8-hmgxs3M5TfLHThijxdc'), ('width', 3000), ('height', 3000)]))]))])]",https://www.photometrics.com/products/kinetix-family/kinetix,https://github.com/python-microscope/microscope/blob/master/microscope/cameras/pvcam.py,https://pylablib.readthedocs.io/en/latest/devices/Pvcam.html,,,,,"#!/usr/bin/env python3

## Copyright (C) 2009 David Baddeley <d.baddeley@auckland.ac.nz>
## Copyright (C) 2020 Mick Phillips <mick.phillips@gmail.com>
##
## This file is part of Microscope.
##
## Microscope is free software: you can redistribute it and/or modify
## it under the terms of the GNU General Public License as published by
## the Free Software Foundation, either version 3 of the License, or
## (at your option) any later version.
##
## Microscope is distributed in the hope that it will be useful,
## but WITHOUT ANY WARRANTY; without even the implied warranty of
## MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
## GNU General Public License for more details.
##
## You should have received a copy of the GNU General Public License
## along with Microscope.  If not, see <http://www.gnu.org/licenses/>.

## The implementation of dllFunc is based on the implementation in
## PYME, hence copyright to David Baddeley.

""""""pvcam library wrapper.

This module exposes pvcam C library functions in python.

.. todo::
   Support frame metadata.  The following functions are still not implemented::

    /*****************************************************************************/
    /*****************************************************************************/
    /*                                                                           */
    /*                         Frame metadata functions                          */
    /*                                                                           */
    /*****************************************************************************/
    /*****************************************************************************/

    /**
    Decodes all the raw frame buffer metadata into a friendly structure.
    @param pDstFrame A pre-allocated helper structure that will be filled with
                     information from the given raw buffer.
    @param pSrcBuf A raw frame buffer as retrieved from PVCAM
    @param srcBufSize The size of the raw frame buffer
    @return #PV_FAIL in case of failure.
    */
    rs_bool PV_DECL pl_md_frame_decode (md_frame* pDstFrame, void* pSrcBuf, uns32 srcBufSize);

    /**
    Optional function that recomposes a multi-ROI frame into a displayable image buffer.
    Every ROI will be copied into its appropriate location in the provided buffer.
    Please note that the function will subtract the Implied ROI position from each ROI
    position which essentially moves the entire Implied ROI to a [0, 0] position.
    Use the Offset arguments to shift all ROIs back to desired positions if needed.
    If you use the Implied ROI position for offset arguments the frame will be recomposed
    as it appears on the full frame.
    The caller is responsible for black-filling the input buffer. Usually this function
    is called during live/preview mode where the destination buffer is re-used. If the
    ROIs do move during acquisition it is essential to black-fill the destination buffer
    before calling this function. This is not needed if the ROIs do not move.
    If the ROIs move during live mode it is also recommended to use the offset arguments
    and recompose the ROI to a full frame - with moving ROIs the implied ROI may change
    with each frame and this may cause undesired ROI ""twitching"" in the displayable image.

    @param pDstBuf An output buffer, the buffer must be at least the size of the implied
                   ROI that is calculated during the frame decoding process. The buffer
                   must be of type uns16. If offset is set the buffer must be large
                   enough to allow the entire implied ROI to be shifted.
    @param offX    Offset in the destination buffer, in pixels. If 0 the Implied
                   ROI will be shifted to position 0 in the target buffer.
                   Use (ImpliedRoi.s1 / ImplierRoi.sbin) as offset value to
                   disable the shift and keep the ROIs in their absolute positions.
    @param offY    Offset in the destination buffer, in pixels. If 0 the Implied
                   ROI will be shifted to position 0 in the target buffer.
                   Use (ImpliedRoi.p1 / ImplierRoi.pbin) as offset value to
                   disable the shift and keep the ROIs in their absolute positions.
    @param dstWidth  Width, in pixels of the destination image buffer. The buffer
                     must be large enough to hold the entire Implied ROI, including
                     the offsets (if used).
    @param dstHeight Height, in pixels of the destination image buffer.
    @param pSrcFrame A helper structure, previously decoded using the frame
                     decoding function.
    @return #PV_FAIL in case of failure.
    */
    rs_bool PV_DECL pl_md_frame_recompose (void* pDstBuf, uns16 offX, uns16 offY,
                                           uns16 dstWidth, uns16 dstHeight,
                                           md_frame* pSrcFrame);

    /**
    This method creates an empty md_frame structure for known number of ROIs.
    Use this method to prepare and pre-allocate one structure before starting
    continous acquisition. Once callback arrives fill the structure with
    pl_md_frame_decode() and display the metadata.
    Release the structure when not needed.
    @param pFrame a pointer to frame helper structure address where the structure
                  will be allocated.
    @param roiCount Number of ROIs the structure should be prepared for.
    @return #PV_FAIL in case of failure.
    */
    rs_bool PV_DECL pl_md_create_frame_struct_cont (md_frame** pFrame, uns16 roiCount);

    /**
    This method creates an empty md_frame structure from an existing buffer.
    Use this method when loading buffers from disk or when performance is not
    critical. Do not forget to release the structure when not needed.
    For continous acquisition where the number or ROIs is known it is recommended
    to use the other provided method to avoid frequent memory allocation.
    @param pFrame A pointer address where the newly created structure will be stored.
    @param pSrcBuf A raw frame data pointer as returned from the camera
    @param srcBufSize Size of the raw frame data buffer
    @return #PV_FAIL in case of failure
    */
    rs_bool PV_DECL pl_md_create_frame_struct (md_frame** pFrame, void* pSrcBuf,
                                               uns32 srcBufSize);

    /**
    Releases the md_frame struct
    @param pFrame a pointer to the previously allocated structure
    */
    rs_bool PV_DECL pl_md_release_frame_struct (md_frame* pFrame);

    /**
    Reads all the extended metadata from the given ext. metadata buffer.
    @param pOutput A pre-allocated structure that will be filled with metadata
    @param pExtMdPtr A pointer to the ext. MD buffer, this can be obtained from
                    the md_frame and md_frame_roi structures.
    @param extMdSize Size of the ext. MD buffer, also retrievable from the helper
                     structures.
    @return #PV_FAIL in case the metadata cannot be decoded.
    */
    rs_bool PV_DECL pl_md_read_extended (md_ext_item_collection* pOutput, void* pExtMdPtr,
                                         uns32 extMdSize);

""""""

import ctypes
import logging
import os
import platform
import time
import weakref

import numpy as np
import Pyro4

import microscope
import microscope.abc


_logger = logging.getLogger(__name__)


# Readout transform mapping - {CHIP_NAME: {port: transform}}
READOUT_TRANSFORMS = {""Evolve-5"": {0: (0, 0, 0), 1: (1, 0, 0)}}

# === Data types ===
# Base typedefs, from pvcam SDK master.h
# typedef unsigned short rs_bool;
rs_bool = ctypes.c_ushort
# typedef signed char    int8;
int8 = ctypes.c_byte
# typedef unsigned char  uns8;
uns8 = ctypes.c_ubyte
# typedef short          int16;
int16 = ctypes.c_short
# typedef unsigned short uns16;
uns16 = ctypes.c_ushort
# typedef int            int32;
int32 = ctypes.c_int32
# typedef unsigned int   uns32;
uns32 = ctypes.c_uint32
# typedef float          flt32;
flt32 = ctypes.c_float
# typedef double         flt64;
flt64 = ctypes.c_double
# typedef unsigned long long ulong64;
ulong64 = ctypes.c_ulonglong
# typedef signed long long long64;
long64 = ctypes.c_longlong
# enums
enumtype = ctypes.c_int32


# defines, typedefs and enums parsed from pvcam.h .
MAX_CAM = 16
CAM_NAME_LEN = 32
PARAM_NAME_LEN = 32
ERROR_MSG_LEN = 255
CCD_NAME_LEN = 17
MAX_ALPHA_SER_NUM_LEN = 32
MAX_PP_NAME_LEN = 32
MAX_SYSTEM_NAME_LEN = 32
MAX_VENDOR_NAME_LEN = 32
MAX_PRODUCT_NAME_LEN = 32
MAX_CAM_PART_NUM_LEN = 32
MAX_GAIN_NAME_LEN = 32
OPEN_EXCLUSIVE = 0
NORMAL_COOL = 0
CRYO_COOL = 1
MPP_UNKNOWN = 0
MPP_ALWAYS_OFF = 1
MPP_ALWAYS_ON = 2
MPP_SELECTABLE = 3
SHTR_FAULT = 0
SHTR_OPENING = 1
SHTR_OPEN = 2
SHTR_CLOSING = 3
SHTR_CLOSED = 4
SHTR_UNKNOWN = 5
PMODE_NORMAL = 0
PMODE_FT = 1
PMODE_MPP = 2
PMODE_FT_MPP = 3
PMODE_ALT_NORMAL = 4
PMODE_ALT_FT = 5
PMODE_ALT_MPP = 6
PMODE_ALT_FT_MPP = 7
COLOR_NONE = 0
COLOR_RESERVED = 1
COLOR_RGGB = 2
COLOR_GRBG = 3
COLOR_GBRG = 4
COLOR_BGGR = 5
ATTR_CURRENT = 0
ATTR_COUNT = 1
ATTR_TYPE = 2
ATTR_MIN = 3
ATTR_MAX = 4
ATTR_DEFAULT = 5
ATTR_INCREMENT = 6
ATTR_ACCESS = 7
ATTR_AVAIL = 8
ACC_READ_ONLY = 1
ACC_READ_WRITE = 2
ACC_EXIST_CHECK_ONLY = 3
ACC_WRITE_ONLY = 4
IO_TYPE_TTL = 0
IO_TYPE_DAC = 1
IO_DIR_INPUT = 0
IO_DIR_OUTPUT = 1
IO_DIR_INPUT_OUTPUT = 2
READOUT_PORT_0 = 0
READOUT_PORT_1 = 1
CLEAR_NEVER = 0
CLEAR_PRE_EXPOSURE = 1
CLEAR_PRE_SEQUENCE = 2
CLEAR_POST_SEQUENCE = 3
CLEAR_PRE_POST_SEQUENCE = 4
CLEAR_PRE_EXPOSURE_POST_SEQ = 5
MAX_CLEAR_MODE = 6
OPEN_NEVER = 0
OPEN_PRE_EXPOSURE = 1
OPEN_PRE_SEQUENCE = 2
OPEN_PRE_TRIGGER = 3
OPEN_NO_CHANGE = 4
TIMED_MODE = 0
STROBED_MODE = 1
BULB_MODE = 2
TRIGGER_FIRST_MODE = 3
FLASH_MODE = 4
VARIABLE_TIMED_MODE = 5
INT_STROBE_MODE = 6
MAX_EXPOSE_MODE = 7
Extended = 8
camera = 9
The = 10
definition = 11
EXT_TRIG_INTERNAL = 12
EXT_TRIG_TRIG_FIRST = 13
EXT_TRIG_EDGE_RISING = 14
EXPOSE_OUT_FIRST_ROW = 0
EXPOSE_OUT_ALL_ROWS = 1
EXPOSE_OUT_ANY_ROW = 2
MAX_EXPOSE_OUT_MODE = 3
FAN_SPEED_HIGH = 0
FAN_SPEED_MEDIUM = 1
FAN_SPEED_LOW = 2
FAN_SPEED_OFF = 3
PL_TRIGTAB_SIGNAL_EXPOSE_OUT = 0
PP_FEATURE_RING_FUNCTION = 0
PP_FEATURE_BIAS = 1
PP_FEATURE_BERT = 2
PP_FEATURE_QUANT_VIEW = 3
PP_FEATURE_BLACK_LOCK = 4
PP_FEATURE_TOP_LOCK = 5
PP_FEATURE_VARI_BIT = 6
PP_FEATURE_RESERVED = 7
PP_FEATURE_DESPECKLE_BRIGHT_HIGH = 8
PP_FEATURE_DESPECKLE_DARK_LOW = 9
PP_FEATURE_DEFECTIVE_PIXEL_CORRECTION = 10
PP_FEATURE_DYNAMIC_DARK_FRAME_CORRECTION = 11
PP_FEATURE_HIGH_DYNAMIC_RANGE = 12
PP_FEATURE_DESPECKLE_BRIGHT_LOW = 13
PP_FEATURE_DENOISING = 14
PP_FEATURE_DESPECKLE_DARK_HIGH = 15
PP_FEATURE_ENHANCED_DYNAMIC_RANGE = 16
PP_FEATURE_MAX = 17
PP_MAX_PARAMETERS_PER_FEATURE = 10
PP_PARAMETER_RF_FUNCTION = 0
PP_FEATURE_BIAS_ENABLED = 1
PP_FEATURE_BIAS_LEVEL = 2
PP_FEATURE_BERT_ENABLED = 3
PP_FEATURE_BERT_THRESHOLD = 4
PP_FEATURE_QUANT_VIEW_ENABLED = 5
PP_FEATURE_QUANT_VIEW_E = 6
PP_FEATURE_BLACK_LOCK_ENABLED = 7
PP_FEATURE_BLACK_LOCK_BLACK_CLIP = 8
PP_FEATURE_TOP_LOCK_ENABLED = 9
PP_FEATURE_TOP_LOCK_WHITE_CLIP = 10
PP_FEATURE_VARI_BIT_ENABLED = 11
PP_FEATURE_VARI_BIT_BIT_DEPTH = 12
PP_FEATURE_DESPECKLE_BRIGHT_HIGH_ENABLED = 13
PP_FEATURE_DESPECKLE_BRIGHT_HIGH_THRESHOLD = 14
PP_FEATURE_DESPECKLE_BRIGHT_HIGH_MIN_ADU_AFFECTED = 15
PP_FEATURE_DESPECKLE_DARK_LOW_ENABLED = 16
PP_FEATURE_DESPECKLE_DARK_LOW_THRESHOLD = 17
PP_FEATURE_DESPECKLE_DARK_LOW_MAX_ADU_AFFECTED = 18
PP_FEATURE_DEFECTIVE_PIXEL_CORRECTION_ENABLED = 19
PP_FEATURE_DYNAMIC_DARK_FRAME_CORRECTION_ENABLED = 20
PP_FEATURE_HIGH_DYNAMIC_RANGE_ENABLED = 21
PP_FEATURE_DESPECKLE_BRIGHT_LOW_ENABLED = 22
PP_FEATURE_DESPECKLE_BRIGHT_LOW_THRESHOLD = 23
PP_FEATURE_DESPECKLE_BRIGHT_LOW_MAX_ADU_AFFECTED = 24
PP_FEATURE_DENOISING_ENABLED = 25
PP_FEATURE_DENOISING_NO_OF_ITERATIONS = 26
PP_FEATURE_DENOISING_GAIN = 27
PP_FEATURE_DENOISING_OFFSET = 28
PP_FEATURE_DENOISING_LAMBDA = 29
PP_FEATURE_DESPECKLE_DARK_HIGH_ENABLED = 30
PP_FEATURE_DESPECKLE_DARK_HIGH_THRESHOLD = 31
PP_FEATURE_DESPECKLE_DARK_HIGH_MIN_ADU_AFFECTED = 32
PP_FEATURE_ENHANCED_DYNAMIC_RANGE_ENABLED = 33
PP_PARAMETER_ID_MAX = 34
SMTMODE_ARBITRARY_ALL = 0
SMTMODE_MAX = 1
READOUT_NOT_ACTIVE = 0
EXPOSURE_IN_PROGRESS = 1
READOUT_IN_PROGRESS = 2
READOUT_COMPLETE = 3
FRAME_AVAILABLE = 3
READOUT_FAILED = 4
ACQUISITION_IN_PROGRESS = 5
MAX_CAMERA_STATUS = 6
CCS_NO_CHANGE = 0
CCS_HALT = 1
CCS_HALT_CLOSE_SHTR = 2
CCS_CLEAR = 3
CCS_CLEAR_CLOSE_SHTR = 4
CCS_OPEN_SHTR = 5
CCS_CLEAR_OPEN_SHTR = 6
NO_FRAME_IRQS = 0
BEGIN_FRAME_IRQS = 1
END_FRAME_IRQS = 2
BEGIN_END_FRAME_IRQS = 3
CIRC_NONE = 0
CIRC_OVERWRITE = 1
CIRC_NO_OVERWRITE = 2
EXP_RES_ONE_MILLISEC = 0
EXP_RES_ONE_MICROSEC = 1
EXP_RES_ONE_SEC = 2
SCR_PRE_OPEN_SHTR = 0
SCR_POST_OPEN_SHTR = 1
SCR_PRE_FLASH = 2
SCR_POST_FLASH = 3
SCR_PRE_INTEGRATE = 4
SCR_POST_INTEGRATE = 5
SCR_PRE_READOUT = 6
SCR_POST_READOUT = 7
SCR_PRE_CLOSE_SHTR = 8
SCR_POST_CLOSE_SHTR = 9
PL_CALLBACK_BOF = 0
PL_CALLBACK_EOF = 1
PL_CALLBACK_CHECK_CAMS = 2
PL_CALLBACK_CAM_REMOVED = 3
PL_CALLBACK_CAM_RESUMED = 4
PL_CALLBACK_MAX = 5
PL_MD_FRAME_FLAG_ROI_TS_SUPPORTED = 1
PL_MD_FRAME_FLAG_UNUSED_2 = 2
PL_MD_FRAME_FLAG_UNUSED_3 = 4
PL_MD_FRAME_FLAG_UNUSED_4 = 16
PL_MD_FRAME_FLAG_UNUSED_5 = 32
PL_MD_FRAME_FLAG_UNUSED_6 = 64
PL_MD_FRAME_FLAG_UNUSED_7 = 128
PL_MD_ROI_FLAG_INVALID = 1
PL_MD_ROI_FLAG_UNUSED_2 = 2
PL_MD_ROI_FLAG_UNUSED_3 = 4
PL_MD_ROI_FLAG_UNUSED_4 = 16
PL_MD_ROI_FLAG_UNUSED_5 = 32
PL_MD_ROI_FLAG_UNUSED_6 = 64
PL_MD_ROI_FLAG_UNUSED_7 = 128
PL_MD_FRAME_SIGNATURE = 5328208
PL_MD_EXT_TAGS_MAX_SUPPORTED = 255
PL_MD_EXT_TAG_MAX = 0
TYPE_INT16 = 1
TYPE_INT32 = 2
TYPE_FLT64 = 4
TYPE_UNS8 = 5
TYPE_UNS16 = 6
TYPE_UNS32 = 7
TYPE_UNS64 = 8
TYPE_ENUM = 9
TYPE_BOOLEAN = 11
TYPE_INT8 = 12
TYPE_CHAR_PTR = 13
TYPE_VOID_PTR = 14
TYPE_VOID_PTR_PTR = 15
TYPE_INT64 = 16
TYPE_SMART_STREAM_TYPE = 17
TYPE_SMART_STREAM_TYPE_PTR = 18
TYPE_FLT32 = 19
CLASS0 = 0
CLASS2 = 2
CLASS3 = 3
PARAM_DD_INFO_LENGTH = 16777217
PARAM_DD_VERSION = 100663298
PARAM_DD_RETRIES = 100663299
PARAM_DD_TIMEOUT = 100663300
PARAM_DD_INFO = 218103813
PARAM_ADC_OFFSET = 16908483
PARAM_CHIP_NAME = 218235009
PARAM_SYSTEM_NAME = 218235010
PARAM_VENDOR_NAME = 218235011
PARAM_PRODUCT_NAME = 218235012
PARAM_CAMERA_PART_NUMBER = 218235013
PARAM_COOLING_MODE = 151126230
PARAM_PREAMP_DELAY = 100794870
PARAM_COLOR_MODE = 151126520
PARAM_MPP_CAPABLE = 151126240
PARAM_PREAMP_OFF_CONTROL = 117572091
PARAM_PREMASK = 100794421
PARAM_PRESCAN = 100794423
PARAM_POSTMASK = 100794422
PARAM_POSTSCAN = 100794424
PARAM_PIX_PAR_DIST = 100794868
PARAM_PIX_PAR_SIZE = 100794431
PARAM_PIX_SER_DIST = 100794869
PARAM_PIX_SER_SIZE = 100794430
PARAM_SUMMING_WELL = 184680953
PARAM_FWELL_CAPACITY = 117572090
PARAM_PAR_SIZE = 100794425
PARAM_SER_SIZE = 100794426
PARAM_ACCUM_CAPABLE = 184680986
PARAM_FLASH_DWNLD_CAPABLE = 184680987
PARAM_READOUT_TIME = 67240115
PARAM_CLEAR_CYCLES = 100794465
PARAM_CLEAR_MODE = 151126539
PARAM_FRAME_CAPABLE = 184680957
PARAM_PMODE = 151126540
PARAM_TEMP = 16908813
PARAM_TEMP_SETPOINT = 16908814
PARAM_CAM_FW_VERSION = 100794900
PARAM_HEAD_SER_NUM_ALPHA = 218235413
PARAM_PCI_FW_VERSION = 100794902
PARAM_FAN_SPEED_SETPOINT = 151126726
PARAM_EXPOSURE_MODE = 151126551
PARAM_EXPOSE_OUT_MODE = 151126576
PARAM_BIT_DEPTH = 16908799
PARAM_GAIN_INDEX = 16908800
PARAM_SPDTAB_INDEX = 16908801
PARAM_GAIN_NAME = 218235394
PARAM_READOUT_PORT = 151126263
PARAM_PIX_TIME = 100794884
PARAM_SHTR_CLOSE_DELAY = 100794887
PARAM_SHTR_OPEN_DELAY = 100794888
PARAM_SHTR_OPEN_MODE = 151126537
PARAM_SHTR_STATUS = 151126538
PARAM_IO_ADDR = 100794895
PARAM_IO_TYPE = 151126544
PARAM_IO_DIRECTION = 151126545
PARAM_IO_STATE = 67240466
PARAM_IO_BITDEPTH = 100794899
PARAM_GAIN_MULT_FACTOR = 100794905
PARAM_GAIN_MULT_ENABLE = 184680989
PARAM_PP_FEAT_NAME = 218235422
PARAM_PP_INDEX = 16908831
PARAM_ACTUAL_GAIN = 100794912
PARAM_PP_PARAM_INDEX = 16908833
PARAM_PP_PARAM_NAME = 218235426
PARAM_PP_PARAM = 117572131
PARAM_READ_NOISE = 100794916
PARAM_PP_FEAT_ID = 100794917
PARAM_PP_PARAM_ID = 100794918
PARAM_SMART_STREAM_MODE_ENABLED = 184681148
PARAM_SMART_STREAM_MODE = 100795069
PARAM_SMART_STREAM_EXP_PARAMS = 235012798
PARAM_SMART_STREAM_DLY_PARAMS = 235012799
PARAM_EXP_TIME = 100859905
PARAM_EXP_RES = 151191554
PARAM_EXP_RES_INDEX = 100859908
PARAM_EXPOSURE_TIME = 134414344
PARAM_BOF_EOF_ENABLE = 151191557
PARAM_BOF_EOF_COUNT = 117637126
PARAM_BOF_EOF_CLR = 184745991
PARAM_CIRC_BUFFER = 184746283
PARAM_FRAME_BUFFER_SIZE = 134414636
PARAM_BINNING_SER = 151191717
PARAM_BINNING_PAR = 151191718
PARAM_METADATA_ENABLED = 184746152
PARAM_ROI_COUNT = 100860073
PARAM_CENTROIDS_ENABLED = 184746154
PARAM_CENTROIDS_RADIUS = 100860075
PARAM_CENTROIDS_COUNT = 100860076
PARAM_TRIGTAB_SIGNAL = 151191732
PARAM_LAST_MUXED_SIGNAL = 84082869


# === C structures ===
# GUID for #FRAME_INFO structure.
class PVCAM_FRAME_INFO_GUID(ctypes.Structure):
    _fields_ = [
        (""f1"", uns32),
        (""f2"", uns16),
        (""f3"", uns16),
        (""f4"", uns8 * 8),
    ]


# Structure used to uniquely identify frames in the camera.
class FRAME_INFO(ctypes.Structure):
    _fields_ = [
        (""FrameInfoGUID"", PVCAM_FRAME_INFO_GUID),
        (""hCam"", int16),
        (""FrameNr"", int32),
        (""TimeStamp"", long64),
        (""ReadoutTime"", int32),
        (""TimeStampBOF"", long64),
    ]


class smart_stream_type(ctypes.Structure):
    _fields_ = [
        (""entries"", uns16),
        (""params"", uns32),
    ]


class rgn_type(ctypes.Structure):
    _fields_ = [
        (""s1"", uns16),
        (""s2"", uns16),
        (""sbin"", uns16),
        (""p1"", uns16),
        (""p2"", uns16),
        (""pbin"", uns16),
    ]


class io_struct(ctypes.Structure):
    pass


io_struct._fields_ = [
    (""io_port"", uns16),
    (""io_type"", uns32),
    (""state"", flt64),
    (""next"", ctypes.POINTER(io_struct)),
]


class io_list(ctypes.Structure):
    _fields_ = [
        (""pre_open"", ctypes.POINTER(io_struct)),
        (""post_open"", ctypes.POINTER(io_struct)),
        (""pre_flash"", ctypes.POINTER(io_struct)),
        (""post_flash"", ctypes.POINTER(io_struct)),
        (""pre_integrate"", ctypes.POINTER(io_struct)),
        (""post_integrate"", ctypes.POINTER(io_struct)),
        (""pre_readout"", ctypes.POINTER(io_struct)),
        (""post_readout"", ctypes.POINTER(io_struct)),
        (""pre_close"", ctypes.POINTER(io_struct)),
        (""post_close"", ctypes.POINTER(io_struct)),
    ]


class active_camera_type(ctypes.Structure):
    _fields_ = [
        (""shutter_close_delay"", uns16),
        (""shutter_open_delay"", uns16),
        (""rows"", uns16),
        (""cols"", uns16),
        (""prescan"", uns16),
        (""postscan"", uns16),
        (""premask"", uns16),
        (""postmask"", uns16),
        (""preflash"", uns16),
        (""clear_count"", uns16),
        (""preamp_delay"", uns16),
        (""mpp_selectable"", rs_bool),
        (""frame_selectable"", rs_bool),
        (""do_clear"", uns16),
        (""open_shutter"", uns16),
        (""mpp_mode"", rs_bool),
        (""frame_transfer"", rs_bool),
        (""alt_mode"", rs_bool),
        (""exp_res"", uns32),
        (""io_hdr"", ctypes.POINTER(io_list)),
    ]


class md_frame_header(ctypes.Structure):
    _fields_ = [
        (""signature"", uns32),
        (""version"", uns8),
        (""frameNr"", uns32),
        (""roiCount"", uns16),
        (""timestampBOF"", uns32),
        (""timestampEOF"", uns32),
        (""timestampResNs"", uns32),
        (""exposureTime"", uns32),
        (""exposureTimeResN"", uns32),
        (""roiTimestampResN"", uns32),
        (""bitDepth"", uns8),
        (""colorMask"", uns8),
        (""flags"", uns8),
        (""extendedMdSize"", uns16),
        (""_reserved"", uns8 * 8),
    ]


class md_frame_roi_header(ctypes.Structure):
    _fields_ = [
        (""roiNr"", uns16),
        (""timestampBOR"", uns32),
        (""timestampEOR"", uns32),
        (""roi"", rgn_type),
        (""flags"", uns8),
        (""extendedMdSize"", uns16),
        (""_reserved"", uns8 * 7),
    ]


class md_ext_item_info(ctypes.Structure):
    _fields_ = [
        (""tag"", uns16),
        (""size"", uns16),
        (""name"", ctypes.c_char_p),
    ]


class md_ext_item(ctypes.Structure):
    _fields_ = [
        (""tagInfo"", ctypes.POINTER(md_ext_item_info)),  #
        (""value"", ctypes.c_void_p),
    ]


class md_ext_item_collection(ctypes.Structure):
    _fields_ = [
        (""list"", md_ext_item * PL_MD_EXT_TAGS_MAX_SUPPORTED),
        (""map"", ctypes.POINTER(md_ext_item) * PL_MD_EXT_TAGS_MAX_SUPPORTED),
        (""count"", uns16),
    ]


class md_frame_roi(ctypes.Structure):
    _fields_ = [
        (""header"", ctypes.POINTER(md_frame_roi_header)),
        (""data"", ctypes.c_void_p),
        (""dataSize"", uns32),
        (""extMdData"", ctypes.c_void_p),
        (""extMdDataSize"", uns16),
    ]


class md_frame(ctypes.Structure):
    _fields_ = [
        (""header"", ctypes.POINTER(md_frame_header)),
        (""extMdData"", ctypes.c_void_p),
        (""extMdDataSize"", uns16),
        (""impliedRoi"", rgn_type),
        (""roiArray"", ctypes.POINTER(md_frame_roi)),
        (""roiCapacity"", uns16),
        (""roiCount"", uns16),
    ]


if os.name == ""nt"":  # is windows
    if platform.architecture()[0] == ""32bit"":
        _lib = ctypes.WinDLL(""pvcam32"")
    else:
        _lib = ctypes.WinDLL(""pvcam64"")
else:
    _lib = ctypes.CDLL(""pvcam.so"")

### Functions ###
STRING = ctypes.c_char_p


# classes so that we do some magic and automatically add byrefs etc ... can classify outputs
# (Nicked from PYME's Ixon wrapper.)
class _meta:
    pass


class OUTPUT(_meta):
    def __init__(self, val):
        self.type = val
        self.val = ctypes.POINTER(val)

    def get_var(self, buf_len=0):
        if self.type in [STRING, ctypes.c_void_p] and buf_len > 0:
            v = ctypes.create_string_buffer(buf_len)
            ref = ctypes.cast(ctypes.pointer(v), self.val)
        else:
            v = self.type()
            ref = ctypes.byref(v)
        return v, ref


class _OUTSTRING(OUTPUT):
    def __init__(self):
        self.val = STRING

    def get_var(self, buf_len):
        v = ctypes.create_string_buffer(buf_len)
        return v, v


OUTSTRING = _OUTSTRING()


def stripMeta(val):
    """"""Strip meta info from OUTPUT and OUTSTRING instances.""""""
    if isinstance(val, _meta):
        return val.val
    else:
        return val


# Function type for callbacks.
CALLBACK = ctypes.CFUNCTYPE(ctypes.c_void_p)


class dllFunction:
    """"""Expose a DLL function to python.

    (Again, largely nicked from PYME.)""""""

    def __init__(self, name, args=[], argnames=[], buf_len=-1, lib=_lib):
        self.f = getattr(lib, name)
        self.f.restype = rs_bool
        self.f.argtypes = [stripMeta(a) for a in args]

        self.fargs = args
        self.fargnames = argnames
        self.name = name

        self.inp = [not isinstance(a, OUTPUT) for a in args]
        self.in_args = [a for a in args if not isinstance(a, OUTPUT)]
        self.out_args = [a for a in args if isinstance(a, OUTPUT)]

        self.buf_len = buf_len

        docstring = name + ""\n\nArguments:\n===========\n""
        for i in range(len(args)):
            an = """"
            if i < len(argnames):
                an = argnames[i]
            docstring += ""\t%s\t%s\n"" % (args[i], an)

        self.f.__doc__ = docstring

    def __call__(self, *args, **kwargs):
        ars = []
        i = 0
        ret = []
        # pl_get_param buffer length depends on the parameter being fetched, so
        # use kwargs to pass buffer length.
        if ""buf_len"" in kwargs:
            bs = kwargs[""buf_len""]
        elif self.name == ""pl_get_enum_param"":
            # last argument is buffer length
            bs = args[-1]
        elif self.buf_len >= 0:
            bs = self.buf_len
        else:
            bs = 256
        # May have been passed a ctype; if so, fetch its value.
        if isinstance(bs, ctypes._SimpleCData):
            bs = bs.value

        for j in range(len(self.inp)):
            if self.inp[j]:  # an input
                if self.f.argtypes[j] is CALLBACK and not isinstance(
                    args[i], CALLBACK
                ):
                    ars.append(CALLBACK(args[i]))
                else:
                    ars.append(args[i])
                i += 1
            else:  # an output
                r, ar = self.fargs[j].get_var(bs)
                ars.append(ar)
                ret.append(r)
                # print r, r._type_

        # print (self.name, ars)
        res = self.f(*ars)
        # print res

        if res == False:
            err_code = _lib.pl_error_code()
            err_msg = ctypes.create_string_buffer(ERROR_MSG_LEN)
            _lib.pl_error_message(err_code, err_msg)
            raise microscope.DeviceError(
                ""pvcam error %d: %s"" % (err_code, err_msg.value)
            )

        if len(ret) == 0:
            return None
        if len(ret) == 1:
            return ret[0]
        else:
            return ret


def _status():
    """"""Fetch the PVCAM DLL status.""""""
    err_code = _lib.pl_error_code()
    err_msg = ctypes.create_string_buffer(ERROR_MSG_LEN)
    _lib.pl_error_message(err_code, err_msg)


def dllFunc(name, args=[], argnames=[], buf_len=0):
    """"""Register a function using dllFunction.""""""
    f = dllFunction(name, args, argnames, buf_len=buf_len)
    globals()[name[2:]] = f


""""""DLL function imports.""""""
# Class 0 functions - library
dllFunc(""pl_pvcam_get_ver"", [OUTPUT(uns16)], [""version""])
dllFunc(""pl_pvcam_init"")
dllFunc(""pl_pvcam_uninit"")
# Class 0 functions - camera
dllFunc(""pl_cam_close"", [int16], [""hcam""])
dllFunc(
    ""pl_cam_get_name"",
    [int16, OUTSTRING],
    [""can_num"", ""cam_name""],
    buf_len=CAM_NAME_LEN,
)
dllFunc(
    ""pl_cam_get_total"",
    [
        OUTPUT(int16),
    ],
    [
        ""total_cams"",
    ],
)
dllFunc(
    ""pl_cam_open"",
    [STRING, OUTPUT(int16), int16],
    [""cam_name"", ""hcam"", ""o_mode""],
)
dllFunc(
    ""pl_cam_register_callback"",
    [int16, int32, CALLBACK],
    [""hcam"", ""event"", ""Callback""],
)
dllFunc(
    ""pl_cam_register_callback_ex"",
    [int16, int32, CALLBACK, ctypes.c_void_p],
    [""hcam"", ""event"", ""Callback"", ""Context""],
)
dllFunc(
    ""pl_cam_register_callback_ex2"",
    [int16, int32, CALLBACK],
    [""hcam"", ""event"", ""Callback""],
)
dllFunc(
    ""pl_cam_register_callback_ex3"",
    [int16, int32, CALLBACK, ctypes.c_void_p],
    [""hcam"", ""event"", ""Callback"", ""Context""],
)
dllFunc(
    ""pl_cam_deregister_callback"", [int16, ctypes.c_void_p], [""hcam"", ""event""]
)
# Class 1 functions - error handling. Handled in dllFunction.
# Class 2 functions - configuration/setup.
dllFunc(
    ""pl_get_param"",
    [int16, uns32, int16, OUTPUT(ctypes.c_void_p)],
    [""hcam"", ""param_id"", ""param_attrib"", ""param_value""],
)
dllFunc(
    ""pl_set_param"",
    [int16, uns32, ctypes.c_void_p],
    [""hcam"", ""param_id"", ""param_value""],
)
dllFunc(
    ""pl_get_enum_param"",
    [int16, uns32, uns32, OUTPUT(int32), OUTSTRING, uns32],
    [""hcam"", ""param_id"", ""index"", ""value"", ""desc"", ""length""],
)
dllFunc(
    ""pl_enum_str_length"",
    [int16, uns32, uns32, OUTPUT(uns32)],
    [""hcam"", ""param_id"", ""index"", ""length""],
)
dllFunc(
    ""pl_pp_reset"",
    [
        int16,
    ],
    [""hcam""],
)
dllFunc(
    ""pl_create_smart_stream_struct"",
    [OUTPUT(smart_stream_type), uns16],
    [""pSmtStruct"", ""entries""],
)
dllFunc(
    ""pl_release_smart_stream_struct"",
    [
        ctypes.POINTER(smart_stream_type),
    ],
    [
        ""pSmtStruct"",
    ],
)
dllFunc(
    ""pl_create_frame_info_struct"",
    [
        OUTPUT(FRAME_INFO),
    ],
    [""pNewFrameInfo""],
)
dllFunc(
    ""pl_release_frame_info_struct"",
    [
        ctypes.POINTER(FRAME_INFO),
    ],
    [
        ""pFrameInfoToDel"",
    ],
)
dllFunc(""pl_exp_abort"", [int16, int16], [""hcam"", ""cam_state""])
dllFunc(
    ""pl_exp_setup_seq"",
    [
        int16,
        uns16,
        uns16,
        ctypes.POINTER(rgn_type),
        int16,
        uns32,
        OUTPUT(uns32),
    ],
    [
        ""hcam"",
        ""exp_total"",
        ""rgn_total"",
        ""rgn_array"",
        ""exp_mode"",
        ""exposure_time"",
        ""exp_bytes"",
    ],
)
dllFunc(""pl_exp_start_seq"", [int16, ctypes.c_void_p], [""hcam"", ""pixel_stream""])
dllFunc(
    ""pl_exp_setup_cont"",
    [
        int16,
        uns16,
        ctypes.POINTER(rgn_type),
        int16,
        uns32,
        OUTPUT(uns32),
        int16,
    ],
    [
        ""hcam"",
        ""rgn_total"",
        ""rgn_array"",
        ""exp_mode"",
        ""exposure_time"",
        ""exp_bytes"",
        ""buffer_mode"",
    ],
)
dllFunc(
    ""pl_exp_start_cont"",
    [int16, ctypes.c_void_p, uns32],
    [""hcam"", ""pixel_stream"", ""size""],
)
dllFunc(
    ""pl_exp_check_status"",
    [int16, OUTPUT(int16), OUTPUT(uns32)],
    [""hcam"", ""status"", ""bytes_arrived""],
)
dllFunc(
    ""pl_exp_check_cont_status"",
    [int16, OUTPUT(int16), OUTPUT(uns32), OUTPUT(uns32)],
    [""hcam"", ""status"", ""bytes_arrived"", ""buffer_cnt""],
)
dllFunc(
    ""pl_exp_check_cont_status_ex"",
    [
        int16,
        OUTPUT(int16),
        OUTPUT(uns32),
        OUTPUT(uns32),
        ctypes.POINTER(FRAME_INFO),
    ],
    [""hcam"", ""status"", ""byte_cnt"", ""buffer_cnt"", ""pFrameInfo""],
)
dllFunc(
    ""pl_exp_get_latest_frame"",
    [int16, OUTPUT(ctypes.c_void_p)],
    [""hcam"", ""frame""],
)
dllFunc(
    ""pl_exp_get_latest_frame_ex"",
    [int16, OUTPUT(ctypes.c_void_p), ctypes.POINTER(FRAME_INFO)],
    [""hcam"", ""frame"", ""pFrameInfo""],
)
dllFunc(
    ""pl_exp_get_oldest_frame"",
    [int16, OUTPUT(ctypes.c_void_p)],
    [""hcam"", ""frame""],
)
dllFunc(
    ""pl_exp_get_oldest_frame_ex"",
    [int16, OUTPUT(ctypes.c_void_p), ctypes.POINTER(FRAME_INFO)],
    [""hcam"", ""frame"", ""pFrameInfo""],
)
dllFunc(""pl_exp_unlock_oldest_frame"", [int16], [""hcam""])
dllFunc(""pl_exp_stop_cont"", [int16, int16], [""hcam"", ""cam_state""])
dllFunc(""pl_exp_abort"", [int16, int16], [""hcam"", ""cam_state""])
dllFunc(
    ""pl_exp_finish_seq"", [int16, ctypes.c_void_p], [""hcam"", ""pixel_stream""]
)


# Map ATTR_ enums to the return type for that ATTR.
_attr_map = {
    ATTR_ACCESS: uns16,
    ATTR_AVAIL: rs_bool,
    ATTR_COUNT: uns32,
    ATTR_CURRENT: None,
    ATTR_DEFAULT: None,
    ATTR_INCREMENT: None,
    ATTR_MAX: None,
    ATTR_MIN: None,
    ATTR_TYPE: uns16,
}

# Map TYPE enums to their type.
_typemap = {
    TYPE_INT16: int16,
    TYPE_INT32: int32,
    TYPE_FLT64: flt64,
    TYPE_UNS8: uns8,
    TYPE_UNS16: uns16,
    TYPE_UNS32: uns32,
    TYPE_UNS64: ulong64,
    TYPE_ENUM: int32,  # from SDK documentation
    TYPE_BOOLEAN: rs_bool,
    TYPE_INT8: int8,
    TYPE_CHAR_PTR: ctypes.c_char_p,
    TYPE_VOID_PTR: ctypes.c_void_p,
    TYPE_VOID_PTR_PTR: ctypes.POINTER(ctypes.c_void_p),
    TYPE_INT64: long64,
    TYPE_SMART_STREAM_TYPE: smart_stream_type,
    TYPE_SMART_STREAM_TYPE_PTR: ctypes.POINTER(smart_stream_type),
    TYPE_FLT32: flt32,
}


# Map TYPE enums to the appropriate setting dtype.
_dtypemap = {
    TYPE_INT16: ""int"",
    TYPE_INT32: ""int"",
    TYPE_FLT64: ""float"",
    TYPE_UNS8: ""int"",
    TYPE_UNS16: ""int"",
    TYPE_UNS32: ""int"",
    TYPE_UNS64: ""int"",
    TYPE_ENUM: ""enum"",
    TYPE_BOOLEAN: ""bool"",
    TYPE_INT8: ""int"",
    TYPE_CHAR_PTR: ""str"",
    TYPE_VOID_PTR: None,
    TYPE_VOID_PTR_PTR: None,
    TYPE_INT64: ""int"",
    TYPE_SMART_STREAM_TYPE: None,
    TYPE_SMART_STREAM_TYPE_PTR: None,
    TYPE_FLT32: ""float"",
}

# Mapping of param ids to maximum string lengths.
# PARAM_DD_INFO is a variable length string, and its length can be found by
# querying PARAM_DD_INFO_LEN. However, querying PARAM_DD_INFO frequently causes
# a general protection fault in the DLL, regardless of buffer length.
_length_map = {
    PARAM_DD_INFO: PARAM_DD_INFO_LENGTH,
    PARAM_CHIP_NAME: CCD_NAME_LEN,
    PARAM_SYSTEM_NAME: MAX_SYSTEM_NAME_LEN,
    PARAM_VENDOR_NAME: MAX_VENDOR_NAME_LEN,
    PARAM_PRODUCT_NAME: MAX_PRODUCT_NAME_LEN,
    PARAM_CAMERA_PART_NUMBER: MAX_CAM_PART_NUM_LEN,
    PARAM_GAIN_NAME: MAX_GAIN_NAME_LEN,
    PARAM_HEAD_SER_NUM_ALPHA: MAX_ALPHA_SER_NUM_LEN,
    PARAM_PP_FEAT_NAME: MAX_PP_NAME_LEN,
    PARAM_PP_PARAM_NAME: MAX_PP_NAME_LEN,
}

# map PARAM enums to the parameter name
_param_to_name = {
    globals()[param]: param
    for param in globals()
    if (param.startswith(""PARAM_"") and param != ""PARAM_NAME_LEN"")
}


def get_param_type(param_id):
    """"""Return parameter type code (for C/DLL) for param_id.""""""
    # Parameter types are encoded in the 4th MSB of the param_id.
    return _typemap[param_id >> 24 & 255]


def get_param_dtype(param_id):
    """"""Return parameter dtype (for microscope settings) for param_id.""""""
    # Parameter types are encoded in the 4th MSB of the param_id.
    return _dtypemap[param_id >> 24 & 255]


# Map status codes to strings.
STATUS_STRINGS = {
    READOUT_NOT_ACTIVE: ""READOUT_NOT_ACTIVE"",
    EXPOSURE_IN_PROGRESS: ""EXPOSURE_IN_PROGRESS"",
    READOUT_IN_PROGRESS: ""READOUT_IN_PROGRESS"",
    READOUT_COMPLETE: ""READOUT_COMPLETE"",
    READOUT_FAILED: ""READOUT_FAILED"",
    FRAME_AVAILABLE: ""FRAME_AVAILABLE"",
}


# === Python classes ===
# Trigger modes.
class TriggerMode:
    """"""A microscope trigger mode using PVCAM PMODES.""""""

    def __init__(self, label, pv_mode):
        self.label = label
        self.pv_mode = pv_mode

    def __repr__(self):
        return ""<%s: '%s'>"" % (type(self).__name__, self.label)


# Enumerate trigger types.
(
    TRIG_SOFT,
    TRIG_TIMED,
    TRIG_VARIABLE,
    TRIG_FIRST,
    TRIG_STROBED,
    TRIG_BULB,
) = range(6)

# Trigger mode definitions.
TRIGGER_MODES = {
    TRIG_SOFT: TriggerMode(""software"", TIMED_MODE),
    TRIG_TIMED: TriggerMode(""timed"", TIMED_MODE),
    TRIG_VARIABLE: TriggerMode(""variable timed"", VARIABLE_TIMED_MODE),
    TRIG_FIRST: TriggerMode(""trig. first"", TRIGGER_FIRST_MODE),
    TRIG_STROBED: TriggerMode(""strobed"", STROBED_MODE),
    TRIG_BULB: TriggerMode(""bulb"", BULB_MODE),
}

PV_MODE_TO_TRIGGER = {
    TRIG_SOFT: (microscope.TriggerType.SOFTWARE, microscope.TriggerMode.ONCE),
    # Microscope and PVCam use mode strobe for very different things,
    # check with the PVCam manual carefully.  PVCam's STROBED_MODE
    # means that one external trigger starts *each* exposure in a
    # sequence, which maps to Microscope trigger mode ONCE.  PVCam's
    # TRIGGER_FIRST_MODE means that one external trigger signals the
    # start of a sequence.
    TRIG_FIRST: (
        microscope.TriggerType.RISING_EDGE,
        microscope.TriggerMode.START,
    ),
    TRIG_STROBED: (
        microscope.TriggerType.RISING_EDGE,
        microscope.TriggerMode.ONCE,
    ),
    TRIG_BULB: (
        microscope.TriggerType.RISING_EDGE,
        microscope.TriggerMode.BULB,
    ),
}


TRIGGER_TO_PV_MODE = {v: k for k, v in PV_MODE_TO_TRIGGER.items()}


class PVParam:
    """"""A wrapper around PVCAM parameters.""""""

    @staticmethod
    def factory(camera, param_id):
        """"""Create a PVParam or appropriate subclass""""""
        # A mapping of pv parameters types to python types.
        #   None means unsupported.
        #   Parameters omitted from the mapping will default to PVParam.
        __types__ = {
            TYPE_SMART_STREAM_TYPE: None,
            TYPE_SMART_STREAM_TYPE_PTR: None,
            TYPE_VOID_PTR: None,
            TYPE_VOID_PTR_PTR: None,
            TYPE_ENUM: PVEnumParam,
            TYPE_CHAR_PTR: PVStringParam,
        }
        # Determine the appropiate type from its id.
        pvtype = __types__.get(param_id >> 24 & 255, PVParam)
        if pvtype is None:
            raise microscope.LibraryLoadError(
                ""Parameter %s not supported"" % _param_to_name[param_id]
            )
        return pvtype(camera, param_id)

    def __init__(self, camera, param_id):
        # Use a weakref back to the camera to avoid circular dependency.
        self.cam = weakref.proxy(camera)
        self.param_id = param_id
        self.name = _param_to_name[param_id]
        self._pvtype = param_id >> 24 & 255
        if self.name == ""PARAM_READOUT_TIME"":
            # Bugged. Here is what the SDK says: ""The parameter type is
            # incorrectly defined. The actual type is TYPE_UNS32.""
            self._pvtype = TYPE_UNS32
        self.dtype = _dtypemap[self._pvtype]
        self._ctype = _typemap[self._pvtype]
        self.__cache = {}

    def set_value(self, new_value):
        """"""Set a parameter value.

        Subclasses should do whatever processing they need on new_value,
        then call super().set_value(new_value)""""""
        try:
            ref = ctypes.byref(new_value)
        except TypeError:
            # Need to convert python type to ctype first.
            ref = ctypes.byref(self._ctype(new_value))
        _set_param(self.cam.handle, self.param_id, ref)
        # Read back the value to update cache.
        self._query(force_query=True)

    def _query(self, what=ATTR_CURRENT, force_query=False):
        """"""Query the DLL for an attribute for this parameter.

        This returns pythonic types, not ctypes.""""""
        err = None
        key = (self, what)  # key for cache
        if self.cam._acquiring and not force_query:
            return self.__cache.get(key, None)
        if what == ATTR_AVAIL:
            return self.available
        elif not self.available:
            raise microscope.DeviceError(
                ""Parameter %s is not available"" % self.name
            )
        rtype = _attr_map[what]  # return type
        if not rtype:
            rtype = _get_param(self.cam.handle, self.param_id, ATTR_TYPE)
        if rtype.value == TYPE_CHAR_PTR:
            buf_len = _length_map[self.param_id]
            if not buf_len:
                raise microscope.DeviceError(
                    ""pvcam: parameter %s not supported in python."" % self.name
                )
            try:
                result = _get_param(
                    self.cam.handle, self.param_id, what, buf_len=buf_len
                )
            except Exception as e:
                err = e
            else:
                result = result.value
        else:
            try:
                result = _get_param(self.cam.handle, self.param_id, what)
            except Exception as e:
                err = e
            else:
                result = ctypes.POINTER(self._ctype)(result).contents.value
        # Test on err.args prevents indexing into empty tuple.
        if err and err.args and err.args[0].startswith(""pvcam error 49""):
            _logger.warn(
                ""Parameter %s not available due to camera state."", self.name
            )
            result = None
        elif err:
            raise e
        else:
            self.__cache[key] = result
        return result

    @property
    def access(self):
        """"""Return parameter access attribute.""""""
        return int(
            _get_param(self.cam.handle, self.param_id, ATTR_ACCESS).value
        )

    @property
    def available(self):
        """"""Return whether or not parameter is available on hardware.""""""
        return bool(
            _get_param(self.cam.handle, self.param_id, ATTR_AVAIL).value
        )

    @property
    def count(self):
        """"""Return count of parameter enum entries.""""""
        return int(
            _get_param(self.cam.handle, self.param_id, ATTR_COUNT).value
        )

    @property
    def values(self):
        """"""Get parameter min and max values.

        Subclasses for strings and enum override this.""""""
        return (self._query(ATTR_MIN), self._query(ATTR_MAX))

    @property
    def current(self):
        """"""Return the current (or cached) parameter value.

        Subclasses should override this for more complex data types.""""""
        return self._query()


class PVEnumParam(PVParam):
    """"""PVParam subclass for enums""""""

    def set_value(self, new_value):
        """"""Set an enum parameter value.""""""
        # We may be passed a value, a description string, or a tuple of
        # (value, string).
        values, descriptions = list(zip(*self.values.items()))
        if hasattr(new_value, ""__iter__""):
            desc = str(new_value[1])
        elif isinstance(new_value, str):
            desc = str(new_value)
        else:
            desc = None
        # If we have a description, rely on that, as this avoids any confusion
        # of index and value.
        if desc and desc in descriptions:
            new_index = descriptions.index(desc)
            new_value = values[new_index]
        elif desc:
            raise Exception(
                ""Could not find description '%s' for enum %s.""
                % (desc, self.name)
            )
        super().set_value(new_value)

    @property
    def current(self):
        """"""Return the current (or cached) enum parameter value.""""""
        # c_void_p(0) is None, so replace with 0
        return int(self._query() or 0)

    @property
    def values(self):
        """"""Get allowable enum values""""""
        values = {}
        for i in range(self.count):
            length = _enum_str_length(self.cam.handle, self.param_id, i)
            value, desc = _get_enum_param(
                self.cam.handle, self.param_id, i, length
            )
            values[value.value] = desc.value.decode()
        return values


class PVStringParam(PVParam):
    """"""PVParam subclass for strings""""""

    def set_value(self, new_value):
        """"""Set a string parameter value""""""
        if hasattr(new_value, ""encode""):
            new_value = new_value.encode()
        super().set_value(new_value)

    @property
    def current(self):
        """"""Return the current (or cached) string parameter value.""""""
        return self._query().decode()

    @property
    def values(self):
        """"""Get allowable string length.""""""
        values = _length_map[self.param_id] or 0
        return values


class PVCamera(
    microscope.abc.FloatingDeviceMixin,
    microscope.abc.Camera,
):
    """"""Implements the CameraDevice interface for the pvcam library.""""""

    # Keep track of open cameras.
    open_cameras = []

    def __init__(self, index=0, **kwargs):
        super().__init__(index=index, **kwargs)
        # Camera name in DLL.
        self._pv_name = None
        # Camera handle.
        self.handle = None
        # Sensor shape.
        self.shape = (None, None)
        # Region of interest.
        self.roi = (None, None, None, None)
        # Binning setting.
        self.binning = microscope.Binning(1, 1)
        self._trigger = TRIG_STROBED
        self.exposure_time = 0.001  # in seconds
        # Cycle time
        self.cycle_time = self.exposure_time
        # Data buffer.
        self._buffer = None
        # This devices PVCAM parameters.
        self._params = {}
        # Circular buffer length.
        self._circ_buffer_length = 10

        # Add common settings.
        self.add_setting(
            ""exposure time"",
            ""float"",
            lambda: self.exposure_time,
            self.set_exposure_time,
            lambda: (1e-6, 1),
        )
        self.add_setting(
            ""trigger mode"",
            ""enum"",
            lambda: self._trigger,
            lambda value: setattr(self, ""_trigger"", value),
            {k: v.label for k, v in TRIGGER_MODES.items()},
        )
        self.add_setting(
            ""circular buffer length"",
            ""int"",
            lambda: self._circ_buffer_length,
            lambda value: setattr(self, ""_circ_buffer_length"", value),
            (2, 100),
        )

        self.initialize()

    @property
    def _region(self):
        """"""Return a rgn_type for current roi and binning settings.""""""
        return rgn_type(
            self.roi.left,
            self.roi.left + self.roi.width - 1,
            self.binning.h,
            self.roi.top,
            self.roi.top + self.roi.height - 1,
            self.binning.v,
        )

    """"""Private methods, called here and within super classes.""""""

    def _fetch_data(self):
        """"""Fetch data - for use in fetch_loop.""""""
        # Not used: images fetched using callback.
        return None

    def _do_enable(self):
        """"""Enable the camera hardware and make ready to respond to triggers.

        Return True if successful, False if not.""""""
        # Set exposure time resolution on camera and determine t_exp, the
        # integer value used to set exposure time on the hardware later.
        if self.exposure_time < 1e-3:
            self._params[PARAM_EXP_RES].set_value(EXP_RES_ONE_MICROSEC)
            t_exp = int(self.exposure_time * 1e6)
        else:
            self._params[PARAM_EXP_RES].set_value(EXP_RES_ONE_MILLISEC)
            t_exp = int(self.exposure_time * 1e3)
        # Determine the data type of the buffer
        # Kinetix has an 8 bit mode, may need more options for colour
        # cameras.
        buffer_dtype = ""uint16""
        if self._params[PARAM_BIT_DEPTH].current == 8:
            buffer_dtype = ""uint8""
        # Configure camera, allocate buffer, and register callback.
        if self._trigger == TRIG_SOFT:
            # Software triggering for single frames.
            # Set up callback.
            self._using_callback = True

            def cb():
                """"""Soft trigger mode end-of-frame callback.""""""
                timestamp = time.time()
                frame = self._buffer.copy()
                _logger.debug(""Fetched single frame."")
                _exp_finish_seq(self.handle, CCS_CLEAR)
                self._put(frame, timestamp)
                return

            # Need to keep a reference to the callback.
            self._eof_callback = CALLBACK(cb)
            _cam_register_callback(
                self.handle, PL_CALLBACK_EOF, self._eof_callback
            )
            nbytes = _exp_setup_seq(
                self.handle,
                1,
                1,  # cam, num epxosures, num regions
                self._region,
                TRIGGER_MODES[self._trigger].pv_mode,
                t_exp,
            )
            buffer_shape = (
                self.roi.height // self.binning.v,
                self.roi.width // self.binning.h,
            )
            self._buffer = np.require(
                np.empty(buffer_shape, dtype=buffer_dtype),
                requirements=[""C_CONTIGUOUS"", ""ALIGNED"", ""OWNDATA""],
            )
        else:
            # Use a circular buffer.
            self._using_callback = True

            # Determine the data type of the frame
            frame_type = uns16
            if buffer_dtype == ""uint8"":
                frame_type = uns8

            def cb():
                """"""Circular buffer mode end-of-frame callback.""""""
                timestamp = time.time()
                frame_p = ctypes.cast(
                    _exp_get_latest_frame(self.handle),
                    ctypes.POINTER(frame_type),
                )
                frame = np.ctypeslib.as_array(
                    frame_p, (self.roi[2], self.roi[3])
                ).copy()
                _logger.debug(""Fetched frame from circular buffer."")
                self._put(frame, timestamp)
                return

            # Need to keep a reference to the callback.
            self._eof_callback = CALLBACK(cb)
            _cam_register_callback(
                self.handle, PL_CALLBACK_EOF, self._eof_callback
            )
            buffer_shape = (
                self._circ_buffer_length,
                self.roi.height // self.binning.v,
                self.roi.width // self.binning.h,
            )
            self._buffer = np.require(
                np.empty(buffer_shape, dtype=buffer_dtype),
                requirements=[""C_CONTIGUOUS"", ""ALIGNED"", ""OWNDATA""],
            )
            nbytes = _exp_setup_cont(
                self.handle,
                1,
                self._region,
                TRIGGER_MODES[self._trigger].pv_mode,
                t_exp,
                CIRC_OVERWRITE,
            ).value

        # Read back exposure time.
        t_readback = self._params[PARAM_EXPOSURE_TIME].current
        t_resolution = self._params[PARAM_EXP_RES].current
        multipliers = {
            EXP_RES_ONE_SEC: 1.0,
            EXP_RES_ONE_MILLISEC: 1e-3,
            EXP_RES_ONE_MICROSEC: 1e-6,
        }
        if isinstance(t_resolution, tuple):
            self.exposure_time = t_readback * multipliers[t_resolution[0]]
        else:
            self.exposure_time = t_readback * multipliers[t_resolution]
        # Update cycle time. Exposure time in seconds; readout time in microseconds.
        self.cycle_time = (
            self.exposure_time
            + 1e-6 * self._params[PARAM_READOUT_TIME].current
        )
        # Set up exposure time for VARIABLE_TIMED_MODE, as according to documentation.
        # It doesn't seem to work.
        if self._trigger == TRIG_VARIABLE:
            self._params[PARAM_EXP_TIME].set_value(t_exp)
        # If using real triggering, start triggered acquisition.
        # (Software triggering will start acquisition in soft_trigger().)
        if self._trigger != TRIG_SOFT:
            _exp_start_cont(
                self.handle,
                self._buffer.ctypes.data_as(ctypes.c_void_p),
                self._buffer.nbytes,
            )
        # Done.
        self._acquiring = True
        return self._acquiring

    def _do_disable(self):
        """"""Disable the hardware for a short period of inactivity.""""""
        self.abort()
        _cam_deregister_callback(self.handle, PL_CALLBACK_EOF)

    def _do_shutdown(self) -> None:
        """"""Disable the hardware for a prolonged period of inactivity.""""""
        self.abort()
        _cam_close(self.handle)
        PVCamera.open_cameras.remove(self.handle)
        if not PVCamera.open_cameras:
            _logger.info(""No more open cameras - calling _pvcam_uninit."")
            _pvcam_uninit()

    """"""Private shape-related methods. These methods do not need to account
    for camera orientation or transforms due to readout mode, as that
    is handled in the parent class.""""""

    def _get_sensor_shape(self):
        """"""Return the sensor shape (width, height).""""""
        return self.shape

    def _get_binning(self):
        """"""Return the current binning (horizontal, vertical).""""""
        return self.binning

    @microscope.abc.keep_acquiring
    def _set_binning(self, binning):
        """"""Set binning to (h, v).""""""
        #  The keep_acquiring decorator will cause recreation of buffers.
        self.binning = microscope.Binning(binning.h, binning.v)

    def _get_roi(self):
        """"""Return the current ROI (left, top, width, height).""""""
        return self.roi

    @microscope.abc.keep_acquiring
    def _set_roi(self, roi):
        """"""Set the ROI to (left, tip, width, height).""""""
        right = roi.left + roi.width
        bottom = roi.top + roi.height
        if (right, bottom) > self.shape:
            raise ValueError(""ROI exceeds sensor area."")
        self.roi = roi

    """"""Public methods, callable from client.""""""

    def get_id(self):
        """"""Get hardware's unique identifier.""""""
        return self._params[PARAM_HEAD_SER_NUM_ALPHA].current

    def abort(self):
        """"""Abort acquisition.

        This should put the camera into a state in which settings can
        be modified.""""""
        if self._trigger == TRIG_SOFT:
            _exp_finish_seq(self.handle, CCS_CLEAR)
        else:
            _exp_stop_cont(self.handle, CCS_CLEAR)
        _exp_abort(self.handle, CCS_HALT)
        self._acquiring = False

    def initialize(self):
        """"""Initialise the camera.""""""
        # Init the DLL if necessary.
        if not PVCamera.open_cameras:
            try:
                _pvcam_init()
            except:
                pass
        # If no cameras detected, need to deinit DLL so it can be reinited to update count.
        if _cam_get_total().value == 0:
            _pvcam_uninit()
            raise microscope.InitialiseError(""No cameras detected."")
        # Connect to the camera.
        _logger.info(""DLL version: %s"", _pvcam_get_ver().value)
        self._pv_name = _cam_get_name(self._index).value
        _logger.info(""Initializing %s"", self._pv_name)
        self.handle = _cam_open(self._pv_name, OPEN_EXCLUSIVE)
        PVCamera.open_cameras.append(self.handle)

        # Set up event callbacks. Tried to use the resume callback to reinit camera
        # after power loss, but any attempt to close/reopen the camera or deinit the
        # DLL throws a Windows Error 0xE06D7363.
        def _cb(event):
            _logger.info(""Received %s event."", event)
            if event == ""removed"":
                _logger.critical(""Can not re-init hardware. Exiting."")
                exit(-1)
            return

        self._cbs = {
            ""check"": CALLBACK(lambda: _cb(""check"")),
            ""resumed"": CALLBACK(lambda: _cb(""resumed"")),
            ""removed"": CALLBACK(lambda: _cb(""removed"")),
        }
        _cam_register_callback(
            self.handle, PL_CALLBACK_CHECK_CAMS, self._cbs[""check""]
        )
        _cam_register_callback(
            self.handle, PL_CALLBACK_CAM_REMOVED, self._cbs[""removed""]
        )
        _cam_register_callback(
            self.handle, PL_CALLBACK_CAM_RESUMED, self._cbs[""resumed""]
        )
        # Repopulate _params.
        self._params = {}
        for param_id, name in _param_to_name.items():
            try:
                p = PVParam.factory(self, param_id)
            except:
                _logger.warn(""Skipping unsupported parameter %s."", name)
                continue
            if not p.dtype or not p.available:
                continue
            self._params[param_id] = p
            name = name[6:]

            try:
                p.current
            except KeyError:
                # Raise these here, as the message is a tuple, not a str.
                raise
            except Exception as err:
                # Test on err.args prevents indexing into empty tuple.
                if err.args and not err.args[0].startswith(""pvcam error 49""):
                    _logger.warn(
                        ""Skipping parameter %s: not supported"" "" in python."",
                        p.name,
                    )
                    continue
            self.add_setting(
                p.name,
                p.dtype,
                lambda p=p: p.current,
                p.set_value
                if p.access in [ACC_READ_WRITE, ACC_WRITE_ONLY]
                else None,
                lambda p=p: p.values,
            )
        if PARAM_GAIN_MULT_FACTOR in self._params:
            self.add_setting(
                ""gain"",
                self._params[PARAM_GAIN_MULT_FACTOR].dtype,
                lambda: self._params[PARAM_GAIN_MULT_FACTOR].current,
                self._params[PARAM_GAIN_MULT_FACTOR].set_value,
                self._params[PARAM_GAIN_MULT_FACTOR].values,
            )

        if PARAM_PMODE in self._params:
            self.add_setting(
                ""frame transfer mode"",
                self._params[PARAM_PMODE].dtype,
                lambda: self._params[PARAM_PMODE].current,
                self._params[PARAM_PMODE].set_value,
                self._params[PARAM_PMODE].values,
            )

        self.shape = (
            self._params[PARAM_PAR_SIZE].current,
            self._params[PARAM_SER_SIZE].current,
        )
        self.roi = microscope.ROI(0, 0, self.shape[0], self.shape[1])

        # Populate readout modes by iterating over readout ports and speed
        # table entries.
        ro_ports = self._params[PARAM_READOUT_PORT].values
        self._readout_mode = 0  # The index of the current readout mode
        self._readout_modes = []
        self._readout_mode_parameters = []
        for i, port in ro_ports.items():
            self._params[PARAM_READOUT_PORT].set_value(i)
            ro_speeds = self._params[PARAM_SPDTAB_INDEX].values
            for j in range(ro_speeds[0], ro_speeds[1] + 1):
                self._params[PARAM_SPDTAB_INDEX].set_value(j)
                bit_depth = self._params[PARAM_BIT_DEPTH].current
                freq = 1e9 / self._params[PARAM_PIX_TIME].current
                if freq > 1e6:
                    freq *= 1e-6
                    prefix = ""M""
                elif freq > 1e3:
                    freq *= 1e-3
                    prefix = ""k""
                else:
                    prefix = ""Hz""
                mode_str = ""%s, %s-bit, %s %sHz"" % (
                    port,
                    bit_depth,
                    freq,
                    prefix,
                )
                self._readout_modes.append(mode_str)
                self._readout_mode_parameters.append(
                    {""port"": i, ""spdtab_index"": j}
                )
        # Set to default mode.
        self._set_readout_mode(self._readout_mode)
        self.add_setting(
            ""readout mode"",
            ""enum"",
            lambda: self._readout_mode,
            self._set_readout_mode,
            lambda: self._readout_modes,
        )
        self._params[PARAM_CLEAR_MODE].set_value(CLEAR_PRE_EXPOSURE_POST_SEQ)

    @microscope.abc.keep_acquiring
    def _set_readout_mode(self, index):
        """"""Set the readout mode and transform.""""""
        params = self._readout_mode_parameters[index]
        self._params[PARAM_READOUT_PORT].set_value(params[""port""])
        self._params[PARAM_SPDTAB_INDEX].set_value(params[""spdtab_index""])
        self._readout_mode = index
        # Update transforms, if available.
        chip = self._params[PARAM_CHIP_NAME].current
        new_readout_transform = None
        readout_map = READOUT_TRANSFORMS.get(chip, None)
        if readout_map:
            new_readout_transform = readout_map.get(params[""port""], None)
        if new_readout_transform:
            self._set_readout_transform(new_readout_transform)

    @microscope.abc.keep_acquiring
    def set_exposure_time(self, value):
        """"""Set the exposure time to value.""""""
        self.exposure_time = value

    def get_exposure_time(self):
        """"""Return the current exposure time.

        Just return self.exposure_time, which is updated with the real
        value during _do_enable.""""""
        return self.exposure_time

    def get_cycle_time(self):
        """"""Return the cycle time.

        Just return self.cycle_time, which is updated with the real
        value during _do_enable.""""""
        return self.cycle_time

    @Pyro4.oneway
    def soft_trigger(self):
        """"""Expose software triggering to a client.

        Deprecated, use trigger().

        Trigger an exposure in TRIG_SOFT mode.
        Log some debugging stats in other trigger modes.""""""
        if self._trigger == TRIG_SOFT:
            _logger.debug(""Received soft trigger ..."")
            _exp_start_seq(
                self.handle, self._buffer.ctypes.data_as(ctypes.c_void_p)
            )
        else:
            cstatus, cbytes, cframes = _exp_check_cont_status(self.handle)
            status, bytes = _exp_check_status(self.handle)

            _logger.debug(
                ""Status checks\n""
                ""check_cont:   %s \t bytes: %d\tframes: %d\n""
                ""check_status: %s \t bytes: %d\t"",
                STATUS_STRINGS[cstatus.value],
                cbytes.value,
                cframes.value,
                STATUS_STRINGS[status.value],
                bytes.value,
            )
        return

    @property
    def trigger_mode(self) -> microscope.TriggerMode:
        return PV_MODE_TO_TRIGGER[self._trigger][1]

    @property
    def trigger_type(self) -> microscope.TriggerType:
        return PV_MODE_TO_TRIGGER[self._trigger][0]

    def set_trigger(
        self, ttype: microscope.TriggerType, tmode: microscope.TriggerMode
    ) -> None:
        try:
            self._trigger = TRIGGER_TO_PV_MODE[(ttype, tmode)]
        except KeyError:
            raise microscope.UnsupportedFeatureError(
                ""no PVCam mode for %s and %s"" % (ttype, tmode)
            )

    def _do_trigger(self) -> None:
        _exp_start_seq(
            self.handle, self._buffer.ctypes.data_as(ctypes.c_void_p)
        )
"
267,44.0,UK,"The v2 Camera Module has a Sony IMX219 8-megapixel sensor (compared to the 5-megapixel OmniVision OV5647 sensor of the original camera).
",Pi camera,,"**Raspberry Pi** is a series of small single-board computers (SBCs) developed in the United Kingdom by the **Raspberry Pi** Foundation in association with Broadcom
",Instrumental,Picam,"[OrderedDict([('id', 'atti0qfPjfhBvCOcX'), ('width', 318), ('height', 159), ('url', 'https://v5.airtableusercontent.com/v1/18/18/1689537600000/O4eYObUrMEacm-kJL7Gs1w/iUP7KBlupyLRvHlkLT7_QFWOzLvl73qPG2zKl5v22POS9_eUKNkH3v_CsrrQlF27vQXyqyGqMvBOR6A8Gq_PQh2d_FpswXe5oncZZIzaWIQ/XhNTG34Fk3qfcNvy_j6TK80ZEDLbkDiAB76c4TsJPYI'), ('filename', 'téléchargement.png'), ('size', 3789), ('type', 'image/png'), ('thumbnails', OrderedDict([('small', OrderedDict([('url', 'https://v5.airtableusercontent.com/v1/18/18/1689537600000/mKy8_QGXZGqvhVH1h0Oupw/jkFlFLleKJ_6bHob7uc9WYkTZjmFIop02F_-kRijYV8hpJJVOmFrlFRIQuV9DilhQ3T3w4N7RtKTnPYqwX30dg/h9usdYyJtdtl7NumGEL9OHTp5iJQG56f4-Zn1i5XQXw'), ('width', 72), ('height', 36)])), ('large', OrderedDict([('url', 'https://v5.airtableusercontent.com/v1/18/18/1689537600000/8VpLK1P3951B9zvBmVEErQ/9gyHGvZeqSi6RrrRwttx0I1tzO-NOECQZc8aRibl9RF6U4ioLKb2xSU-NhVTNHzpFMDKo8fmlfPudgoA-PVmHA/aV3lO4oPwhZljTcQhGjMiMLR2OOcbAY9YpywQ1wQqxA'), ('width', 318), ('height', 159)])), ('full', OrderedDict([('url', 'https://v5.airtableusercontent.com/v1/18/18/1689537600000/DR45RzeF0DBsID5pt1IvWg/kxdE4KRluZBlSkHICvgyRSQiQ1H7nfRQgYceM3H4jKMcXgSzQAfxRXGCV9wT3hulE3zBLwCcdpSR_EMyW7Tc0Q/aitOBneOWcqORED3vOLp6vQmj7BXzKNubSlUImgtUWY'), ('width', 3000), ('height', 3000)]))]))])]",https://www.raspberrypi.org/,Write a Python script that uses Instrumental to connect to a Pi Camera Cameras,https://en.wikipedia.org/wiki/Camera_module,['Cameras'],"A camera module is an image sensor integrated with a lens, control electronics, and an interface like CSI, Ethernet or plain raw low-voltage differential signaling.",Pi Camera,https://www.raspberrypi.com/documentation/accessories/camera.html,"[OrderedDict([('id', 'attliEXRgwCLXh2Iw'), ('width', 225), ('height', 225), ('url', 'https://v5.airtableusercontent.com/v1/18/18/1689537600000/o3uMj_4eGXpNSk0GoTPgYA/tgYjkGhdAq9HTxNPCBbwRvbnN_q8UhgvUBxlRvO6pTmzCSkV62yPTF3a8ppSkVj6SKzLZ9k64dO9hoL8KTj1_pK9AMABUKGv6M6KZpVxXYE/u7BkwxghclgM7xVS2rcM64UdlQR6oGyIiW2TbnsbdrY'), ('filename', 'téléchargement (2.jpeg'), ('size', 7947), ('type', 'image/jpeg'), ('thumbnails', OrderedDict([('small', OrderedDict([('url', 'https://v5.airtableusercontent.com/v1/18/18/1689537600000/FCDDgHpCautCDFCm335Bsg/Jrj_wVaL-Jg7KmD_glo4Sv_nnb7GbHI5ahtwo4HISPeyyd_tyjkG4UUNcHERpMaYVpcdx_AMvgB3154PFX2C4g/yW2z3ERy1VofVL0zqgIVBkPT2VJM3VvSujK1eUlw77E'), ('width', 36), ('height', 36)])), ('large', OrderedDict([('url', 'https://v5.airtableusercontent.com/v1/18/18/1689537600000/zA8OfOA4zXazstDFus3OSw/yCldN-3VnpWRyPMvHM4QjCzvOBY-Fa9mhNC3Tj9KHzvmoQHvS22xNXAjRoHAGnJ4PSfegtpUAxuD81E9O7Sjug/sasyN1JKMTM5DBTilExeXOZJR4wjODS5OEph_UmKxVM'), ('width', 225), ('height', 225)])), ('full', OrderedDict([('url', 'https://v5.airtableusercontent.com/v1/18/18/1689537600000/YERw85S_jxJn3kBUPtA1kg/gt1VLMP83uK8JLW0hrYn9s6vo7pzdfi0HoFn4o1Nzfs8inhNVIAC7tIeucQQR0L8p0wRE0nPZXy0dB8fxx6uNQ/TyfMzVRT_SRhUBWCEVO68LaDTiP4AdEUwdkfh9o10NE'), ('width', 3000), ('height', 3000)]))]))])]",https://www.raspberrypi.com/products/camera-module-v2/,https://github.com/waveform80/picamera/blob/master/picamera/camera.py,https://picamera.readthedocs.io/en/release-1.13/,25.0,,,,"# vim: set et sw=4 sts=4 fileencoding=utf-8:
#
# Python camera library for the Rasperry-Pi camera module
# Copyright (c) 2013-2017 Dave Jones <dave@waveform.org.uk>
#
# Redistribution and use in source and binary forms, with or without
# modification, are permitted provided that the following conditions are met:
#
#     * Redistributions of source code must retain the above copyright
#       notice, this list of conditions and the following disclaimer.
#     * Redistributions in binary form must reproduce the above copyright
#       notice, this list of conditions and the following disclaimer in the
#       documentation and/or other materials provided with the distribution.
#     * Neither the name of the copyright holder nor the
#       names of its contributors may be used to endorse or promote products
#       derived from this software without specific prior written permission.
#
# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""
# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE
# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
# POSSIBILITY OF SUCH DAMAGE.

from __future__ import (
    unicode_literals,
    print_function,
    division,
    absolute_import,
    )

# Make Py2's str equivalent to Py3's
str = type('')

import warnings
import datetime
import mimetypes
import ctypes as ct
import threading
from fractions import Fraction
from operator import itemgetter, and_
from functools import reduce
from collections import namedtuple

from . import bcm_host, mmal, mmalobj as mo
from .exc import (
    PiCameraError,
    PiCameraValueError,
    PiCameraRuntimeError,
    PiCameraClosed,
    PiCameraNotRecording,
    PiCameraAlreadyRecording,
    PiCameraMMALError,
    PiCameraDeprecated,
    PiCameraFallback,
    )
from .encoders import (
    PiVideoFrame,
    PiVideoEncoder,
    PiRawVideoEncoder,
    PiCookedVideoEncoder,
    PiRawOneImageEncoder,
    PiRawMultiImageEncoder,
    PiCookedOneImageEncoder,
    PiCookedMultiImageEncoder,
    )
from .renderers import (
    PiPreviewRenderer,
    PiOverlayRenderer,
    PiNullSink,
    )
from .color import Color

try:
    from RPi import GPIO
except ImportError:
    # Can't find RPi.GPIO so just null-out the reference
    GPIO = None


def docstring_values(values, indent=8):
    """"""
    Formats a dictionary of values for inclusion in a docstring.
    """"""
    return ('\n' + ' ' * indent).join(
        ""* ``'%s'``"" % k
        for (k, v) in
        sorted(values.items(), key=itemgetter(1)))


class PiCameraMaxResolution(object):
    """"""
    Singleton representing the maximum resolution of the camera module.
    """"""
PiCameraMaxResolution = PiCameraMaxResolution()


class PiCameraMaxFramerate(object):
    """"""
    Singleton representing the maximum framerate of the camera module.
    """"""
PiCameraMaxFramerate = PiCameraMaxFramerate()


PiCameraConfig = namedtuple('PiCameraConfig', (
    'sensor_mode',
    'clock_mode',
    'resolution',
    'framerate',
    'isp_blocks',
    'colorspace',
))


class PiCamera(object):
    """"""
    Provides a pure Python interface to the Raspberry Pi's camera module.

    Upon construction, this class initializes the camera. The *camera_num*
    parameter (which defaults to 0) selects the camera module that the instance
    will represent. Only the Raspberry Pi compute module currently supports
    more than one camera.

    The *sensor_mode*, *resolution*, *framerate*, *framerate_range*,
    *clock_mode*, and *isp_blocks* parameters provide initial values for the
    :attr:`sensor_mode`, :attr:`resolution`, :attr:`framerate`,
    :attr:`framerate_range`, :attr:`clock_mode`, and :attr:`isp_blocks`
    attributes of the class (these attributes are all relatively expensive to
    set individually, hence setting them all upon construction is a speed
    optimization). Please refer to the attribute documentation for more
    information and default values.

    The *stereo_mode* and *stereo_decimate* parameters configure dual cameras
    on a compute module for sterescopic mode. These parameters can only be set
    at construction time; they cannot be altered later without closing the
    :class:`PiCamera` instance and recreating it. The *stereo_mode* parameter
    defaults to ``'none'`` (no stereoscopic mode) but can be set to
    ``'side-by-side'`` or ``'top-bottom'`` to activate a stereoscopic mode. If
    the *stereo_decimate* parameter is ``True``, the resolution of the two
    cameras will be halved so that the resulting image has the same dimensions
    as if stereoscopic mode were not being used.

    The *led_pin* parameter can be used to specify the GPIO pin which should be
    used to control the camera's LED via the :attr:`led` attribute. If this is
    not specified, it should default to the correct value for your Pi platform.
    You should only need to specify this parameter if you are using a custom
    DeviceTree blob (this is only typical on the `Compute Module`_ platform).

    No preview or recording is started automatically upon construction.  Use
    the :meth:`capture` method to capture images, the :meth:`start_recording`
    method to begin recording video, or the :meth:`start_preview` method to
    start live display of the camera's input.

    Several attributes are provided to adjust the camera's configuration. Some
    of these can be adjusted while a recording is running, like
    :attr:`brightness`. Others, like :attr:`resolution`, can only be adjusted
    when the camera is idle.

    When you are finished with the camera, you should ensure you call the
    :meth:`close` method to release the camera resources::

        camera = PiCamera()
        try:
            # do something with the camera
            pass
        finally:
            camera.close()

    The class supports the context manager protocol to make this particularly
    easy (upon exiting the :keyword:`with` statement, the :meth:`close` method
    is automatically called)::

        with PiCamera() as camera:
            # do something with the camera
            pass

    .. versionchanged:: 1.8
        Added *stereo_mode* and *stereo_decimate* parameters.

    .. versionchanged:: 1.9
        Added *resolution*, *framerate*, and *sensor_mode* parameters.

    .. versionchanged:: 1.10
        Added *led_pin* parameter.

    .. versionchanged:: 1.11
        Added *clock_mode* parameter, and permitted setting of resolution as
        appropriately formatted string.

    .. versionchanged:: 1.13
        Added *framerate_range* parameter.

    .. versionchanged:: 1.14
        Positional arguments are now deprecated; all arguments to the
        constructor should be specified as keyword-args.

    .. _Compute Module: https://www.raspberrypi.org/documentation/hardware/computemodule/cmio-camera.md
    """"""

    CAMERA_PREVIEW_PORT = 0
    CAMERA_VIDEO_PORT = 1
    CAMERA_CAPTURE_PORT = 2
    MAX_RESOLUTION = PiCameraMaxResolution # modified by PiCamera.__init__
    MAX_FRAMERATE = PiCameraMaxFramerate # modified by PiCamera.__init__
    DEFAULT_ANNOTATE_SIZE = 32
    CAPTURE_TIMEOUT = 60

    SENSOR_MODES = {
        'ov5647': {
            1: mo.PiSensorMode('1080p', (1, 30), full_fov=False),
            2: mo.PiSensorMode('2592x1944', (1, 15), still=True),
            3: mo.PiSensorMode('2592x1944', (1/6, 1), still=True),
            4: mo.PiSensorMode('1296x972', (1, 42)),
            5: mo.PiSensorMode('1296x730', (1, 49)),
            6: mo.PiSensorMode('VGA', (42, 60)),
            7: mo.PiSensorMode('VGA', (60, 90)),
            },
        'imx219': {
            1: mo.PiSensorMode('1080p', (1/10, 30), full_fov=False),
            2: mo.PiSensorMode('3280x2464', (1/10, 15), still=True),
            3: mo.PiSensorMode('3280x2464', (1/10, 15), still=True),
            4: mo.PiSensorMode('1640x1232', (1/10, 40)),
            5: mo.PiSensorMode('1640x922', (1/10, 40)),
            6: mo.PiSensorMode('720p', (40, 90), full_fov=False),
            7: mo.PiSensorMode('VGA', (40, 90), full_fov=False),
            },
        }

    METER_MODES = {
        'average': mmal.MMAL_PARAM_EXPOSUREMETERINGMODE_AVERAGE,
        'spot':    mmal.MMAL_PARAM_EXPOSUREMETERINGMODE_SPOT,
        'backlit': mmal.MMAL_PARAM_EXPOSUREMETERINGMODE_BACKLIT,
        'matrix':  mmal.MMAL_PARAM_EXPOSUREMETERINGMODE_MATRIX,
        }

    EXPOSURE_MODES = {
        'off':           mmal.MMAL_PARAM_EXPOSUREMODE_OFF,
        'auto':          mmal.MMAL_PARAM_EXPOSUREMODE_AUTO,
        'night':         mmal.MMAL_PARAM_EXPOSUREMODE_NIGHT,
        'nightpreview':  mmal.MMAL_PARAM_EXPOSUREMODE_NIGHTPREVIEW,
        'backlight':     mmal.MMAL_PARAM_EXPOSUREMODE_BACKLIGHT,
        'spotlight':     mmal.MMAL_PARAM_EXPOSUREMODE_SPOTLIGHT,
        'sports':        mmal.MMAL_PARAM_EXPOSUREMODE_SPORTS,
        'snow':          mmal.MMAL_PARAM_EXPOSUREMODE_SNOW,
        'beach':         mmal.MMAL_PARAM_EXPOSUREMODE_BEACH,
        'verylong':      mmal.MMAL_PARAM_EXPOSUREMODE_VERYLONG,
        'fixedfps':      mmal.MMAL_PARAM_EXPOSUREMODE_FIXEDFPS,
        'antishake':     mmal.MMAL_PARAM_EXPOSUREMODE_ANTISHAKE,
        'fireworks':     mmal.MMAL_PARAM_EXPOSUREMODE_FIREWORKS,
        }

    FLASH_MODES = {
        'off':           mmal.MMAL_PARAM_FLASH_OFF,
        'auto':          mmal.MMAL_PARAM_FLASH_AUTO,
        'on':            mmal.MMAL_PARAM_FLASH_ON,
        'redeye':        mmal.MMAL_PARAM_FLASH_REDEYE,
        'fillin':        mmal.MMAL_PARAM_FLASH_FILLIN,
        'torch':         mmal.MMAL_PARAM_FLASH_TORCH,
        }

    AWB_MODES = {
        'off':           mmal.MMAL_PARAM_AWBMODE_OFF,
        'auto':          mmal.MMAL_PARAM_AWBMODE_AUTO,
        'sunlight':      mmal.MMAL_PARAM_AWBMODE_SUNLIGHT,
        'cloudy':        mmal.MMAL_PARAM_AWBMODE_CLOUDY,
        'shade':         mmal.MMAL_PARAM_AWBMODE_SHADE,
        'tungsten':      mmal.MMAL_PARAM_AWBMODE_TUNGSTEN,
        'fluorescent':   mmal.MMAL_PARAM_AWBMODE_FLUORESCENT,
        'incandescent':  mmal.MMAL_PARAM_AWBMODE_INCANDESCENT,
        'flash':         mmal.MMAL_PARAM_AWBMODE_FLASH,
        'horizon':       mmal.MMAL_PARAM_AWBMODE_HORIZON,
        'greyworld':     mmal.MMAL_PARAM_AWBMODE_GREYWORLD,
        }

    IMAGE_EFFECTS = {
        'none':          mmal.MMAL_PARAM_IMAGEFX_NONE,
        'negative':      mmal.MMAL_PARAM_IMAGEFX_NEGATIVE,
        'solarize':      mmal.MMAL_PARAM_IMAGEFX_SOLARIZE,
        # The following don't work
        #'posterize':     mmal.MMAL_PARAM_IMAGEFX_POSTERIZE,
        #'whiteboard':    mmal.MMAL_PARAM_IMAGEFX_WHITEBOARD,
        #'blackboard':    mmal.MMAL_PARAM_IMAGEFX_BLACKBOARD,
        'sketch':        mmal.MMAL_PARAM_IMAGEFX_SKETCH,
        'denoise':       mmal.MMAL_PARAM_IMAGEFX_DENOISE,
        'emboss':        mmal.MMAL_PARAM_IMAGEFX_EMBOSS,
        'oilpaint':      mmal.MMAL_PARAM_IMAGEFX_OILPAINT,
        'hatch':         mmal.MMAL_PARAM_IMAGEFX_HATCH,
        'gpen':          mmal.MMAL_PARAM_IMAGEFX_GPEN,
        'pastel':        mmal.MMAL_PARAM_IMAGEFX_PASTEL,
        'watercolor':    mmal.MMAL_PARAM_IMAGEFX_WATERCOLOUR,
        'film':          mmal.MMAL_PARAM_IMAGEFX_FILM,
        'blur':          mmal.MMAL_PARAM_IMAGEFX_BLUR,
        'saturation':    mmal.MMAL_PARAM_IMAGEFX_SATURATION,
        'colorswap':     mmal.MMAL_PARAM_IMAGEFX_COLOURSWAP,
        'washedout':     mmal.MMAL_PARAM_IMAGEFX_WASHEDOUT,
        'posterise':     mmal.MMAL_PARAM_IMAGEFX_POSTERISE,
        'colorpoint':    mmal.MMAL_PARAM_IMAGEFX_COLOURPOINT,
        'colorbalance':  mmal.MMAL_PARAM_IMAGEFX_COLOURBALANCE,
        'cartoon':       mmal.MMAL_PARAM_IMAGEFX_CARTOON,
        'deinterlace1':  mmal.MMAL_PARAM_IMAGEFX_DEINTERLACE_DOUBLE,
        'deinterlace2':  mmal.MMAL_PARAM_IMAGEFX_DEINTERLACE_ADV,
        }

    DRC_STRENGTHS = {
        'off':    mmal.MMAL_PARAMETER_DRC_STRENGTH_OFF,
        'low':    mmal.MMAL_PARAMETER_DRC_STRENGTH_LOW,
        'medium': mmal.MMAL_PARAMETER_DRC_STRENGTH_MEDIUM,
        'high':   mmal.MMAL_PARAMETER_DRC_STRENGTH_HIGH,
        }

    RAW_FORMATS = {
        'yuv',
        'rgb',
        'rgba',
        'bgr',
        'bgra',
        }

    STEREO_MODES = {
        'none':         mmal.MMAL_STEREOSCOPIC_MODE_NONE,
        'side-by-side': mmal.MMAL_STEREOSCOPIC_MODE_SIDE_BY_SIDE,
        'top-bottom':   mmal.MMAL_STEREOSCOPIC_MODE_BOTTOM,
        }

    CLOCK_MODES = {
        'reset':        mmal.MMAL_PARAM_TIMESTAMP_MODE_RESET_STC,
        'raw':          mmal.MMAL_PARAM_TIMESTAMP_MODE_RAW_STC,
        }

    ISP_BLOCKS = {
        'black-level':   1 << 2,
        'lens-shading':  1 << 3,
        'white-balance': 1 << 5,
        'bad-pixel':     1 << 7,
        'crosstalk':     1 << 9,
        'demosaic':      1 << 11,
        'gamma':         1 << 18,
        'sharpening':    1 << 22,
        }

    COLORSPACES = {
        'auto':   mmal.MMAL_COLOR_SPACE_UNKNOWN,
        'jfif':   mmal.MMAL_COLOR_SPACE_JPEG_JFIF,
        'bt601':  mmal.MMAL_COLOR_SPACE_ITUR_BT601,
        'bt709':  mmal.MMAL_COLOR_SPACE_ITUR_BT709,
        }

    _METER_MODES_R    = {v: k for (k, v) in METER_MODES.items()}
    _EXPOSURE_MODES_R = {v: k for (k, v) in EXPOSURE_MODES.items()}
    _FLASH_MODES_R    = {v: k for (k, v) in FLASH_MODES.items()}
    _AWB_MODES_R      = {v: k for (k, v) in AWB_MODES.items()}
    _IMAGE_EFFECTS_R  = {v: k for (k, v) in IMAGE_EFFECTS.items()}
    _DRC_STRENGTHS_R  = {v: k for (k, v) in DRC_STRENGTHS.items()}
    _STEREO_MODES_R   = {v: k for (k, v) in STEREO_MODES.items()}
    _CLOCK_MODES_R    = {v: k for (k, v) in CLOCK_MODES.items()}
    _ISP_BLOCKS_R     = {v: k for (k, v) in ISP_BLOCKS.items()}
    _COLORSPACES_R    = {v: k for (k, v) in COLORSPACES.items()}

    __slots__ = (
        '_used_led',
        '_led_pin',
        '_camera',
        '_camera_config',
        '_camera_exception',
        '_revision',
        '_preview',
        '_preview_alpha',
        '_preview_layer',
        '_preview_fullscreen',
        '_preview_window',
        '_splitter',
        '_splitter_connection',
        '_encoders_lock',
        '_encoders',
        '_overlays',
        '_raw_format',
        '_image_effect_params',
        '_exif_tags',
        )

    def __init__(self, *args, **kwargs):
        options = self._parse_options(args, kwargs)
        self._camera = None
        self._camera_config = None
        self._camera_exception = None
        self._preview = None
        self._preview_alpha = 255
        self._preview_layer = 2
        self._preview_fullscreen = True
        self._preview_window = None
        self._splitter = None
        self._splitter_connection = None
        self._encoders_lock = threading.Lock()
        self._encoders = {}
        self._overlays = []
        self._raw_format = 'yuv'
        self._image_effect_params = None
        self._exif_tags = {}
        self._used_led = None
        self._led_pin = None
        bcm_host.bcm_host_init()
        try:
            self._init_revision(options)
            old_config, new_config = self._init_config(options)
            self._init_led(options)
            self._init_camera(options)
            self._configure_camera(old_config, new_config)
            self._init_preview()
            self._init_splitter()
            self._camera.enable()
            self._init_defaults()
        except:
            self.close()
            raise
        else:
            mimetypes.add_type('application/h264',  '.h264',  False)
            mimetypes.add_type('application/mjpeg', '.mjpg',  False)
            mimetypes.add_type('application/mjpeg', '.mjpeg', False)

    @staticmethod
    def _parse_options(args, kwargs):
        """"""
        Parse the constructor options.

        In future versions we'll only support keyword args; for now (for
        backwards compatibility) we'll allow the positional args that we
        previously accepted but raise a deprecation warning for each.
        """"""
        options = {  # with defaults
            'camera_num': 0,
            'stereo_mode': 'none',
            'stereo_decimate': False,
            'resolution': None,
            'framerate': None,
            'sensor_mode': 0,
            'led_pin': None,
            'clock_mode': 'reset',
            'framerate_range': None,
            'isp_blocks': None,
            'colorspace': 'auto',
            }
        arg_names = (
            'camera_num',
            'stereo_mode',
            'stereo_decimate',
            'resolution',
            'framerate',
            'sensor_mode',
            'led_pin',
            'clock_mode',
            'framerate_range',
            )
        for arg_name, arg in zip(arg_names, args):
            warnings.warn(
                PiCameraDeprecated(
                    'Specifying %s as a non-keyword argument is '
                    'deprecated' % arg_name))
            options[arg_name] = arg
        for arg_name in options:
            options[arg_name] = kwargs.pop(arg_name, options[arg_name])
        if kwargs:
            raise TypeError(
                'PiCamera.__init__ got an unexpected keyword '
                'argument %r' % kwargs.popitem()[0])
        return options

    def _init_revision(self, options):
        """"""
        Query the firmware for the attached camera revision; older firmwares
        can't return the revision but only support the OV5647 sensor so we can
        assume that revision in such a case. This is also where the placeholder
        objects for MAX_RESOLUTION and MAX_FRAMERATE are replaced with their
        actual values
        """"""
        with mo.MMALCameraInfo() as camera_info:
            camera_num = options['camera_num']
            info = camera_info.control.params[mmal.MMAL_PARAMETER_CAMERA_INFO]
            revision = 'ov5647'
            if camera_info.info_rev > 1:
                revision = info.cameras[camera_num].camera_name.decode('ascii')
            if PiCamera.MAX_RESOLUTION is PiCameraMaxResolution:
                PiCamera.MAX_RESOLUTION = mo.PiResolution(
                        info.cameras[camera_num].max_width,
                        info.cameras[camera_num].max_height,
                        )
        if PiCamera.MAX_FRAMERATE is PiCameraMaxFramerate:
            if revision.lower() == 'ov5647':
                PiCamera.MAX_FRAMERATE = 90
            else:
                PiCamera.MAX_FRAMERATE = 120
        self._revision = revision

    @classmethod
    def _init_config(cls, options):
        """"""
        Construct initial and desired configurations to pass to the
        :meth:`_configure_camera` method. The initial configuration is mostly
        hard-coded defaults. The desired configuration comes from the specified
        options.
        """"""
        if options['resolution'] is None:
            # Get screen resolution
            w = ct.c_uint32()
            h = ct.c_uint32()
            if bcm_host.graphics_get_display_size(0, w, h) == -1:
                w = 1280
                h = 720
            else:
                w = int(w.value)
                h = int(h.value)
            resolution = mo.PiResolution(w, h)
        elif options['resolution'] is PiCameraMaxResolution:
            resolution = cls.MAX_RESOLUTION
        else:
            resolution = mo.to_resolution(options['resolution'])

        if options['framerate_range'] is None:
            if options['framerate'] is None:
                framerate = 30
            elif options['framerate'] is PiCameraMaxFramerate:
                framerate = cls.MAX_FRAMERATE
            else:
                framerate = mo.to_fraction(options['framerate'])
        elif options['framerate'] is not None:
            raise PiCameraValueError(
                ""Can't specify framerate and framerate_range"")
        else:
            try:
                low, high = options['framerate_range']
            except TypeError:
                raise PiCameraValueError(
                    ""framerate_range must have (low, high) values"")
            if low is PiCameraMaxFramerate:
                low = cls.MAX_FRAMERATE
            if high is PiCameraMaxFramerate:
                high = cls.MAX_FRAMERATE
            framerate = (mo.to_fraction(low), mo.to_fraction(high))

        try:
            clock_mode = cls.CLOCK_MODES[options['clock_mode']]
        except KeyError:
            raise PiCameraValueError(
                'Invalid clock mode: %s' % options['clock_mode'])

        try:
            colorspace = cls.COLORSPACES[options['colorspace']]
        except KeyError:
            raise PiCameraValueError(
                'Invalid colorspace: %s' % options['colorspace'])

        all_blocks = set(cls.ISP_BLOCKS.keys())
        if options['isp_blocks'] is None:
            isp_blocks = 0
        else:
            isp_blocks = set(options['isp_blocks'])
            invalid = isp_blocks - all_blocks
            if invalid:
                raise PiCameraValueError(
                    'Invalid ISP block: %s' % invalid.pop())
            isp_blocks = reduce(and_, (~v for k, v in cls.ISP_BLOCKS.items()
                                      if k not in isp_blocks), 0xFFFFFFFF)

        old_config = PiCameraConfig(
            sensor_mode=0,
            clock_mode=clock_mode,
            resolution=cls.MAX_RESOLUTION,
            framerate=30,
            isp_blocks=0,
            colorspace=mmal.MMAL_COLOR_SPACE_UNKNOWN)
        new_config = PiCameraConfig(
            sensor_mode=options['sensor_mode'],
            clock_mode=clock_mode,
            resolution=resolution,
            framerate=framerate,
            isp_blocks=isp_blocks,
            colorspace=colorspace)
        return old_config, new_config

    def _init_led(self, options):
        """"""
        Determine the GPIO pin to use for controlling the camera's LED, if any.
        """"""
        led_pin = options['led_pin']
        if GPIO and led_pin is None:
            try:
                led_pin = {
                    (0, 0): 2,  # compute module (default for cam 0)
                    (0, 1): 30, # compute module (default for cam 1)
                    (1, 0): 5,  # Pi 1 model B rev 1
                    (2, 0): 5,  # Pi 1 model B rev 2 or model A
                    (3, 0): 32, # Pi 1 model B+ or Pi 2 model B
                    }[(GPIO.RPI_REVISION, options['camera_num'])]
            except KeyError:
                raise PiCameraError(
                        'Unable to determine default GPIO LED pin for RPi '
                        'revision %d and camera num %d' % (
                            GPIO.RPI_REVISION, options['camera_num']))
        self._used_led = False
        self._led_pin = led_pin

    def _init_camera(self, options):
        """"""
        Construct the MMAL camera component and perform all early configuration
        on it (e.g. most stereoscopic configuration has to be done before the
        camera is activated).
        """"""
        try:
            stereo_mode = self.STEREO_MODES[options['stereo_mode']]
        except KeyError:
            raise PiCameraValueError(
                'Invalid stereo mode: %s' % options['stereo_mode'])
        try:
            self._camera = mo.MMALCamera()
        except PiCameraMMALError as e:
            if e.status == mmal.MMAL_ENOMEM:
                raise PiCameraError(
                    ""Camera is not enabled. Try running 'sudo raspi-config' ""
                    ""and ensure that the camera has been enabled."")
            else:
                raise
        self._camera_config = self._camera.control.params[
            mmal.MMAL_PARAMETER_CAMERA_CONFIG]
        # Don't attempt to set this if stereo mode isn't requested as it'll
        # break compatibility on older firmwares
        if stereo_mode != mmal.MMAL_STEREOSCOPIC_MODE_NONE:
            for p in self._camera.outputs:
                mp = mmal.MMAL_PARAMETER_STEREOSCOPIC_MODE_T(
                    mmal.MMAL_PARAMETER_HEADER_T(
                        mmal.MMAL_PARAMETER_STEREOSCOPIC_MODE,
                        ct.sizeof(mmal.MMAL_PARAMETER_STEREOSCOPIC_MODE_T),
                    ),
                    mode=stereo_mode,
                    decimate=options['stereo_decimate'],
                    swap_eyes=False,
                    )
                p.params[mmal.MMAL_PARAMETER_STEREOSCOPIC_MODE] = mp
        # Must be done *after* stereo-scopic setting
        self._camera.control.params[
            mmal.MMAL_PARAMETER_CAMERA_NUM] = options['camera_num']

    def _init_defaults(self):
        """"""
        Sets most camera settings to various default values.
        """"""
        self._exif_tags = {
            'IFD0.Model': 'RP_%s' % self.revision,
            'IFD0.Make': 'RaspberryPi',
            }
        self.sharpness = 0
        self.contrast = 0
        self.brightness = 50
        self.saturation = 0
        self.iso = 0 # auto
        self.video_stabilization = False
        self.exposure_compensation = 0
        self.exposure_mode = 'auto'
        self.meter_mode = 'average'
        self.awb_mode = 'auto'
        self.image_effect = 'none'
        self.color_effects = None
        self.rotation = 0
        self.hflip = self.vflip = False
        self.zoom = (0.0, 0.0, 1.0, 1.0)

    def _init_splitter(self):
        """"""
        Create a splitter component for the video port. This is to permit video
        recordings and captures where use_video_port=True to occur
        simultaneously (#26)
        """"""
        self._splitter = mo.MMALSplitter()
        self._splitter.inputs[0].connect(
                self._camera.outputs[self.CAMERA_VIDEO_PORT]).enable()

    def _init_preview(self):
        """"""
        Create a null-sink component, enable it and connect it to the camera's
        preview port. If nothing is connected to the preview port, the camera
        doesn't measure exposure and captured images gradually fade to black
        (issue #22; subsequently fixed in firmware but there's no harm in
        leaving this in place for the sake of backwards compat).
        """"""
        self._preview = PiNullSink(
            self, self._camera.outputs[self.CAMERA_PREVIEW_PORT])

    def _start_capture(self, port):
        """"""
        Starts the camera capturing frames.

        This method starts the camera feeding frames to any attached encoders,
        but only enables capture if the port is the camera's still port, or if
        there's a single active encoder on the video splitter.
        """"""
        if (
                port == self._camera.outputs[self.CAMERA_CAPTURE_PORT] or
                len([e for e in self._encoders.values() if e.active]) == 1):
            port.params[mmal.MMAL_PARAMETER_CAPTURE] = True

    def _stop_capture(self, port):
        """"""
        Stops the camera capturing frames.

        This method stops the camera feeding frames to any attached encoders,
        but only disables capture if the port is the camera's still port, or if
        there's a single active encoder on the video splitter.
        """"""
        if (
                port == self._camera.outputs[self.CAMERA_CAPTURE_PORT] or
                len([e for e in self._encoders.values() if e.active]) == 1):
            port.params[mmal.MMAL_PARAMETER_CAPTURE] = False

    def _check_camera_open(self):
        """"""
        Raise an exception if the camera is already closed, or if the camera
        has encountered a fatal error.
        """"""
        exc, self._camera_exception = self._camera_exception, None
        if exc:
            raise exc
        if self.closed:
            raise PiCameraClosed(""Camera is closed"")

    def _check_recording_stopped(self):
        """"""
        Raise an exception if the camera is currently recording.
        """"""
        if self.recording:
            raise PiCameraRuntimeError(""Recording is currently running"")

    def _get_ports(self, from_video_port, splitter_port):
        """"""
        Determine the camera and output ports for given capture options.

        See :ref:`camera_hardware` for more information on picamera's usage of
        camera, splitter, and encoder ports. The general idea here is that the
        capture (still) port operates on its own, while the video port is
        always connected to a splitter component, so requests for a video port
        also have to specify which splitter port they want to use.
        """"""
        self._check_camera_open()
        if from_video_port and (splitter_port in self._encoders):
            raise PiCameraAlreadyRecording(
                    'The camera is already using port %d ' % splitter_port)
        camera_port = (
            self._camera.outputs[self.CAMERA_VIDEO_PORT]
            if from_video_port else
            self._camera.outputs[self.CAMERA_CAPTURE_PORT]
            )
        output_port = (
            self._splitter.outputs[splitter_port]
            if from_video_port else
            camera_port
            )
        return (camera_port, output_port)

    def _get_output_format(self, output):
        """"""
        Given an output object, attempt to determine the requested format.

        We attempt to determine the filename of the *output* object and derive
        a MIME type from the extension. If *output* has no filename, an error
        is raised.
        """"""
        if isinstance(output, bytes):
            filename = output.decode('utf-8')
        elif isinstance(output, str):
            filename = output
        else:
            try:
                filename = output.name
            except AttributeError:
                raise PiCameraValueError(
                    'Format must be specified when output has no filename')
        type, encoding = mimetypes.guess_type(filename, strict=False)
        if not type:
            raise PiCameraValueError(
                'Unable to determine type from filename %s' % filename)
        return type

    def _get_image_format(self, output, format=None):
        """"""
        Given an output object and an optional format, attempt to determine the
        requested image format.

        This method is used by all capture methods to determine the requested
        output format. If *format* is specified as a MIME-type the ""image/""
        prefix is stripped. If *format* is not specified, then
        :meth:`_get_output_format` will be called to attempt to determine
        format from the *output* object.
        """"""
        if isinstance(format, bytes):
            format = format.decode('utf-8')
        format = format or self._get_output_format(output)
        format = (
            format[6:] if format.startswith('image/') else
            format)
        if format == 'x-ms-bmp':
            format = 'bmp'
        if format == 'raw':
            format = self.raw_format
        return format

    def _get_video_format(self, output, format=None):
        """"""
        Given an output object and an optional format, attempt to determine the
        requested video format.

        This method is used by all recording methods to determine the requested
        output format. If *format* is specified as a MIME-type the ""video/"" or
        ""application/"" prefix will be stripped. If *format* is not specified,
        then :meth:`_get_output_format` will be called to attempt to determine
        format from the *output* object.
        """"""
        if isinstance(format, bytes):
            format = format.decode('utf-8')
        format = format or self._get_output_format(output)
        format = (
            format[6:]  if format.startswith('video/') else
            format[12:] if format.startswith('application/') else
            format)
        return format

    def _get_image_encoder(
            self, camera_port, output_port, format, resize, **options):
        """"""
        Construct an image encoder for the requested parameters.

        This method is called by :meth:`capture` and :meth:`capture_continuous`
        to construct an image encoder. The *camera_port* parameter gives the
        MMAL camera port that should be enabled for capture by the encoder. The
        *output_port* parameter gives the MMAL port that the encoder should
        read output from (this may be the same as the camera port, but may be
        different if other component(s) like a splitter have been placed in the
        pipeline). The *format* parameter indicates the image format and will
        be one of:

        * ``'jpeg'``
        * ``'png'``
        * ``'gif'``
        * ``'bmp'``
        * ``'yuv'``
        * ``'rgb'``
        * ``'rgba'``
        * ``'bgr'``
        * ``'bgra'``

        The *resize* parameter indicates the size that the encoder should
        resize the output to (presumably by including a resizer in the
        pipeline). Finally, *options* includes extra keyword arguments that
        should be passed verbatim to the encoder.
        """"""
        encoder_class = (
                PiRawOneImageEncoder if format in self.RAW_FORMATS else
                PiCookedOneImageEncoder)
        return encoder_class(
                self, camera_port, output_port, format, resize, **options)

    def _get_images_encoder(
            self, camera_port, output_port, format, resize, **options):
        """"""
        Construct a multi-image encoder for the requested parameters.

        This method is largely equivalent to :meth:`_get_image_encoder` with
        the exception that the encoder returned should expect to be passed an
        iterable of outputs to its :meth:`~PiEncoder.start` method, rather than
        a single output object. This method is called by the
        :meth:`capture_sequence` method.

        All parameters are the same as in :meth:`_get_image_encoder`. Please
        refer to the documentation for that method for further information.
        """"""
        encoder_class = (
                PiRawMultiImageEncoder if format in self.RAW_FORMATS else
                PiCookedMultiImageEncoder)
        return encoder_class(
                self, camera_port, output_port, format, resize, **options)

    def _get_video_encoder(
            self, camera_port, output_port, format, resize, **options):
        """"""
        Construct a video encoder for the requested parameters.

        This method is called by :meth:`start_recording` and
        :meth:`record_sequence` to construct a video encoder.  The
        *camera_port* parameter gives the MMAL camera port that should be
        enabled for capture by the encoder. The *output_port* parameter gives
        the MMAL port that the encoder should read output from (this may be the
        same as the camera port, but may be different if other component(s)
        like a splitter have been placed in the pipeline). The *format*
        parameter indicates the video format and will be one of:

        * ``'h264'``
        * ``'mjpeg'``

        The *resize* parameter indicates the size that the encoder should
        resize the output to (presumably by including a resizer in the
        pipeline). Finally, *options* includes extra keyword arguments that
        should be passed verbatim to the encoder.
        """"""
        encoder_class = (
                PiRawVideoEncoder if format in self.RAW_FORMATS else
                PiCookedVideoEncoder)
        return encoder_class(
                self, camera_port, output_port, format, resize, **options)

    def close(self):
        """"""
        Finalizes the state of the camera.

        After successfully constructing a :class:`PiCamera` object, you should
        ensure you call the :meth:`close` method once you are finished with the
        camera (e.g. in the ``finally`` section of a ``try..finally`` block).
        This method stops all recording and preview activities and releases all
        resources associated with the camera; this is necessary to prevent GPU
        memory leaks.
        """"""
        for port in list(self._encoders):
            self.stop_recording(splitter_port=port)
        assert not self.recording
        for overlay in list(self._overlays):
            self.remove_overlay(overlay)
        if self._preview:
            self._preview.close()
            self._preview = None
        if self._splitter:
            self._splitter.close()
            self._splitter = None
        if self._camera:
            self._camera.close()
            self._camera = None
        exc, self._camera_exception = self._camera_exception, None
        if exc:
            raise exc

    def __enter__(self):
        return self

    def __exit__(self, exc_type, exc_value, exc_tb):
        self.close()

    def start_preview(self, **options):
        """"""
        Displays the preview overlay.

        This method starts a camera preview as an overlay on the Pi's primary
        display (HDMI or composite). A :class:`PiRenderer` instance (more
        specifically, a :class:`PiPreviewRenderer`) is constructed with the
        keyword arguments captured in *options*, and is returned from the
        method (this instance is also accessible from the :attr:`preview`
        attribute for as long as the renderer remains active).  By default, the
        renderer will be opaque and fullscreen.

        This means the default preview overrides whatever is currently visible
        on the display. More specifically, the preview does not rely on a
        graphical environment like X-Windows (it can run quite happily from a
        TTY console); it is simply an overlay on the Pi's video output. To stop
        the preview and reveal the display again, call :meth:`stop_preview`.
        The preview can be started and stopped multiple times during the
        lifetime of the :class:`PiCamera` object.

        All other camera properties can be modified ""live"" while the preview is
        running (e.g. :attr:`brightness`).

        .. note::

            Because the default preview typically obscures the screen, ensure
            you have a means of stopping a preview before starting one. If the
            preview obscures your interactive console you won't be able to
            Alt+Tab back to it as the preview isn't in a window. If you are in
            an interactive Python session, simply pressing Ctrl+D usually
            suffices to terminate the environment, including the camera and its
            associated preview.
        """"""
        self._check_camera_open()
        self._preview.close()
        options.setdefault('layer', self._preview_layer)
        options.setdefault('alpha', self._preview_alpha)
        options.setdefault('fullscreen', self._preview_fullscreen)
        options.setdefault('window', self._preview_window)
        renderer = PiPreviewRenderer(
            self, self._camera.outputs[self.CAMERA_PREVIEW_PORT], **options)
        self._preview = renderer
        return renderer

    def stop_preview(self):
        """"""
        Hides the preview overlay.

        If :meth:`start_preview` has previously been called, this method shuts
        down the preview display which generally results in the underlying
        display becoming visible again. If a preview is not currently running,
        no exception is raised - the method will simply do nothing.
        """"""
        self._check_camera_open()
        self._preview.close()
        self._preview = PiNullSink(
            self, self._camera.outputs[self.CAMERA_PREVIEW_PORT])

    def add_overlay(self, source, size=None, format=None, **options):
        """"""
        Adds a static overlay to the preview output.

        This method creates a new static overlay using the same rendering
        mechanism as the preview. Overlays will appear on the Pi's video
        output, but will not appear in captures or video recordings. Multiple
        overlays can exist; each call to :meth:`add_overlay` returns a new
        :class:`PiOverlayRenderer` instance representing the overlay.

        The *source* must be an object that supports the :ref:`buffer protocol
        <bufferobjects>` in one of the supported unencoded formats: ``'yuv'``,
        ``'rgb'``, ``'rgba'``, ``'bgr'``, or ``'bgra'``. The format can
        specified explicitly with the optional *format* parameter. If not
        specified, the method will attempt to guess the format based on the
        length of *source* and the *size* (assuming 3 bytes per pixel for RGB,
        and 4 bytes for RGBA).

        The optional *size* parameter specifies the size of the source image as
        a ``(width, height)`` tuple. If this is omitted or ``None`` then the
        size is assumed to be the same as the camera's current
        :attr:`resolution`.

        The length of *source* must take into account that widths are rounded
        up to the nearest multiple of 32, and heights to the nearest multiple
        of 16.  For example, if *size* is ``(1280, 720)``, and *format* is
        ``'rgb'``, then *source* must be a buffer with length 1280 × 720 × 3
        bytes, or 2,764,800 bytes (because 1280 is a multiple of 32, and 720 is
        a multiple of 16 no extra rounding is required).  However, if *size* is
        ``(97, 57)``, and *format* is ``'rgb'`` then *source* must be a buffer
        with length 128 × 64 × 3 bytes, or 24,576 bytes (pixels beyond column
        97 and row 57 in the source will be ignored).

        New overlays default to *layer* 0, whilst the preview defaults to layer
        2. Higher numbered layers obscure lower numbered layers, hence new
        overlays will be invisible (if the preview is running) by default. You
        can make the new overlay visible either by making any existing preview
        transparent (with the :attr:`~PiRenderer.alpha` property) or by moving
        the overlay into a layer higher than the preview (with the
        :attr:`~PiRenderer.layer` property).

        All keyword arguments captured in *options* are passed onto the
        :class:`PiRenderer` constructor. All camera properties except
        :attr:`resolution` and :attr:`framerate` can be modified while overlays
        exist. The reason for these exceptions is that the overlay has a static
        resolution and changing the camera's mode would require resizing of the
        source.

        .. warning::

            If too many overlays are added, the display output will be disabled
            and a reboot will generally be required to restore the display.
            Overlays are composited ""on the fly"". Hence, a real-time constraint
            exists wherein for each horizontal line of HDMI output, the content
            of all source layers must be fetched, resized, converted, and
            blended to produce the output pixels.

            If enough overlays exist (where ""enough"" is a number dependent on
            overlay size, display resolution, bus frequency, and several other
            factors making it unrealistic to calculate in advance), this
            process breaks down and video output fails. One solution is to add
            ``dispmanx_offline=1`` to ``/boot/config.txt`` to force the use of
            an off-screen buffer. Be aware that this requires more GPU memory
            and may reduce the update rate.

        .. _RGB: https://en.wikipedia.org/wiki/RGB
        .. _RGBA: https://en.wikipedia.org/wiki/RGBA_color_space

        .. versionadded:: 1.8

        .. versionchanged:: 1.13
            Added *format* parameter
        """"""
        self._check_camera_open()
        renderer = PiOverlayRenderer(self, source, size, format, **options)
        self._overlays.append(renderer)
        return renderer

    def remove_overlay(self, overlay):
        """"""
        Removes a static overlay from the preview output.

        This method removes an overlay which was previously created by
        :meth:`add_overlay`. The *overlay* parameter specifies the
        :class:`PiRenderer` instance that was returned by :meth:`add_overlay`.

        .. versionadded:: 1.8
        """"""
        if not overlay in self._overlays:
            raise PiCameraValueError(
                ""The specified overlay is not owned by this instance of ""
                ""PiCamera"")
        overlay.close()
        self._overlays.remove(overlay)

    def start_recording(
            self, output, format=None, resize=None, splitter_port=1, **options):
        """"""
        Start recording video from the camera, storing it in *output*.

        If *output* is a string, it will be treated as a filename for a new
        file which the video will be written to. If *output* is not a string,
        but is an object with a ``write`` method, it is assumed to be a
        file-like object and the video data is appended to it (the
        implementation only assumes the object has a ``write()`` method - no
        other methods are required but ``flush`` will be called at the end of
        recording if it is present). If *output* is not a string, and has no
        ``write`` method it is assumed to be a writeable object implementing
        the buffer protocol. In this case, the video frames will be written
        sequentially to the underlying buffer (which must be large enough to
        accept all frame data).

        If *format* is ``None`` (the default), the method will attempt to guess
        the required video format from the extension of *output* (if it's a
        string), or from the *name* attribute of *output* (if it has one). In
        the case that the format cannot be determined, a
        :exc:`PiCameraValueError` will be raised.

        If *format* is not ``None``, it must be a string specifying the format
        that you want the video output in. The format can be a MIME-type or
        one of the following strings:

        * ``'h264'`` - Write an H.264 video stream
        * ``'mjpeg'`` - Write an M-JPEG video stream
        * ``'yuv'`` - Write the raw video data to a file in YUV420 format
        * ``'rgb'`` - Write the raw video data to a file in 24-bit RGB format
        * ``'rgba'`` - Write the raw video data to a file in 32-bit RGBA format
        * ``'bgr'`` - Write the raw video data to a file in 24-bit BGR format
        * ``'bgra'`` - Write the raw video data to a file in 32-bit BGRA format

        If *resize* is not ``None`` (the default), it must be a two-element
        tuple specifying the width and height that the video recording should
        be resized to. This is particularly useful for recording video using
        the full resolution of the camera sensor (which is not possible in
        H.264 without down-sizing the output).

        The *splitter_port* parameter specifies the port of the built-in
        splitter that the video encoder will be attached to. This defaults to
        ``1`` and most users will have no need to specify anything different.
        If you wish to record multiple (presumably resized) streams
        simultaneously, specify a value between ``0`` and ``3`` inclusive for
        this parameter, ensuring that you do not specify a port that is
        currently in use.

        Certain formats accept additional options which can be specified
        as keyword arguments. The ``'h264'`` format accepts the following
        additional options:

        * *profile* - The H.264 profile to use for encoding. Defaults to
          'high', but can be one of 'baseline', 'main', 'high', or
          'constrained'.

        * *level* - The `H.264 level`_ to use for encoding. Defaults to '4',
          but can be any H.264 level up to '4.2'.

        * *intra_period* - The key frame rate (the rate at which I-frames are
          inserted in the output). Defaults to ``None``, but can be any 32-bit
          integer value representing the number of frames between successive
          I-frames. The special value 0 causes the encoder to produce a single
          initial I-frame, and then only P-frames subsequently. Note that
          :meth:`split_recording` will fail in this mode.

        * *intra_refresh* - The key frame format (the way in which I-frames
          will be inserted into the output stream). Defaults to ``None``, but
          can be one of 'cyclic', 'adaptive', 'both', or 'cyclicrows'.

        * *inline_headers* - When ``True``, specifies that the encoder should
          output SPS/PPS headers within the stream to ensure GOPs (groups of
          pictures) are self describing. This is important for streaming
          applications where the client may wish to seek within the stream, and
          enables the use of :meth:`split_recording`. Defaults to ``True`` if
          not specified.

        * *sei* - When ``True``, specifies the encoder should include
          ""Supplemental Enhancement Information"" within the output stream.
          Defaults to ``False`` if not specified.

        * *sps_timing* - When ``True`` the encoder includes the camera's
          framerate in the SPS header. Defaults to ``False`` if not specified.

        * *motion_output* - Indicates the output destination for motion vector
          estimation data. When ``None`` (the default), motion data is not
          output. Otherwise, this can be a filename string, a file-like object,
          or a writeable buffer object (as with the *output* parameter).

        All encoded formats accept the following additional options:

        * *bitrate* - The bitrate at which video will be encoded. Defaults to
          17000000 (17Mbps) if not specified. The maximum value depends on the
          selected `H.264 level`_ and profile. Bitrate 0 indicates the encoder
          should not use bitrate control (the encoder is limited by the quality
          only).

        * *quality* - Specifies the quality that the encoder should attempt
          to maintain. For the ``'h264'`` format, use values between 10 and 40
          where 10 is extremely high quality, and 40 is extremely low (20-25 is
          usually a reasonable range for H.264 encoding). For the ``mjpeg``
          format, the quality is ignored (it can be specified without error
          though); *bitrate* alone controls the quality of the output.

        * *quantization* - Deprecated alias for *quality*.

        .. versionchanged:: 1.0
            The *resize* parameter was added, and ``'mjpeg'`` was added as a
            recording format

        .. versionchanged:: 1.3
            The *splitter_port* parameter was added

        .. versionchanged:: 1.5
            The *quantization* parameter was deprecated in favor of *quality*,
            and the *motion_output* parameter was added.

        .. versionchanged:: 1.11
            Support for buffer outputs was added.

        .. _H.264 level: https://en.wikipedia.org/wiki/H.264/MPEG-4_AVC#Levels
        """"""
        if 'quantization' in options:
            warnings.warn(
                PiCameraDeprecated(
                    'The quantization option is deprecated; please use '
                    'quality instead (same value)'))
        with self._encoders_lock:
            camera_port, output_port = self._get_ports(True, splitter_port)
            format = self._get_video_format(output, format)
            encoder = self._get_video_encoder(
                    camera_port, output_port, format, resize, **options)
            self._encoders[splitter_port] = encoder
        try:
            encoder.start(output, options.get('motion_output'))
        except Exception as e:
            encoder.close()
            with self._encoders_lock:
                del self._encoders[splitter_port]
            raise

    def split_recording(self, output, splitter_port=1, **options):
        """"""
        Continue the recording in the specified output; close existing output.

        When called, the video encoder will wait for the next appropriate
        split point (an inline SPS header), then will cease writing to the
        current output (and close it, if it was specified as a filename), and
        continue writing to the newly specified *output*.

        The *output* parameter is treated as in the :meth:`start_recording`
        method (it can be a string, a file-like object, or a writeable
        buffer object).

        The *motion_output* parameter can be used to redirect the output of the
        motion vector data in the same fashion as *output*. If *motion_output*
        is ``None`` (the default) then motion vector data will not be
        redirected and will continue being written to the output specified by
        the *motion_output* parameter given to :meth:`start_recording`.
        Alternatively, if you only wish to redirect motion vector data, you can
        set *output* to ``None`` and given a new value for *motion_output*.

        The *splitter_port* parameter specifies which port of the video
        splitter the encoder you wish to change outputs is attached to. This
        defaults to ``1`` and most users will have no need to specify anything
        different. Valid values are between ``0`` and ``3`` inclusive.

        Note that unlike :meth:`start_recording`, you cannot specify format or
        other options as these cannot be changed in the middle of recording.
        Only the new *output* (and *motion_output*) can be specified.
        Furthermore, the format of the recording is currently limited to H264,
        and *inline_headers* must be ``True`` when :meth:`start_recording` is
        called (this is the default).

        The method returns the meta-data of the first :class:`PiVideoFrame`
        that is written to the new output.

        .. versionchanged:: 1.3
            The *splitter_port* parameter was added

        .. versionchanged:: 1.5
            The *motion_output* parameter was added

        .. versionchanged:: 1.11
            Support for buffer outputs was added.
        """"""
        try:
            with self._encoders_lock:
                encoder = self._encoders[splitter_port]
        except KeyError:
            raise PiCameraNotRecording(
                    'There is no recording in progress on '
                    'port %d' % splitter_port)
        else:
            return encoder.split(output, options.get('motion_output'))

    def request_key_frame(self, splitter_port=1):
        """"""
        Request the encoder generate a key-frame as soon as possible.

        When called, the video encoder running on the specified *splitter_port*
        will attempt to produce a key-frame (full-image frame) as soon as
        possible. The *splitter_port* defaults to ``1``. Valid values are
        between ``0`` and ``3`` inclusive.

        .. note::

            This method is only meaningful for recordings encoded in the H264
            format as MJPEG produces full frames for every frame recorded.
            Furthermore, there's no guarantee that the *next* frame will be
            a key-frame; this is simply a request to produce one as soon as
            possible after the call.

        .. versionadded:: 1.11
        """"""
        try:
            with self._encoders_lock:
                encoder = self._encoders[splitter_port]
        except KeyError:
            raise PiCameraNotRecording(
                    'There is no recording in progress on '
                    'port %d' % splitter_port)
        else:
            encoder.request_key_frame()

    def wait_recording(self, timeout=0, splitter_port=1):
        """"""
        Wait on the video encoder for timeout seconds.

        It is recommended that this method is called while recording to check
        for exceptions. If an error occurs during recording (for example out of
        disk space) the recording will stop, but an exception will only be
        raised when the :meth:`wait_recording` or :meth:`stop_recording`
        methods are called.

        If ``timeout`` is 0 (the default) the function will immediately return
        (or raise an exception if an error has occurred).

        The *splitter_port* parameter specifies which port of the video
        splitter the encoder you wish to wait on is attached to. This
        defaults to ``1`` and most users will have no need to specify anything
        different. Valid values are between ``0`` and ``3`` inclusive.

        .. versionchanged:: 1.3
            The *splitter_port* parameter was added
        """"""
        assert timeout is not None
        try:
            with self._encoders_lock:
                encoder = self._encoders[splitter_port]
        except KeyError:
            raise PiCameraNotRecording(
                    'There is no recording in progress on '
                    'port %d' % splitter_port)
        else:
            encoder.wait(timeout)

    def stop_recording(self, splitter_port=1):
        """"""
        Stop recording video from the camera.

        After calling this method the video encoder will be shut down and
        output will stop being written to the file-like object specified with
        :meth:`start_recording`. If an error occurred during recording and
        :meth:`wait_recording` has not been called since the error then this
        method will raise the exception.

        The *splitter_port* parameter specifies which port of the video
        splitter the encoder you wish to stop is attached to. This defaults to
        ``1`` and most users will have no need to specify anything different.
        Valid values are between ``0`` and ``3`` inclusive.

        .. versionchanged:: 1.3
            The *splitter_port* parameter was added
        """"""
        try:
            with self._encoders_lock:
                encoder = self._encoders[splitter_port]
        except KeyError:
            raise PiCameraNotRecording(
                    'There is no recording in progress on '
                    'port %d' % splitter_port)
        else:
            try:
                self.wait_recording(0, splitter_port)
            finally:
                encoder.close()
                with self._encoders_lock:
                    del self._encoders[splitter_port]

    def record_sequence(
            self, outputs, format='h264', resize=None, splitter_port=1, **options):
        """"""
        Record a sequence of video clips from the camera.

        This method accepts a sequence or iterator of *outputs* each of which
        must either be a string specifying a filename for output, or a
        file-like object with a ``write`` method.

        The method acts as an iterator itself, yielding each item of the
        sequence in turn. In this way, the caller can control how long to
        record to each item by only permitting the loop to continue when ready
        to switch to the next output.

        The *format*, *splitter_port*, *resize*, and *options* parameters are
        the same as in :meth:`start_recording`, but *format* defaults to
        ``'h264'``.  The format is **not** derived from the filenames in
        *outputs* by this method.

        For example, to record 3 consecutive 10-second video clips, writing the
        output to a series of H.264 files named clip01.h264, clip02.h264, and
        clip03.h264 one could use the following::

            import picamera
            with picamera.PiCamera() as camera:
                for filename in camera.record_sequence([
                        'clip01.h264',
                        'clip02.h264',
                        'clip03.h264']):
                    print('Recording to %s' % filename)
                    camera.wait_recording(10)

        Alternatively, a more flexible method of writing the previous example
        (which is easier to expand to a large number of output files) is by
        using a generator expression as the input sequence::

            import picamera
            with picamera.PiCamera() as camera:
                for filename in camera.record_sequence(
                        'clip%02d.h264' % i for i in range(3)):
                    print('Recording to %s' % filename)
                    camera.wait_recording(10)

        More advanced techniques are also possible by utilising infinite
        sequences, such as those generated by :func:`itertools.cycle`. In the
        following example, recording is switched between two in-memory streams.
        Whilst one stream is recording, the other is being analysed. The script
        only stops recording when a video recording meets some criteria defined
        by the ``process`` function::

            import io
            import itertools
            import picamera
            with picamera.PiCamera() as camera:
                analyse = None
                for stream in camera.record_sequence(
                        itertools.cycle((io.BytesIO(), io.BytesIO()))):
                    if analyse is not None:
                        if process(analyse):
                            break
                        analyse.seek(0)
                        analyse.truncate()
                    camera.wait_recording(5)
                    analyse = stream

        .. versionadded:: 1.3
        """"""
        with self._encoders_lock:
            camera_port, output_port = self._get_ports(True, splitter_port)
            format = self._get_video_format('', format)
            encoder = self._get_video_encoder(
                    camera_port, output_port, format, resize, **options)
            self._encoders[splitter_port] = encoder
        try:
            start = True
            for output in outputs:
                if start:
                    start = False
                    encoder.start(output, options.get('motion_output'))
                else:
                    encoder.split(output)
                yield output
        finally:
            try:
                encoder.wait(0)
            finally:
                encoder.close()
                with self._encoders_lock:
                    del self._encoders[splitter_port]

    def capture(
            self, output, format=None, use_video_port=False, resize=None,
            splitter_port=0, bayer=False, **options):
        """"""
        Capture an image from the camera, storing it in *output*.

        If *output* is a string, it will be treated as a filename for a new
        file which the image will be written to. If *output* is not a string,
        but is an object with a ``write`` method, it is assumed to be a
        file-like object and the image data is appended to it (the
        implementation only assumes the object has a ``write`` method - no
        other methods are required but ``flush`` will be called at the end of
        capture if it is present). If *output* is not a string, and has no
        ``write`` method it is assumed to be a writeable object implementing
        the buffer protocol. In this case, the image data will be written
        directly to the underlying buffer (which must be large enough to accept
        the image data).

        If *format* is ``None`` (the default), the method will attempt to guess
        the required image format from the extension of *output* (if it's a
        string), or from the *name* attribute of *output* (if it has one). In
        the case that the format cannot be determined, a
        :exc:`PiCameraValueError` will be raised.

        If *format* is not ``None``, it must be a string specifying the format
        that you want the image output in. The format can be a MIME-type or
        one of the following strings:

        * ``'jpeg'`` - Write a JPEG file
        * ``'png'`` - Write a PNG file
        * ``'gif'`` - Write a GIF file
        * ``'bmp'`` - Write a Windows bitmap file
        * ``'yuv'`` - Write the raw image data to a file in YUV420 format
        * ``'rgb'`` - Write the raw image data to a file in 24-bit RGB format
        * ``'rgba'`` - Write the raw image data to a file in 32-bit RGBA format
        * ``'bgr'`` - Write the raw image data to a file in 24-bit BGR format
        * ``'bgra'`` - Write the raw image data to a file in 32-bit BGRA format
        * ``'raw'`` - Deprecated option for raw captures; the format is taken
          from the deprecated :attr:`raw_format` attribute

        The *use_video_port* parameter controls whether the camera's image or
        video port is used to capture images. It defaults to ``False`` which
        means that the camera's image port is used. This port is slow but
        produces better quality pictures. If you need rapid capture up to the
        rate of video frames, set this to ``True``.

        When *use_video_port* is ``True``, the *splitter_port* parameter
        specifies the port of the video splitter that the image encoder will be
        attached to. This defaults to ``0`` and most users will have no need to
        specify anything different. This parameter is ignored when
        *use_video_port* is ``False``. See :ref:`mmal` for more information
        about the video splitter.

        If *resize* is not ``None`` (the default), it must be a two-element
        tuple specifying the width and height that the image should be resized
        to.

        .. warning::

            If *resize* is specified, or *use_video_port* is ``True``, Exif
            metadata will **not** be included in JPEG output. This is due to an
            underlying firmware limitation.

        Certain file formats accept additional options which can be specified
        as keyword arguments. Currently, only the ``'jpeg'`` encoder accepts
        additional options, which are:

        * *quality* - Defines the quality of the JPEG encoder as an integer
          ranging from 1 to 100. Defaults to 85. Please note that JPEG quality
          is not a percentage and `definitions of quality`_ vary widely.

        * *restart* - Defines the restart interval for the JPEG encoder as a
          number of JPEG MCUs. The actual restart interval used will be a
          multiple of the number of MCUs per row in the resulting image.

        * *thumbnail* - Defines the size and quality of the thumbnail to embed
          in the Exif metadata. Specifying ``None`` disables thumbnail
          generation.  Otherwise, specify a tuple of ``(width, height,
          quality)``. Defaults to ``(64, 48, 35)``.

        * *bayer* - If ``True``, the raw bayer data from the camera's sensor
          is included in the Exif metadata.

        .. note::

            The so-called ""raw"" formats listed above (``'yuv'``, ``'rgb'``,
            etc.) do not represent the raw bayer data from the camera's sensor.
            Rather they provide access to the image data after GPU processing,
            but before format encoding (JPEG, PNG, etc). Currently, the only
            method of accessing the raw bayer data is via the *bayer* parameter
            described above.

        .. versionchanged:: 1.0
            The *resize* parameter was added, and raw capture formats can now
            be specified directly

        .. versionchanged:: 1.3
            The *splitter_port* parameter was added, and *bayer* was added as
            an option for the ``'jpeg'`` format

        .. versionchanged:: 1.11
            Support for buffer outputs was added.

        .. _definitions of quality: http://photo.net/learn/jpeg/#qual
        """"""
        if format == 'raw':
            warnings.warn(
                PiCameraDeprecated(
                    'The ""raw"" format option is deprecated; specify the '
                    'required format directly instead (""yuv"", ""rgb"", etc.)'))
        if use_video_port and bayer:
            raise PiCameraValueError(
                'bayer is only valid with still port captures')
        if 'burst' in options:
            raise PiCameraValueError(
                'burst is only valid with capture_sequence or capture_continuous')
        with self._encoders_lock:
            camera_port, output_port = self._get_ports(use_video_port, splitter_port)
            format = self._get_image_format(output, format)
            encoder = self._get_image_encoder(
                    camera_port, output_port, format, resize, **options)
            if use_video_port:
                self._encoders[splitter_port] = encoder
        try:
            if bayer:
                camera_port.params[mmal.MMAL_PARAMETER_ENABLE_RAW_CAPTURE] = True
            encoder.start(output)
            # Wait for the callback to set the event indicating the end of
            # image capture
            if not encoder.wait(self.CAPTURE_TIMEOUT):
                raise PiCameraRuntimeError(
                    'Timed out waiting for capture to end')
        finally:
            encoder.close()
            with self._encoders_lock:
                if use_video_port:
                    del self._encoders[splitter_port]

    def capture_sequence(
            self, outputs, format='jpeg', use_video_port=False, resize=None,
            splitter_port=0, burst=False, bayer=False, **options):
        """"""
        Capture a sequence of consecutive images from the camera.

        This method accepts a sequence or iterator of *outputs* each of which
        must either be a string specifying a filename for output, or a
        file-like object with a ``write`` method, or a writeable buffer object.
        For each item in the sequence or iterator of outputs, the camera
        captures a single image as fast as it can.

        The *format*, *use_video_port*, *splitter_port*, *resize*, and
        *options* parameters are the same as in :meth:`capture`, but *format*
        defaults to ``'jpeg'``.  The format is **not** derived from the
        filenames in *outputs* by this method.

        If *use_video_port* is ``False`` (the default), the *burst* parameter
        can be used to make still port captures faster.  Specifically, this
        prevents the preview from switching resolutions between captures which
        significantly speeds up consecutive captures from the still port. The
        downside is that this mode is currently has several bugs; the major
        issue is that if captures are performed too quickly some frames will
        come back severely underexposed. It is recommended that users avoid the
        *burst* parameter unless they absolutely require it and are prepared to
        work around such issues.

        For example, to capture 3 consecutive images::

            import time
            import picamera
            with picamera.PiCamera() as camera:
                camera.start_preview()
                time.sleep(2)
                camera.capture_sequence([
                    'image1.jpg',
                    'image2.jpg',
                    'image3.jpg',
                    ])
                camera.stop_preview()

        If you wish to capture a large number of images, a list comprehension
        or generator expression can be used to construct the list of filenames
        to use::

            import time
            import picamera
            with picamera.PiCamera() as camera:
                camera.start_preview()
                time.sleep(2)
                camera.capture_sequence([
                    'image%02d.jpg' % i
                    for i in range(100)
                    ])
                camera.stop_preview()

        More complex effects can be obtained by using a generator function to
        provide the filenames or output objects.

        .. versionchanged:: 1.0
            The *resize* parameter was added, and raw capture formats can now
            be specified directly

        .. versionchanged:: 1.3
            The *splitter_port* parameter was added

        .. versionchanged:: 1.11
            Support for buffer outputs was added.
        """"""
        if use_video_port:
            if burst:
                raise PiCameraValueError(
                    'burst is only valid with still port captures')
            if bayer:
                raise PiCameraValueError(
                    'bayer is only valid with still port captures')
        with self._encoders_lock:
            camera_port, output_port = self._get_ports(use_video_port, splitter_port)
            format = self._get_image_format('', format)
            if use_video_port:
                encoder = self._get_images_encoder(
                        camera_port, output_port, format, resize, **options)
                self._encoders[splitter_port] = encoder
            else:
                encoder = self._get_image_encoder(
                        camera_port, output_port, format, resize, **options)
        try:
            if use_video_port:
                encoder.start(outputs)
                encoder.wait()
            else:
                if burst:
                    camera_port.params[mmal.MMAL_PARAMETER_CAMERA_BURST_CAPTURE] = True
                try:
                    for output in outputs:
                        if bayer:
                            camera_port.params[mmal.MMAL_PARAMETER_ENABLE_RAW_CAPTURE] = True
                        encoder.start(output)
                        if not encoder.wait(self.CAPTURE_TIMEOUT):
                            raise PiCameraRuntimeError(
                                'Timed out waiting for capture to end')
                finally:
                    if burst:
                        camera_port.params[mmal.MMAL_PARAMETER_CAMERA_BURST_CAPTURE] = False
        finally:
            encoder.close()
            with self._encoders_lock:
                if use_video_port:
                    del self._encoders[splitter_port]

    def capture_continuous(
            self, output, format=None, use_video_port=False, resize=None,
            splitter_port=0, burst=False, bayer=False, **options):
        """"""
        Capture images continuously from the camera as an infinite iterator.

        This method returns an infinite iterator of images captured
        continuously from the camera. If *output* is a string, each captured
        image is stored in a file named after *output* after substitution of
        two values with the :meth:`~str.format` method. Those two values are:

        * ``{counter}`` - a simple incrementor that starts at 1 and increases
          by 1 for each image taken

        * ``{timestamp}`` - a :class:`~datetime.datetime` instance

        The table below contains several example values of *output* and the
        sequence of filenames those values could produce:

        .. tabularcolumns:: |p{80mm}|p{40mm}|p{10mm}|

        +--------------------------------------------+--------------------------------------------+-------+
        | *output* Value                             | Filenames                                  | Notes |
        +============================================+============================================+=======+
        | ``'image{counter}.jpg'``                   | image1.jpg, image2.jpg, image3.jpg, ...    |       |
        +--------------------------------------------+--------------------------------------------+-------+
        | ``'image{counter:02d}.jpg'``               | image01.jpg, image02.jpg, image03.jpg, ... |       |
        +--------------------------------------------+--------------------------------------------+-------+
        | ``'image{timestamp}.jpg'``                 | image2013-10-05 12:07:12.346743.jpg,       | (1)   |
        |                                            | image2013-10-05 12:07:32.498539, ...       |       |
        +--------------------------------------------+--------------------------------------------+-------+
        | ``'image{timestamp:%H-%M-%S-%f}.jpg'``     | image12-10-02-561527.jpg,                  |       |
        |                                            | image12-10-14-905398.jpg                   |       |
        +--------------------------------------------+--------------------------------------------+-------+
        | ``'{timestamp:%H%M%S}-{counter:03d}.jpg'`` | 121002-001.jpg, 121013-002.jpg,            | (2)   |
        |                                            | 121014-003.jpg, ...                        |       |
        +--------------------------------------------+--------------------------------------------+-------+

        1. Note that because timestamp's default output includes colons (:),
           the resulting filenames are not suitable for use on Windows. For
           this reason (and the fact the default contains spaces) it is
           strongly recommended you always specify a format when using
           ``{timestamp}``.

        2. You can use both ``{timestamp}`` and ``{counter}`` in a single
           format string (multiple times too!) although this tends to be
           redundant.

        If *output* is not a string, but has a ``write`` method, it is assumed
        to be a file-like object and each image is simply written to this
        object sequentially. In this case you will likely either want to write
        something to the object between the images to distinguish them, or
        clear the object between iterations. If *output* is not a string, and
        has no ``write`` method, it is assumed to be a writeable object
        supporting the buffer protocol; each image is simply written to the
        buffer sequentially.

        The *format*, *use_video_port*, *splitter_port*, *resize*, and
        *options* parameters are the same as in :meth:`capture`.

        If *use_video_port* is ``False`` (the default), the *burst* parameter
        can be used to make still port captures faster.  Specifically, this
        prevents the preview from switching resolutions between captures which
        significantly speeds up consecutive captures from the still port. The
        downside is that this mode is currently has several bugs; the major
        issue is that if captures are performed too quickly some frames will
        come back severely underexposed. It is recommended that users avoid the
        *burst* parameter unless they absolutely require it and are prepared to
        work around such issues.

        For example, to capture 60 images with a one second delay between them,
        writing the output to a series of JPEG files named image01.jpg,
        image02.jpg, etc. one could do the following::

            import time
            import picamera
            with picamera.PiCamera() as camera:
                camera.start_preview()
                try:
                    for i, filename in enumerate(
                            camera.capture_continuous('image{counter:02d}.jpg')):
                        print(filename)
                        time.sleep(1)
                        if i == 59:
                            break
                finally:
                    camera.stop_preview()

        Alternatively, to capture JPEG frames as fast as possible into an
        in-memory stream, performing some processing on each stream until
        some condition is satisfied::

            import io
            import time
            import picamera
            with picamera.PiCamera() as camera:
                stream = io.BytesIO()
                for foo in camera.capture_continuous(stream, format='jpeg'):
                    # Truncate the stream to the current position (in case
                    # prior iterations output a longer image)
                    stream.truncate()
                    stream.seek(0)
                    if process(stream):
                        break

        .. versionchanged:: 1.0
            The *resize* parameter was added, and raw capture formats can now
            be specified directly

        .. versionchanged:: 1.3
            The *splitter_port* parameter was added

        .. versionchanged:: 1.11
            Support for buffer outputs was added.
        """"""
        if use_video_port:
            if burst:
                raise PiCameraValueError(
                    'burst is only valid with still port captures')
            if bayer:
                raise PiCameraValueError(
                    'bayer is only valid with still port captures')
        with self._encoders_lock:
            camera_port, output_port = self._get_ports(use_video_port, splitter_port)
            format = self._get_image_format(output, format)
            encoder = self._get_image_encoder(
                    camera_port, output_port, format, resize, **options)
            if use_video_port:
                self._encoders[splitter_port] = encoder
        try:
            if burst:
                camera_port.params[mmal.MMAL_PARAMETER_CAMERA_BURST_CAPTURE] = True
            try:
                if isinstance(output, bytes):
                    # If we're fed a bytes string, assume it's UTF-8 encoded
                    # and convert it to Unicode. Technically this is wrong
                    # (file-systems use all sorts of encodings), but UTF-8 is a
                    # reasonable default and this keeps compatibility with
                    # Python 2 simple although it breaks the edge cases of
                    # non-UTF-8 encoded bytes strings with non-UTF-8 encoded
                    # file-systems
                    output = output.decode('utf-8')
                if isinstance(output, str):
                    counter = 1
                    while True:
                        filename = output.format(
                            counter=counter,
                            timestamp=datetime.datetime.now(),
                            )
                        if bayer:
                            camera_port.params[mmal.MMAL_PARAMETER_ENABLE_RAW_CAPTURE] = True
                        encoder.start(filename)
                        if not encoder.wait(self.CAPTURE_TIMEOUT):
                            raise PiCameraRuntimeError(
                                'Timed out waiting for capture to end')
                        yield filename
                        counter += 1
                else:
                    while True:
                        if bayer:
                            camera_port.params[mmal.MMAL_PARAMETER_ENABLE_RAW_CAPTURE] = True
                        encoder.start(output)
                        if not encoder.wait(self.CAPTURE_TIMEOUT):
                            raise PiCameraRuntimeError(
                                'Timed out waiting for capture to end')
                        yield output
            finally:
                if burst:
                    camera_port.params[mmal.MMAL_PARAMETER_CAMERA_BURST_CAPTURE] = False
        finally:
            encoder.close()
            with self._encoders_lock:
                if use_video_port:
                    del self._encoders[splitter_port]

    @property
    def closed(self):
        """"""
        Returns ``True`` if the :meth:`close` method has been called.
        """"""
        return not self._camera

    @property
    def recording(self):
        """"""
        Returns ``True`` if the :meth:`start_recording` method has been called,
        and no :meth:`stop_recording` call has been made yet.
        """"""
        return any(
                isinstance(e, PiVideoEncoder) and e.active
                for e in self._encoders.values()
                )

    @property
    def previewing(self):
        """"""
        Returns ``True`` if the :meth:`start_preview` method has been called,
        and no :meth:`stop_preview` call has been made yet.

        .. deprecated:: 1.8
            Test whether :attr:`preview` is ``None`` instead.
        """"""
        warnings.warn(
            PiCameraDeprecated(
                'PiCamera.previewing is deprecated; test PiCamera.preview '
                'is not None instead'))
        return isinstance(self._preview, PiPreviewRenderer)

    @property
    def revision(self):
        """"""
        Returns a string representing the revision of the Pi's camera module.
        At the time of writing, the string returned is 'ov5647' for the V1
        module, and 'imx219' for the V2 module.
        """"""
        return self._revision

    @property
    def exif_tags(self):
        """"""
        Holds a mapping of the Exif tags to apply to captured images.

        .. note::

            Please note that Exif tagging is only supported with the ``jpeg``
            format.

        By default several Exif tags are automatically applied to any images
        taken with the :meth:`capture` method: ``IFD0.Make`` (which is set to
        ``RaspberryPi``), ``IFD0.Model`` (which is set to the camera's revision
        string), and three timestamp tags: ``IFD0.DateTime``,
        ``EXIF.DateTimeOriginal``, and ``EXIF.DateTimeDigitized`` which are all
        set to the current date and time just before the picture is taken.

        If you wish to set additional Exif tags, or override any of the
        aforementioned tags, simply add entries to the exif_tags map before
        calling :meth:`capture`. For example::

            camera.exif_tags['IFD0.Copyright'] = 'Copyright (c) 2013 Foo Industries'

        The Exif standard mandates ASCII encoding for all textual values, hence
        strings containing non-ASCII characters will cause an encoding error to
        be raised when :meth:`capture` is called.  If you wish to set binary
        values, use a :func:`bytes` value::

            camera.exif_tags['EXIF.UserComment'] = b'Something containing\\x00NULL characters'

        .. warning::

            Binary Exif values are currently ignored; this appears to be a
            libmmal or firmware bug.

        You may also specify datetime values, integer, or float values, all of
        which will be converted to appropriate ASCII strings (datetime values
        are formatted as ``YYYY:MM:DD HH:MM:SS`` in accordance with the Exif
        standard).

        The currently supported Exif tags are:

        +-------+-------------------------------------------------------------+
        | Group | Tags                                                        |
        +=======+=============================================================+
        | IFD0, | ImageWidth, ImageLength, BitsPerSample, Compression,        |
        | IFD1  | PhotometricInterpretation, ImageDescription, Make, Model,   |
        |       | StripOffsets, Orientation, SamplesPerPixel, RowsPerString,  |
        |       | StripByteCounts, Xresolution, Yresolution,                  |
        |       | PlanarConfiguration, ResolutionUnit, TransferFunction,      |
        |       | Software, DateTime, Artist, WhitePoint,                     |
        |       | PrimaryChromaticities, JPEGInterchangeFormat,               |
        |       | JPEGInterchangeFormatLength, YcbCrCoefficients,             |
        |       | YcbCrSubSampling, YcbCrPositioning, ReferenceBlackWhite,    |
        |       | Copyright                                                   |
        +-------+-------------------------------------------------------------+
        | EXIF  | ExposureTime, FNumber, ExposureProgram,                     |
        |       | SpectralSensitivity, ISOSpeedRatings, OECF, ExifVersion,    |
        |       | DateTimeOriginal, DateTimeDigitized,                        |
        |       | ComponentsConfiguration, CompressedBitsPerPixel,            |
        |       | ShutterSpeedValue, ApertureValue, BrightnessValue,          |
        |       | ExposureBiasValue, MaxApertureValue, SubjectDistance,       |
        |       | MeteringMode, LightSource, Flash, FocalLength, SubjectArea, |
        |       | MakerNote, UserComment, SubSecTime, SubSecTimeOriginal,     |
        |       | SubSecTimeDigitized, FlashpixVersion, ColorSpace,           |
        |       | PixelXDimension, PixelYDimension, RelatedSoundFile,         |
        |       | FlashEnergy, SpacialFrequencyResponse,                      |
        |       | FocalPlaneXResolution, FocalPlaneYResolution,               |
        |       | FocalPlaneResolutionUnit, SubjectLocation, ExposureIndex,   |
        |       | SensingMethod, FileSource, SceneType, CFAPattern,           |
        |       | CustomRendered, ExposureMode, WhiteBalance,                 |
        |       | DigitalZoomRatio, FocalLengthIn35mmFilm, SceneCaptureType,  |
        |       | GainControl, Contrast, Saturation, Sharpness,               |
        |       | DeviceSettingDescription, SubjectDistanceRange,             |
        |       | ImageUniqueID                                               |
        +-------+-------------------------------------------------------------+
        | GPS   | GPSVersionID, GPSLatitudeRef, GPSLatitude, GPSLongitudeRef, |
        |       | GPSLongitude, GPSAltitudeRef, GPSAltitude, GPSTimeStamp,    |
        |       | GPSSatellites, GPSStatus, GPSMeasureMode, GPSDOP,           |
        |       | GPSSpeedRef, GPSSpeed, GPSTrackRef, GPSTrack,               |
        |       | GPSImgDirectionRef, GPSImgDirection, GPSMapDatum,           |
        |       | GPSDestLatitudeRef, GPSDestLatitude, GPSDestLongitudeRef,   |
        |       | GPSDestLongitude, GPSDestBearingRef, GPSDestBearing,        |
        |       | GPSDestDistanceRef, GPSDestDistance, GPSProcessingMethod,   |
        |       | GPSAreaInformation, GPSDateStamp, GPSDifferential           |
        +-------+-------------------------------------------------------------+
        | EINT  | InteroperabilityIndex, InteroperabilityVersion,             |
        |       | RelatedImageFileFormat, RelatedImageWidth,                  |
        |       | RelatedImageLength                                          |
        +-------+-------------------------------------------------------------+
        """"""
        return self._exif_tags

    def _set_led(self, value):
        if not self._used_led:
            global GPIO
            if GPIO:
                try:
                    GPIO.setmode(GPIO.BCM)
                    GPIO.setwarnings(False)
                    GPIO.setup(self._led_pin, GPIO.OUT, initial=GPIO.LOW)
                    self._used_led = True
                except RuntimeError:
                    # We're probably not running as root. In this case, forget the
                    # GPIO reference so we don't try anything further
                    GPIO = None

        if not GPIO:
            raise PiCameraRuntimeError(
                ""GPIO library not found, or not accessible; please install ""
                ""RPi.GPIO or run the script as root"")
        GPIO.output(self._led_pin, bool(value))
    led = property(None, _set_led, doc=""""""
        Sets the state of the camera's LED via GPIO.

        If a GPIO library is available (only RPi.GPIO is currently supported),
        and if the python process has the necessary privileges (typically this
        means running as root via sudo), this property can be used to set the
        state of the camera's LED as a boolean value (``True`` is on, ``False``
        is off).

        .. note::

            This is a write-only property. While it can be used to control the
            camera's LED, you cannot query the state of the camera's LED using
            this property.

        .. note::

            At present, the camera's LED cannot be controlled on the Pi 3 or 3+
            (the GPIOs used to control the camera LED were re-routed to GPIO
            expander on these models).

        .. warning::

            There are circumstances in which the camera firmware may override
            an existing LED setting. For example, in the case that the firmware
            resets the camera (as can happen with a CSI-2 timeout), the LED may
            also be reset. If you wish to guarantee that the LED remain off at
            all times, you may prefer to use the ``disable_camera_led`` option
            in `config.txt`_ (this has the added advantage that GPIO access is
            not required, at least for LED control).

        .. _config.txt: https://www.raspberrypi.org/documentation/configuration/config-txt.md
        """""")

    def _get_raw_format(self):
        warnings.warn(
            PiCameraDeprecated(
                'PiCamera.raw_format is deprecated; use required format '
                'directly with capture methods instead'))
        return self._raw_format
    def _set_raw_format(self, value):
        warnings.warn(
            PiCameraDeprecated(
                'PiCamera.raw_format is deprecated; use required format '
                'directly with capture methods instead'))
        if value not in self.RAW_FORMATS:
            raise PiCameraValueError(""Invalid raw format: %s"" % value)
        self._raw_format = value
    raw_format = property(_get_raw_format, _set_raw_format, doc=""""""
        Retrieves or sets the raw format of the camera's ports.

        .. deprecated:: 1.0
            Please use ``'yuv'`` or ``'rgb'`` directly as a format in the
            various capture methods instead.
        """""")

    def _get_timestamp(self):
        self._check_camera_open()
        return self._camera.control.params[mmal.MMAL_PARAMETER_SYSTEM_TIME]
    timestamp = property(_get_timestamp, doc=""""""
        Retrieves the system time according to the camera firmware.

        The camera's timestamp is a 64-bit integer representing the number of
        microseconds since the last system boot. When the camera's
        :attr:`clock_mode` is ``'raw'`` the values returned by this attribute
        are comparable to those from the :attr:`frame`
        :attr:`~PiVideoFrame.timestamp` attribute.
        """""")

    def _get_frame(self):
        self._check_camera_open()
        for e in self._encoders.values():
            try:
                return e.frame
            except AttributeError:
                pass
        raise PiCameraRuntimeError(
            ""Cannot query frame information when camera is not recording"")
    frame = property(_get_frame, doc=""""""
        Retrieves information about the current frame recorded from the camera.

        When video recording is active (after a call to
        :meth:`start_recording`), this attribute will return a
        :class:`PiVideoFrame` tuple containing information about the current
        frame that the camera is recording.

        If multiple video recordings are currently in progress (after multiple
        calls to :meth:`start_recording` with different values for the
        ``splitter_port`` parameter), which encoder's frame information is
        returned is arbitrary. If you require information from a specific
        encoder, you will need to extract it from :attr:`_encoders` explicitly.

        Querying this property when the camera is not recording will result in
        an exception.

        .. note::

            There is a small window of time when querying this attribute will
            return ``None`` after calling :meth:`start_recording`. If this
            attribute returns ``None``, this means that the video encoder has
            been initialized, but the camera has not yet returned any frames.
        """""")

    def _disable_camera(self):
        """"""
        An internal method for disabling the camera, e.g. for re-configuration.
        This disables the splitter and preview connections (if they exist).
        """"""
        self._splitter.connection.disable()
        self._preview.renderer.connection.disable()
        self._camera.disable()

    def _enable_camera(self):
        """"""
        An internal method for enabling the camera after re-configuration.
        This ensures the splitter configuration is consistent, then re-enables
        the camera along with the splitter and preview connections.
        """"""
        self._camera.enable()
        self._preview.renderer.connection.enable()
        self._splitter.connection.enable()

    def _configure_splitter(self):
        """"""
        Ensures the splitter has the same format as the attached camera
        output port (the video port).

        This method is used to ensure the splitter configuration is sane,
        typically after :meth:`_configure_camera` is called.
        """"""
        self._splitter.inputs[0].copy_from(
            self._camera.outputs[self.CAMERA_VIDEO_PORT])
        self._splitter.inputs[0].commit()

    def _control_callback(self, port, buf):
        try:
            if buf.command == mmal.MMAL_EVENT_ERROR:
                raise PiCameraRuntimeError(
                    ""No data received from sensor. Check all connections, ""
                    ""including the SUNNY chip on the camera board"")
            elif buf.command != mmal.MMAL_EVENT_PARAMETER_CHANGED:
                raise PiCameraRuntimeError(
                    ""Received unexpected camera control callback event, ""
                    ""0x%08x"" % buf[0].cmd)
        except Exception as exc:
            # Pass the exception to the main thread; next time
            # check_camera_open() is called this will get raised
            self._camera_exception = exc

    def _get_config(self):
        """"""
        An internal method for obtaining configuration data to pass to the
        :meth:`_configure_camera` method. This is a namedtuple consisting of
        all configuration that cannot be changed while the camera is active.
        """"""
        port_num = (
            self.CAMERA_VIDEO_PORT
            if self._encoders else
            self.CAMERA_PREVIEW_PORT
            )
        framerate = Fraction(self._camera.outputs[port_num].framerate)
        if framerate == 0:
            mp = self._camera.outputs[port_num].params[
                mmal.MMAL_PARAMETER_FPS_RANGE]
            framerate = mo.PiFramerateRange(mp.fps_low, mp.fps_high)
        return PiCameraConfig(
            sensor_mode=self._camera.control.params[
                mmal.MMAL_PARAMETER_CAMERA_CUSTOM_SENSOR_CONFIG],
            clock_mode=self._camera_config.use_stc_timestamp,
            resolution=mo.PiResolution(
                int(self._camera_config.max_stills_w),
                int(self._camera_config.max_stills_h)
                ),
            framerate=framerate,
            isp_blocks=self._camera.control.params[
                mmal.MMAL_PARAMETER_CAMERA_ISP_BLOCK_OVERRIDE],
            colorspace=self._camera.outputs[0].colorspace
        )

    def _configure_camera(self, old, new):
        """"""
        An internal method for setting a new camera mode, framerate,
        resolution, clock_mode, and/or ISP blocks.

        This method is used by the setters of the :attr:`resolution`,
        :attr:`framerate`, :attr:`framerate_range`, :attr:`sensor_mode`, and
        :attr:`isp_blocks` properties. It assumes the camera is currently
        disabled.

        The *old* and *new* arguments are :class:`PiCameraConfig` structures.
        Both are required to ensure correct operation on older firmwares
        (specifically that we don't try to set the sensor mode when both old
        and new modes are 0 or automatic).
        """"""
        old_cc = mmal.MMAL_PARAMETER_CAMERA_CONFIG_T.from_buffer_copy(
            self._camera_config)
        old_ports = [
            (
                port.framesize,
                port.framerate,
                port.params[mmal.MMAL_PARAMETER_FPS_RANGE]
            )
            for port in self._camera.outputs
        ]
        if old.sensor_mode != 0 or new.sensor_mode != 0:
            # Old firmware support: only attempt to set sensor mode when
            # explicitly requested
            self._camera.control.params[
                mmal.MMAL_PARAMETER_CAMERA_CUSTOM_SENSOR_CONFIG
                ] = new.sensor_mode
        if not self._camera.control.enabled:
            # One-time initial setup
            self._camera.control.enable(self._control_callback)
            preview_resolution = new.resolution
        elif (
                self._camera.outputs[self.CAMERA_PREVIEW_PORT].framesize ==
                self._camera.outputs[self.CAMERA_VIDEO_PORT].framesize
                ):
            preview_resolution = new.resolution
        else:
            preview_resolution = self._camera.outputs[
                self.CAMERA_PREVIEW_PORT].framesize
        try:
            # Old firmware support: only attempt to set ISP block override when
            # explicitly requested
            if (
                    old.isp_blocks not in (0, 0xFFFFFFFF) or
                    new.isp_blocks not in (0, 0xFFFFFFFF)):
                self._camera.control.params[
                    mmal.MMAL_PARAMETER_CAMERA_ISP_BLOCK_OVERRIDE
                    ] = new.isp_blocks
            try:
                fps_low, fps_high = new.framerate
            except TypeError:
                fps_low = fps_high = new.framerate
            else:
                new = new._replace(framerate=0)
            fps_range = mmal.MMAL_PARAMETER_FPS_RANGE_T(
                mmal.MMAL_PARAMETER_HEADER_T(
                    mmal.MMAL_PARAMETER_FPS_RANGE,
                    ct.sizeof(mmal.MMAL_PARAMETER_FPS_RANGE_T)
                ),
                fps_low=mo.to_rational(fps_low),
                fps_high=mo.to_rational(fps_high),
                )

            cc = self._camera_config
            cc.max_stills_w = new.resolution.width
            cc.max_stills_h = new.resolution.height
            cc.stills_yuv422 = 0
            cc.one_shot_stills = 1
            cc.max_preview_video_w = new.resolution.width
            cc.max_preview_video_h = new.resolution.height
            cc.num_preview_video_frames = max(3, fps_high // 10)
            cc.stills_capture_circular_buffer_height = 0
            cc.fast_preview_resume = 0
            cc.use_stc_timestamp = new.clock_mode
            self._camera.control.params[mmal.MMAL_PARAMETER_CAMERA_CONFIG] = cc

            # Clamp preview resolution to camera's resolution
            if (
                    preview_resolution.width > new.resolution.width or
                    preview_resolution.height > new.resolution.height
                    ):
                preview_resolution = new.resolution
            for port in self._camera.outputs:
                port.params[mmal.MMAL_PARAMETER_FPS_RANGE] = fps_range
                if port.index == self.CAMERA_PREVIEW_PORT:
                    port.framesize = preview_resolution
                else:
                    port.framesize = new.resolution
                port.framerate = new.framerate
                port.colorspace = new.colorspace
                port.commit()
        except:
            # If anything goes wrong, restore original resolution and
            # framerate otherwise the camera can be left in unusual states
            # (camera config not matching ports, etc).
            self._camera.control.params[mmal.MMAL_PARAMETER_CAMERA_CONFIG] = old_cc
            self._camera_config = old_cc
            for port, (res, fps, fps_range) in zip(self._camera.outputs, old_ports):
                port.framesize = res
                port.framerate = fps
                port.params[mmal.MMAL_PARAMETER_FPS_RANGE] = fps_range
                port.commit()
            raise

    def _get_framerate(self):
        self._check_camera_open()
        port_num = (
            self.CAMERA_VIDEO_PORT
            if self._encoders else
            self.CAMERA_PREVIEW_PORT
            )
        return mo.PiCameraFraction(self._camera.outputs[port_num].framerate)
    def _set_framerate(self, value):
        self._check_camera_open()
        self._check_recording_stopped()
        value = mo.to_fraction(value, den_limit=256)
        if not (0 < value <= self.MAX_FRAMERATE):
            raise PiCameraValueError(""Invalid framerate: %.2ffps"" % value)
        config = self._get_config()
        self._disable_camera()
        self._configure_camera(config, config._replace(framerate=value))
        self._configure_splitter()
        self._enable_camera()
    framerate = property(_get_framerate, _set_framerate, doc=""""""\
        Retrieves or sets the framerate at which video-port based image
        captures, video recordings, and previews will run.

        When queried, the :attr:`framerate` property returns the rate at which
        the camera's video and preview ports will operate as a
        :class:`~fractions.Fraction` instance (which can be easily converted to
        an :class:`int` or :class:`float`). If :attr:`framerate_range` has been
        set, then :attr:`framerate` will be 0 which indicates that a dynamic
        range of framerates is being used.

        .. note::

            For backwards compatibility, a derivative of the
            :class:`~fractions.Fraction` class is actually used which permits
            the value to be treated as a tuple of ``(numerator, denominator)``.

            Setting and retrieving framerate as a ``(numerator, denominator)``
            tuple is deprecated and will be removed in 2.0. Please use a
            :class:`~fractions.Fraction` instance instead (which is just as
            accurate and also permits direct use with math operators).

        When set, the property configures the camera so that the next call to
        recording and previewing methods will use the new framerate. Setting
        this property implicitly sets :attr:`framerate_range` so that the low
        and high values are equal to the new framerate. The framerate can be
        specified as an :ref:`int <typesnumeric>`, :ref:`float <typesnumeric>`,
        :class:`~fractions.Fraction`, or a ``(numerator, denominator)`` tuple.
        For example, the following definitions are all equivalent::

            from fractions import Fraction

            camera.framerate = 30
            camera.framerate = 30 / 1
            camera.framerate = Fraction(30, 1)
            camera.framerate = (30, 1) # deprecated

        The camera must not be closed, and no recording must be active when the
        property is set.

        .. note::

            This attribute, in combination with :attr:`resolution`, determines
            the mode that the camera operates in. The actual sensor framerate
            and resolution used by the camera is influenced, but not directly
            set, by this property. See :attr:`sensor_mode` for more
            information.

        The initial value of this property can be specified with the
        *framerate* parameter in the :class:`PiCamera` constructor, and will
        default to 30 if not specified.
        """""")

    @property
    def sensor_modes(self):
        """"""
        Returns a mapping describing the available sensor modes for the
        camera's :attr:`revision`.

        This read-only attribute returns a dictionary mapping sensor mode
        numbers (1..7) to instances of :class:`PiSensorMode` which contain the
        resolution, range of framerates, and other details about the mode.
        Note that the default mode (0) is not represented, as this indicates
        that the mode should be selected automatically by the firmware based
        on the requested :attr:`resolution` and :attr:`framerate`.

        .. versionadded:: 1.14
        """"""
        return PiCamera.SENSOR_MODES[self.revision]

    def _get_sensor_mode(self):
        self._check_camera_open()
        return self._camera.control.params[
            mmal.MMAL_PARAMETER_CAMERA_CUSTOM_SENSOR_CONFIG]
    def _set_sensor_mode(self, value):
        self._check_camera_open()
        self._check_recording_stopped()
        try:
            if not (0 <= value <= 7):
                raise PiCameraValueError(
                    ""Invalid sensor mode: %d (valid range 0..7)"" % value)
        except TypeError:
            raise PiCameraValueError(""Invalid sensor mode: %s"" % value)
        config = self._get_config()
        self._disable_camera()
        self._configure_camera(config, config._replace(sensor_mode=value))
        self._configure_splitter()
        self._enable_camera()
    sensor_mode = property(_get_sensor_mode, _set_sensor_mode, doc=""""""\
        Retrieves or sets the input mode of the camera's sensor.

        This is an advanced property which can be used to control the camera's
        sensor mode. By default, mode 0 is used which allows the camera to
        automatically select an input mode based on the requested
        :attr:`resolution` and :attr:`framerate`. Valid values are currently
        between 0 and 7. The set of valid sensor modes (along with the
        heuristic used to select one automatically) are detailed in the
        :ref:`camera_modes` section of the documentation.

        .. note::

            At the time of writing, setting this property does nothing unless
            the camera has been initialized with a sensor mode other than 0.
            Furthermore, some mode transitions appear to require setting the
            property twice (in a row). This appears to be a firmware
            limitation.

        The initial value of this property can be specified with the
        *sensor_mode* parameter in the :class:`PiCamera` constructor, and will
        default to 0 if not specified.

        .. versionadded:: 1.9
        """""")

    def _get_clock_mode(self):
        self._check_camera_open()
        return self._CLOCK_MODES_R[self._camera_config.use_stc_timestamp]
    def _set_clock_mode(self, value):
        self._check_camera_open()
        self._check_recording_stopped()
        try:
            clock_mode = self.CLOCK_MODES[value]
        except KeyError:
            raise PiCameraValueError(""Invalid clock mode %s"" % value)
        config = self._get_config()
        self._disable_camera()
        self._configure_camera(config, config._replace(clock_mode=clock_mode))
        self._configure_splitter()
        self._enable_camera()
    clock_mode = property(_get_clock_mode, _set_clock_mode, doc=""""""\
        Retrieves or sets the mode of the camera's clock.

        This is an advanced property which can be used to control the nature of
        the frame timestamps available from the :attr:`frame` property. When
        this is ""reset"" (the default) each frame's timestamp will be relative
        to the start of the recording. When this is ""raw"", each frame's
        timestamp will be relative to the last initialization of the camera.

        The initial value of this property can be specified with the
        *clock_mode* parameter in the :class:`PiCamera` constructor, and will
        default to ""reset"" if not specified.

        .. versionadded:: 1.11
        """""")

    def _get_isp_blocks(self):
        self._check_camera_open()
        # XXX Older firmware support?
        value = self._camera.control.params[
            mmal.MMAL_PARAMETER_CAMERA_ISP_BLOCK_OVERRIDE]
        return {
            v for k, v in self._ISP_BLOCKS_R.items()
            if value == 0 or k & value
            }
    def _set_isp_blocks(self, value):
        self._check_camera_open()
        self._check_recording_stopped()
        value = set(value)
        invalid = value - set(self.ISP_BLOCKS.keys())
        if invalid:
            raise PiCameraValueerror(""Invalid ISP block %s"" % invalid.pop())
        isp_blocks = reduce(and_, (~v for k, v in self.ISP_BLOCKS.items()
                                  if k not in value), 0xFFFFFFFF)
        config = self._get_config()
        self._disable_camera()
        self._configure_camera(config, config._replace(isp_blocks=isp_blocks))
        self._configure_splitter()
        self._enable_camera()
    isp_blocks = property(_get_isp_blocks, _set_isp_blocks, doc=""""""\
        Retrieves or sets which ISP blocks are enabled for processing.

        This is an advanced property which can be used to disable various
        processes within the camera's firmware. The value is a set of strings
        indicating which blocks are currently active. Valid strings that can
        be included are:

        {values}

        It is strongly recommended that modifications to this property are
        done by union or difference rather than straight assignment. Control
        for further ISP blocks may be added in future and this will ensure
        your code does not inadvertantly disable blocks it does not intend to.
        For instance to disable the block responsible for AWB gains::

            camera.isp_blocks -= {{'white-balance'}}

        Then to re-enable the same block::

            camera.isp_blocks |= {{'white-balance'}}

        The camera must not be closed, and no recording must be active when the
        property is set.

        .. versionadded:: 1.14
        """""".format(values=docstring_values(ISP_BLOCKS)))

    def _get_colorspace(self):
        return self._COLORSPACES_R[self._camera.outputs[0].colorspace]
    def _set_colorspace(self, value):
        self._check_camera_open()
        self._check_recording_stopped()
        try:
            colorspace = self.COLORSPACES[value]
        except KeyError:
            raise PiCameraValueError(""Invalid colorspace %s"" % value)
        config = self._get_config()
        self._disable_camera()
        self._configure_camera(config, config._replace(colorspace=colorspace))
        self._configure_splitter()
        self._enable_camera()
    colorspace = property(_get_colorspace, _set_colorspace, doc=""""""\
        Retrieves or sets the `color space`_ that the camera uses for
        conversion between the `YUV`_ and RGB systems.

        The value is a string that represents which of a series of fixed
        conversion tables are used by the camera firmware (the firmware works
        largely in the YUV color system internally). The following strings are
        the valid values:

        {values}

        The ""bt601"" and ""bt709"" values correspond to the standard `SDTV and
        HDTV tables`_. The ""auto"" value is the default and corresponds to
        ""bt601"" in practice. One of these values is likely what you want when
        recording H.264 video. However, when recording MJPEG video, you may
        want to use the ""jfif"" table instead as it produces luma values in the
        0-255 range, rather than the 16-235 range produced by the standard
        tables.

        The camera must not be closed, and no recording must be active when the
        property is set.

        .. _color space: https://en.wikipedia.org/wiki/Color_space
        .. _YUV: https://en.wikipedia.org/wiki/YUV
        .. _SDTV and HDTV tables: https://en.wikipedia.org/wiki/YUV#Conversion_to/from_RGB
        """""".format(values=docstring_values(COLORSPACES)))

    def _get_resolution(self):
        self._check_camera_open()
        return mo.PiResolution(
            int(self._camera_config.max_stills_w),
            int(self._camera_config.max_stills_h)
            )
    def _set_resolution(self, value):
        self._check_camera_open()
        self._check_recording_stopped()
        value = mo.to_resolution(value)
        if not (
                (0 < value.width <= self.MAX_RESOLUTION.width) and
                (0 < value.height <= self.MAX_RESOLUTION.height)):
            raise PiCameraValueError(
                    ""Invalid resolution requested: %r"" % (value,))
        config = self._get_config()
        self._disable_camera()
        self._configure_camera(config, config._replace(resolution=value))
        self._configure_splitter()
        self._enable_camera()
    resolution = property(_get_resolution, _set_resolution, doc=""""""
        Retrieves or sets the resolution at which image captures, video
        recordings, and previews will be captured.

        When queried, the :attr:`resolution` property returns the resolution at
        which the camera will operate as a tuple of ``(width, height)``
        measured in pixels. This is the resolution that the :meth:`capture`
        method will produce images at, and the resolution that
        :meth:`start_recording` will produce videos at.

        When set, the property configures the camera so that the next call to
        these methods will use the new resolution.  The resolution can be
        specified as a ``(width, height)`` tuple, as a string formatted
        ``'WIDTHxHEIGHT'``, or as a string containing a commonly recognized
        `display resolution`_ name (e.g. ""VGA"", ""HD"", ""1080p"", etc). For
        example, the following definitions are all equivalent::

            camera.resolution = (1280, 720)
            camera.resolution = '1280x720'
            camera.resolution = '1280 x 720'
            camera.resolution = 'HD'
            camera.resolution = '720p'

        The camera must not be closed, and no recording must be active when the
        property is set.

        .. note::

            This attribute, in combination with :attr:`framerate`, determines
            the mode that the camera operates in. The actual sensor framerate
            and resolution used by the camera is influenced, but not directly
            set, by this property. See :attr:`sensor_mode` for more
            information.

        The initial value of this property can be specified with the
        *resolution* parameter in the :class:`PiCamera` constructor, and will
        default to the display's resolution or 1280x720 if the display has
        been disabled (with ``tvservice -o``).

        .. versionchanged:: 1.11
            Resolution permitted to be set as a string. Preview resolution
            added as separate property.

        .. _display resolution: https://en.wikipedia.org/wiki/Graphics_display_resolution
        """""")

    def _get_framerate_range(self):
        self._check_camera_open()
        port_num = (
            self.CAMERA_VIDEO_PORT
            if self._encoders else
            self.CAMERA_PREVIEW_PORT
            )
        mp = self._camera.outputs[port_num].params[mmal.MMAL_PARAMETER_FPS_RANGE]
        return mo.PiFramerateRange(mp.fps_low, mp.fps_high)
    def _set_framerate_range(self, value):
        self._check_camera_open()
        self._check_recording_stopped()
        low, high = value
        low = mo.to_fraction(low, den_limit=256)
        high = mo.to_fraction(high, den_limit=256)
        if not (0 < low <= self.MAX_FRAMERATE):
            raise PiCameraValueError(""Invalid low framerate: %.2ffps"" % low)
        if not (0 < high <= self.MAX_FRAMERATE):
            raise PiCameraValueError(""Invalid high framerate: %.2ffps"" % high)
        if high < low:
            raise PiCameraValueError(""framerate_range is backwards"")
        config = self._get_config()
        self._disable_camera()
        self._configure_camera(config, config._replace(framerate=(low, high)))
        self._configure_splitter()
        self._enable_camera()
    framerate_range = property(_get_framerate_range, _set_framerate_range, doc=""""""\
        Retrieves or sets a range between which the camera's framerate is
        allowed to float.

        When queried, the :attr:`framerate_range` property returns a
        :func:`~collections.namedtuple` derivative with ``low`` and ``high``
        components (index 0 and 1 respectively) which specify the limits of the
        permitted framerate range.

        When set, the property configures the camera so that the next call to
        recording and previewing methods will use the new framerate range.
        Setting this property will implicitly set the :attr:`framerate`
        property to 0 (indicating that a dynamic range of framerates is in use
        by the camera).

        .. note::

            Use of this property prevents use of :attr:`framerate_delta` (there
            would be little point in making fractional adjustments to the
            framerate when the framerate itself is variable).

        The low and high framerates can be specified as :ref:`int
        <typesnumeric>`, :ref:`float <typesnumeric>`, or
        :class:`~fractions.Fraction` values. For example, the following
        definitions are all equivalent::

            from fractions import Fraction

            camera.framerate_range = (0.16666, 30)
            camera.framerate_range = (Fraction(1, 6), 30 / 1)
            camera.framerate_range = (Fraction(1, 6), Fraction(30, 1))

        The camera must not be closed, and no recording must be active when the
        property is set.

        .. note::

            This attribute, like :attr:`framerate`, determines the mode that
            the camera operates in. The actual sensor framerate and resolution
            used by the camera is influenced, but not directly set, by this
            property. See :attr:`sensor_mode` for more information.

        .. versionadded:: 1.13
        """""")

    def _get_framerate_delta(self):
        self._check_camera_open()
        if self.framerate == 0:
            raise PiCameraValueError(
                'framerate_delta cannot be used with framerate_range')
        port_num = (
            self.CAMERA_VIDEO_PORT
            if self._encoders else
            self.CAMERA_PREVIEW_PORT
            )
        return self._camera.outputs[port_num].params[
            mmal.MMAL_PARAMETER_FRAME_RATE] - self.framerate
    def _set_framerate_delta(self, value):
        self._check_camera_open()
        if self.framerate == 0:
            raise PiCameraValueError(
                'framerate_delta cannot be used with framerate_range')
        value = mo.to_fraction(self.framerate + value, den_limit=256)
        self._camera.outputs[self.CAMERA_PREVIEW_PORT].params[
            mmal.MMAL_PARAMETER_FRAME_RATE] = value
        self._camera.outputs[self.CAMERA_VIDEO_PORT].params[
            mmal.MMAL_PARAMETER_FRAME_RATE] = value
    framerate_delta = property(_get_framerate_delta, _set_framerate_delta, doc=""""""\
        Retrieves or sets a fractional amount that is added to the camera's
        framerate for the purpose of minor framerate adjustments.

        When queried, the :attr:`framerate_delta` property returns the amount
        that the camera's :attr:`framerate` has been adjusted. This defaults
        to 0 (so the camera's framerate is the actual framerate used).

        When set, the property adjusts the camera's framerate on the fly. The
        property can be set while recordings or previews are in progress. Thus
        the framerate used by the camera is actually :attr:`framerate` +
        :attr:`framerate_delta`.

        .. note::

            Framerates deltas can be fractional with adjustments as small as
            1/256th of an fps possible (finer adjustments will be rounded).
            With an appropriately tuned PID controller, this can be used to
            achieve synchronization between the camera framerate and other
            devices.

        If the new framerate demands a mode switch (such as moving between a
        low framerate and a high framerate mode), currently active recordings
        may drop a frame. This should only happen when specifying quite large
        deltas, or when framerate is at the boundary of a sensor mode (e.g.
        49fps).

        The framerate delta can be specified as an :ref:`int <typesnumeric>`,
        :ref:`float <typesnumeric>`, :class:`~fractions.Fraction` or a
        ``(numerator, denominator)`` tuple. For example, the following
        definitions are all equivalent::

            from fractions import Fraction

            camera.framerate_delta = 0.5
            camera.framerate_delta = 1 / 2 # in python 3
            camera.framerate_delta = Fraction(1, 2)
            camera.framerate_delta = (1, 2) # deprecated

        .. note::

            This property is implicitly reset to 0 when :attr:`framerate` or
            :attr:`framerate_range` is set. When :attr:`framerate` is 0
            (indicating that :attr:`framerate_range` is set), this property
            cannot be used.  (there would be little point in making fractional
            adjustments to the framerate when the framerate itself is
            variable).

        .. versionadded:: 1.11
        """""")

    def _get_still_stats(self):
        self._check_camera_open()
        return self._camera.control.params[mmal.MMAL_PARAMETER_CAPTURE_STATS_PASS]
    def _set_still_stats(self, value):
        self._check_camera_open()
        self._camera.control.params[mmal.MMAL_PARAMETER_CAPTURE_STATS_PASS] = value
    still_stats = property(_get_still_stats, _set_still_stats, doc=""""""\
        Retrieves or sets whether statistics will be calculated from still
        frames or the prior preview frame.

        When queried, the :attr:`still_stats` property returns a boolean value
        indicating when scene statistics will be calculated for still captures
        (that is, captures where the *use_video_port* parameter of
        :meth:`capture` is ``False``).  When this property is ``False`` (the
        default), statistics will be calculated from the preceding preview
        frame (this also applies when the preview is not visible). When `True`,
        statistics will be calculated from the captured image itself.

        When set, the propetry controls when scene statistics will be
        calculated for still captures. The property can be set while recordings
        or previews are in progress. The default value is ``False``.

        The advantages to calculating scene statistics from the captured image
        are that time between startup and capture is reduced as only the AGC
        (automatic gain control) has to converge. The downside is that
        processing time for captures increases and that white balance and gain
        won't necessarily match the preview.

        .. warning::

            Enabling the still statistics pass will `override fixed white
            balance`_ gains (set via :attr:`awb_gains` and :attr:`awb_mode`).

        .. _override fixed white balance: https://www.raspberrypi.org/forums/viewtopic.php?p=875772&sid=92fa4ea70d1fe24590a4cdfb4a10c489#p875772

        .. versionadded:: 1.9
        """""")

    def _get_saturation(self):
        self._check_camera_open()
        return int(self._camera.control.params[mmal.MMAL_PARAMETER_SATURATION] * 100)
    def _set_saturation(self, value):
        self._check_camera_open()
        if not (-100 <= value <= 100):
            raise PiCameraValueError(
                ""Invalid saturation value: %d (valid range -100..100)"" % value)
        self._camera.control.params[mmal.MMAL_PARAMETER_SATURATION] = Fraction(value, 100)
    saturation = property(_get_saturation, _set_saturation, doc=""""""\
        Retrieves or sets the saturation setting of the camera.

        When queried, the :attr:`saturation` property returns the color
        saturation of the camera as an integer between -100 and 100. When set,
        the property adjusts the saturation of the camera. Saturation can be
        adjusted while previews or recordings are in progress. The default
        value is 0.
        """""")

    def _get_sharpness(self):
        self._check_camera_open()
        return int(self._camera.control.params[mmal.MMAL_PARAMETER_SHARPNESS] * 100)
    def _set_sharpness(self, value):
        self._check_camera_open()
        if not (-100 <= value <= 100):
            raise PiCameraValueError(
                ""Invalid sharpness value: %d (valid range -100..100)"" % value)
        self._camera.control.params[mmal.MMAL_PARAMETER_SHARPNESS] = Fraction(value, 100)
    sharpness = property(_get_sharpness, _set_sharpness, doc=""""""\
        Retrieves or sets the sharpness setting of the camera.

        When queried, the :attr:`sharpness` property returns the sharpness
        level of the camera (a measure of the amount of post-processing to
        reduce or increase image sharpness) as an integer between -100 and 100.
        When set, the property adjusts the sharpness of the camera. Sharpness
        can be adjusted while previews or recordings are in progress. The
        default value is 0.
        """""")

    def _get_contrast(self):
        self._check_camera_open()
        return int(self._camera.control.params[mmal.MMAL_PARAMETER_CONTRAST] * 100)
    def _set_contrast(self, value):
        self._check_camera_open()
        if not (-100 <= value <= 100):
            raise PiCameraValueError(
                ""Invalid contrast value: %d (valid range -100..100)"" % value)
        self._camera.control.params[mmal.MMAL_PARAMETER_CONTRAST] = Fraction(value, 100)
    contrast = property(_get_contrast, _set_contrast, doc=""""""\
        Retrieves or sets the contrast setting of the camera.

        When queried, the :attr:`contrast` property returns the contrast level
        of the camera as an integer between -100 and 100.  When set, the
        property adjusts the contrast of the camera. Contrast can be adjusted
        while previews or recordings are in progress. The default value is 0.
        """""")

    def _get_brightness(self):
        self._check_camera_open()
        return int(self._camera.control.params[mmal.MMAL_PARAMETER_BRIGHTNESS] * 100)
    def _set_brightness(self, value):
        self._check_camera_open()
        if not (0 <= value <= 100):
            raise PiCameraValueError(
                ""Invalid brightness value: %d (valid range 0..100)"" % value)
        self._camera.control.params[mmal.MMAL_PARAMETER_BRIGHTNESS] = Fraction(value, 100)
    brightness = property(_get_brightness, _set_brightness, doc=""""""\
        Retrieves or sets the brightness setting of the camera.

        When queried, the :attr:`brightness` property returns the brightness
        level of the camera as an integer between 0 and 100.  When set, the
        property adjusts the brightness of the camera. Brightness can be
        adjusted while previews or recordings are in progress. The default
        value is 50.
        """""")

    def _get_shutter_speed(self):
        self._check_camera_open()
        return int(self._camera.control.params[mmal.MMAL_PARAMETER_SHUTTER_SPEED])
    def _set_shutter_speed(self, value):
        self._check_camera_open()
        self._camera.control.params[mmal.MMAL_PARAMETER_SHUTTER_SPEED] = value
    shutter_speed = property(_get_shutter_speed, _set_shutter_speed, doc=""""""\
        Retrieves or sets the shutter speed of the camera in microseconds.

        When queried, the :attr:`shutter_speed` property returns the shutter
        speed of the camera in microseconds, or 0 which indicates that the
        speed will be automatically determined by the auto-exposure algorithm.
        Faster shutter times naturally require greater amounts of illumination
        and vice versa.

        When set, the property adjusts the shutter speed of the camera, which
        most obviously affects the illumination of subsequently captured
        images. Shutter speed can be adjusted while previews or recordings are
        running. The default value is 0 (auto).

        .. note::

            You can query the :attr:`exposure_speed` attribute to determine the
            actual shutter speed being used when this attribute is set to 0.
            Please note that this capability requires an up to date firmware
            (#692 or later).

        .. note::

            In later firmwares, this attribute is limited by the value of the
            :attr:`framerate` attribute. For example, if framerate is set to
            30fps, the shutter speed cannot be slower than 33,333µs (1/fps).
        """""")

    def _get_exposure_speed(self):
        self._check_camera_open()
        return self._camera.control.params[mmal.MMAL_PARAMETER_CAMERA_SETTINGS].exposure
    exposure_speed = property(_get_exposure_speed, doc=""""""\
        Retrieves the current shutter speed of the camera.

        When queried, this property returns the shutter speed currently being
        used by the camera. If you have set :attr:`shutter_speed` to a non-zero
        value, then :attr:`exposure_speed` and :attr:`shutter_speed` should be
        equal. However, if :attr:`shutter_speed` is set to 0 (auto), then you
        can read the actual shutter speed being used from this attribute.  The
        value is returned as an integer representing a number of microseconds.
        This is a read-only property.

        .. versionadded:: 1.6
        """""")

    def _get_analog_gain(self):
        self._check_camera_open()
        return mo.to_fraction(
            self._camera.control.params[mmal.MMAL_PARAMETER_CAMERA_SETTINGS].analog_gain)
    analog_gain = property(_get_analog_gain, doc=""""""\
        Retrieves the current analog gain of the camera.

        When queried, this property returns the analog gain currently being
        used by the camera. The value represents the analog gain of the sensor
        prior to digital conversion. The value is returned as a
        :class:`~fractions.Fraction` instance.

        .. versionadded:: 1.6
        """""")

    def _get_digital_gain(self):
        self._check_camera_open()
        return mo.to_fraction(
            self._camera.control.params[mmal.MMAL_PARAMETER_CAMERA_SETTINGS].digital_gain)
    digital_gain = property(_get_digital_gain, doc=""""""\
        Retrieves the current digital gain of the camera.

        When queried, this property returns the digital gain currently being
        used by the camera. The value represents the digital gain the camera
        applies after conversion of the sensor's analog output. The value is
        returned as a :class:`~fractions.Fraction` instance.

        .. versionadded:: 1.6
        """""")

    def _get_video_denoise(self):
        self._check_camera_open()
        return self._camera.control.params[mmal.MMAL_PARAMETER_VIDEO_DENOISE]
    def _set_video_denoise(self, value):
        self._check_camera_open()
        self._camera.control.params[mmal.MMAL_PARAMETER_VIDEO_DENOISE] = value
    video_denoise = property(_get_video_denoise, _set_video_denoise, doc=""""""\
        Retrieves or sets whether denoise will be applied to video recordings.

        When queried, the :attr:`video_denoise` property returns a boolean
        value indicating whether or not the camera software will apply a
        denoise algorithm to video recordings.

        When set, the property activates or deactivates the denoise algorithm
        for video recordings. The property can be set while recordings or
        previews are in progress. The default value is ``True``.

        .. versionadded:: 1.7
        """""")

    def _get_image_denoise(self):
        self._check_camera_open()
        return self._camera.control.params[mmal.MMAL_PARAMETER_STILLS_DENOISE]
    def _set_image_denoise(self, value):
        self._check_camera_open()
        self._camera.control.params[mmal.MMAL_PARAMETER_STILLS_DENOISE] = value
    image_denoise = property(_get_image_denoise, _set_image_denoise, doc=""""""\
        Retrieves or sets whether denoise will be applied to image captures.

        When queried, the :attr:`image_denoise` property returns a boolean
        value indicating whether or not the camera software will apply a
        denoise algorithm to image captures.

        When set, the property activates or deactivates the denoise algorithm
        for image captures. The property can be set while recordings or
        previews are in progress. The default value is ``True``.

        .. versionadded:: 1.7
        """""")

    def _get_drc_strength(self):
        self._check_camera_open()
        return self._DRC_STRENGTHS_R[
            self._camera.control.params[mmal.MMAL_PARAMETER_DYNAMIC_RANGE_COMPRESSION].strength
            ]
    def _set_drc_strength(self, value):
        self._check_camera_open()
        try:
            mp = self._camera.control.params[mmal.MMAL_PARAMETER_DYNAMIC_RANGE_COMPRESSION]
            mp.strength = self.DRC_STRENGTHS[value]
        except KeyError:
            raise PiCameraValueError(
                ""Invalid dynamic range compression strength: %s"" % value)
        self._camera.control.params[mmal.MMAL_PARAMETER_DYNAMIC_RANGE_COMPRESSION] = mp
    drc_strength = property(_get_drc_strength, _set_drc_strength, doc=""""""\
        Retrieves or sets the dynamic range compression strength of the camera.

        When queried, the :attr:`drc_strength` property returns a string
        indicating the amount of `dynamic range compression`_ the camera
        applies to images.

        When set, the attributes adjusts the strength of the dynamic range
        compression applied to the camera's output. Valid values are given
        in the list below:

        {values}

        The default value is ``'off'``. All possible values for the attribute
        can be obtained from the ``PiCamera.DRC_STRENGTHS`` attribute.

        .. warning::

            Enabling DRC will `override fixed white balance`_ gains (set via
            :attr:`awb_gains` and :attr:`awb_mode`).

        .. _dynamic range compression: https://en.wikipedia.org/wiki/Gain_compression
        .. _override fixed white balance: https://www.raspberrypi.org/forums/viewtopic.php?p=875772&sid=92fa4ea70d1fe24590a4cdfb4a10c489#p875772

        .. versionadded:: 1.6
        """""".format(values=docstring_values(DRC_STRENGTHS)))

    def _get_ISO(self):
        warnings.warn(
            PiCameraDeprecated(
                'PiCamera.ISO is deprecated; use PiCamera.iso instead'))
        return self.iso
    def _set_ISO(self, value):
        warnings.warn(
            PiCameraDeprecated(
                'PiCamera.ISO is deprecated; use PiCamera.iso instead'))
        self.iso = value
    ISO = property(_get_ISO, _set_ISO, doc=""""""
        Retrieves or sets the apparent ISO setting of the camera.

        .. deprecated:: 1.8
            Please use the :attr:`iso` attribute instead.
        """""")

    def _get_iso(self):
        self._check_camera_open()
        return self._camera.control.params[mmal.MMAL_PARAMETER_ISO]
    def _set_iso(self, value):
        self._check_camera_open()
        try:
            if not (0 <= value <= 1600):
                raise PiCameraValueError(
                    ""Invalid iso value: %d (valid range 0..800)"" % value)
        except TypeError:
            raise PiCameraValueError(""Invalid iso value: %s"" % value)
        self._camera.control.params[mmal.MMAL_PARAMETER_ISO] = value
    iso = property(_get_iso, _set_iso, doc=""""""\
        Retrieves or sets the apparent ISO setting of the camera.

        When queried, the :attr:`iso` property returns the ISO setting of the
        camera, a value which represents the `sensitivity of the camera to
        light`_. Lower values (e.g. 100) imply less sensitivity than higher
        values (e.g. 400 or 800). Lower sensitivities tend to produce less
        ""noisy"" (smoother) images, but operate poorly in low light conditions.

        When set, the property adjusts the sensitivity of the camera (by
        adjusting the :attr:`analog_gain` and :attr:`digital_gain`). Valid
        values are between 0 (auto) and 1600. The actual value used when iso is
        explicitly set will be one of the following values (whichever is
        closest): 100, 200, 320, 400, 500, 640, 800.

        .. note::

            Some users on the Pi camera forum have noted that higher ISO values
            than 800 (specifically up to 1600) can be achieved in certain
            conditions with :attr:`exposure_mode` set to ``'sports'`` and
            :attr:`iso` set to 0.  It doesn't appear to be possible to manually
            request an ISO setting higher than 800, but the picamera library
            will permit settings up to 1600 in case the underlying firmware
            permits such settings in particular circumstances.

        On the V1 camera module, non-zero ISO values attempt to fix overall
        gain at various levels. For example, ISO 100 attempts to provide an
        overall gain of 1.0, ISO 200 attempts to provide overall gain of 2.0,
        etc. The algorithm prefers analog gain over digital gain to reduce
        noise.

        On the V2 camera module, ISO 100 attempts to produce overall gain of
        ~1.84, and ISO 800 attempts to produce overall gain of ~14.72 (the V2
        camera module was calibrated against the `ISO film speed`_ standard).

        The attribute can be adjusted while previews or recordings are in
        progress. The default value is 0 which means automatically determine a
        value according to image-taking conditions.

        .. note::

            Certain :attr:`exposure_mode` values override the ISO setting. For
            example, ``'off'`` fixes :attr:`analog_gain` and
            :attr:`digital_gain` entirely, preventing this property from
            adjusting them when set.

        .. _sensitivity of the camera to light: https://en.wikipedia.org/wiki/Film_speed#Digital
        .. _ISO film speed: https://en.wikipedia.org/wiki/Film_speed#Current_system:_ISO
        """""")

    def _get_meter_mode(self):
        self._check_camera_open()
        return self._METER_MODES_R[
            self._camera.control.params[mmal.MMAL_PARAMETER_EXP_METERING_MODE].value
            ]
    def _set_meter_mode(self, value):
        self._check_camera_open()
        try:
            mp = self._camera.control.params[mmal.MMAL_PARAMETER_EXP_METERING_MODE]
            mp.value = self.METER_MODES[value]
        except KeyError:
            raise PiCameraValueError(""Invalid metering mode: %s"" % value)
        self._camera.control.params[mmal.MMAL_PARAMETER_EXP_METERING_MODE] = mp
    meter_mode = property(_get_meter_mode, _set_meter_mode, doc=""""""\
        Retrieves or sets the metering mode of the camera.

        When queried, the :attr:`meter_mode` property returns the method by
        which the camera `determines the exposure`_ as one of the following
        strings:

        {values}

        When set, the property adjusts the camera's metering mode. All modes
        set up two regions: a center region, and an outer region. The major
        `difference between each mode`_ is the size of the center region. The
        ``'backlit'`` mode has the largest central region (30% of the width),
        while ``'spot'`` has the smallest (10% of the width).

        The property can be set while recordings or previews are in progress.
        The default value is ``'average'``. All possible values for the
        attribute can be obtained from the ``PiCamera.METER_MODES`` attribute.

        .. _determines the exposure: https://en.wikipedia.org/wiki/Metering_mode
        .. _difference between each mode: https://www.raspberrypi.org/forums/viewtopic.php?p=565644#p565644
        """""".format(values=docstring_values(METER_MODES)))

    def _get_video_stabilization(self):
        self._check_camera_open()
        return self._camera.control.params[mmal.MMAL_PARAMETER_VIDEO_STABILISATION]
    def _set_video_stabilization(self, value):
        self._check_camera_open()
        self._camera.control.params[mmal.MMAL_PARAMETER_VIDEO_STABILISATION] = value
    video_stabilization = property(
        _get_video_stabilization, _set_video_stabilization, doc=""""""\
        Retrieves or sets the video stabilization mode of the camera.

        When queried, the :attr:`video_stabilization` property returns a
        boolean value indicating whether or not the camera attempts to
        compensate for motion.

        When set, the property activates or deactivates video stabilization.
        The property can be set while recordings or previews are in progress.
        The default value is ``False``.

        .. note::

            The built-in video stabilization only accounts for `vertical and
            horizontal motion`_, not rotation.

        .. _vertical and horizontal motion: https://www.raspberrypi.org/forums/viewtopic.php?p=342667&sid=ec7d95e887ab74a90ffaab87888c48cd#p342667
        """""")

    def _get_exposure_compensation(self):
        self._check_camera_open()
        return self._camera.control.params[mmal.MMAL_PARAMETER_EXPOSURE_COMP]
    def _set_exposure_compensation(self, value):
        self._check_camera_open()
        try:
            if not (-25 <= value <= 25):
                raise PiCameraValueError(
                    ""Invalid exposure compensation value: ""
                    ""%d (valid range -25..25)"" % value)
        except TypeError:
            raise PiCameraValueError(
                ""Invalid exposure compensation value: %s"" % value)
        self._camera.control.params[mmal.MMAL_PARAMETER_EXPOSURE_COMP] = value
    exposure_compensation = property(
        _get_exposure_compensation, _set_exposure_compensation, doc=""""""\
        Retrieves or sets the exposure compensation level of the camera.

        When queried, the :attr:`exposure_compensation` property returns an
        integer value between -25 and 25 indicating the exposure level of the
        camera. Larger values result in brighter images.

        When set, the property adjusts the camera's exposure compensation
        level. Each increment represents 1/6th of a stop. Hence setting the
        attribute to 6 increases exposure by 1 stop. The property can be set
        while recordings or previews are in progress. The default value is 0.
        """""")

    def _get_exposure_mode(self):
        self._check_camera_open()
        return self._EXPOSURE_MODES_R[
            self._camera.control.params[mmal.MMAL_PARAMETER_EXPOSURE_MODE].value
            ]
    def _set_exposure_mode(self, value):
        self._check_camera_open()
        try:
            mp = self._camera.control.params[mmal.MMAL_PARAMETER_EXPOSURE_MODE]
            mp.value = self.EXPOSURE_MODES[value]
        except KeyError:
            raise PiCameraValueError(""Invalid exposure mode: %s"" % value)
        self._camera.control.params[mmal.MMAL_PARAMETER_EXPOSURE_MODE] = mp
    exposure_mode = property(_get_exposure_mode, _set_exposure_mode, doc=""""""\
        Retrieves or sets the exposure mode of the camera.

        When queried, the :attr:`exposure_mode` property returns a string
        representing the exposure setting of the camera. The possible values
        can be obtained from the ``PiCamera.EXPOSURE_MODES`` attribute, and
        are as follows:

        {values}

        When set, the property adjusts the camera's exposure mode.  The
        property can be set while recordings or previews are in progress. The
        default value is ``'auto'``.

        .. note::

            Exposure mode ``'off'`` is special: this disables the camera's
            automatic gain control, fixing the values of :attr:`digital_gain`
            and :attr:`analog_gain`.

            Please note that these properties are not directly settable
            (although they can be influenced by setting :attr:`iso` *prior* to
            fixing the gains), and default to low values when the camera is
            first initialized. Therefore it is important to let them settle on
            higher values before disabling automatic gain control otherwise all
            frames captured will appear black.
        """""".format(values=docstring_values(EXPOSURE_MODES)))

    def _get_flash_mode(self):
        self._check_camera_open()
        return self._FLASH_MODES_R[
            self._camera.control.params[mmal.MMAL_PARAMETER_FLASH].value
            ]
    def _set_flash_mode(self, value):
        self._check_camera_open()
        try:
            mp = self._camera.control.params[mmal.MMAL_PARAMETER_FLASH]
            mp.value = self.FLASH_MODES[value]
        except KeyError:
            raise PiCameraValueError(""Invalid flash mode: %s"" % value)
        self._camera.control.params[mmal.MMAL_PARAMETER_FLASH] = mp
    flash_mode = property(_get_flash_mode, _set_flash_mode, doc=""""""\
        Retrieves or sets the flash mode of the camera.

        When queried, the :attr:`flash_mode` property returns a string
        representing the flash setting of the camera. The possible values can
        be obtained from the ``PiCamera.FLASH_MODES`` attribute, and are as
        follows:

        {values}

        When set, the property adjusts the camera's flash mode.  The property
        can be set while recordings or previews are in progress.  The default
        value is ``'off'``.

        .. note::

            You must define which GPIO pins the camera is to use for flash and
            privacy indicators. This is done within the `Device Tree
            configuration`_ which is considered an advanced topic.
            Specifically, you need to define pins ``FLASH_0_ENABLE`` and
            optionally ``FLASH_0_INDICATOR`` (for the privacy indicator). More
            information can be found in this :ref:`recipe
            <flash_configuration>`.

        .. _Device Tree configuration: https://www.raspberrypi.org/documentation/configuration/pin-configuration.md

        .. versionadded:: 1.10
        """""".format(values=docstring_values(FLASH_MODES)))

    def _get_awb_mode(self):
        self._check_camera_open()
        return self._AWB_MODES_R[
            self._camera.control.params[mmal.MMAL_PARAMETER_AWB_MODE].value
            ]
    def _set_awb_mode(self, value):
        self._check_camera_open()
        try:
            mp = self._camera.control.params[mmal.MMAL_PARAMETER_AWB_MODE]
            mp.value = self.AWB_MODES[value]
        except KeyError:
            raise PiCameraValueError(""Invalid auto-white-balance mode: %s"" % value)
        self._camera.control.params[mmal.MMAL_PARAMETER_AWB_MODE] = mp
    awb_mode = property(_get_awb_mode, _set_awb_mode, doc=""""""\
        Retrieves or sets the auto-white-balance mode of the camera.

        When queried, the :attr:`awb_mode` property returns a string
        representing the auto white balance setting of the camera. The possible
        values can be obtained from the ``PiCamera.AWB_MODES`` attribute, and
        are as follows:

        {values}

        When set, the property adjusts the camera's auto-white-balance mode.
        The property can be set while recordings or previews are in progress.
        The default value is ``'auto'``.

        .. note::

            AWB mode ``'off'`` is special: this disables the camera's automatic
            white balance permitting manual control of the white balance via
            the :attr:`awb_gains` property. However, even with AWB disabled,
            some attributes (specifically :attr:`still_stats` and
            :attr:`drc_strength`) can cause AWB re-calculations.
        """""".format(values=docstring_values(AWB_MODES)))

    def _get_awb_gains(self):
        self._check_camera_open()
        mp = self._camera.control.params[mmal.MMAL_PARAMETER_CAMERA_SETTINGS]
        return (
            mo.to_fraction(mp.awb_red_gain),
            mo.to_fraction(mp.awb_blue_gain),
            )
    def _set_awb_gains(self, value):
        self._check_camera_open()
        try:
            red_gain, blue_gain = value
        except (ValueError, TypeError):
            red_gain = blue_gain = value
        if not (0.0 <= red_gain <= 8.0 and 0.0 <= blue_gain <= 8.0):
            raise PiCameraValueError(
                ""Invalid gain(s) in (%f, %f) (valid range: 0.0-8.0)"" % (
                    red_gain, blue_gain))
        mp = mmal.MMAL_PARAMETER_AWB_GAINS_T(
            mmal.MMAL_PARAMETER_HEADER_T(
                mmal.MMAL_PARAMETER_CUSTOM_AWB_GAINS,
                ct.sizeof(mmal.MMAL_PARAMETER_AWB_GAINS_T)
                ),
            mo.to_rational(red_gain),
            mo.to_rational(blue_gain),
            )
        self._camera.control.params[mmal.MMAL_PARAMETER_CUSTOM_AWB_GAINS] = mp
    awb_gains = property(_get_awb_gains, _set_awb_gains, doc=""""""\
        Gets or sets the auto-white-balance gains of the camera.

        When queried, this attribute returns a tuple of values representing
        the `(red, blue)` balance of the camera. The `red` and `blue` values
        are returned :class:`~fractions.Fraction` instances. The values will
        be between 0.0 and 8.0.

        When set, this attribute adjusts the camera's auto-white-balance gains.
        The property can be specified as a single value in which case both red
        and blue gains will be adjusted equally, or as a `(red, blue)` tuple.
        Values can be specified as an :ref:`int <typesnumeric>`, :ref:`float
        <typesnumeric>` or :class:`~fractions.Fraction` and each gain must be
        between 0.0 and 8.0.  Typical values for the gains are between 0.9 and
        1.9.  The property can be set while recordings or previews are in
        progress.

        .. note::

            This attribute only has an effect when :attr:`awb_mode` is set to
            ``'off'``. Also note that even with AWB disabled, some attributes
            (specifically :attr:`still_stats` and :attr:`drc_strength`) can
            cause AWB re-calculations.

        .. versionchanged:: 1.6
            Prior to version 1.6, this attribute was write-only.
        """""")

    def _get_image_effect(self):
        self._check_camera_open()
        return self._IMAGE_EFFECTS_R[
            self._camera.control.params[mmal.MMAL_PARAMETER_IMAGE_EFFECT].value
            ]
    def _set_image_effect(self, value):
        self._check_camera_open()
        try:
            mp = self._camera.control.params[mmal.MMAL_PARAMETER_IMAGE_EFFECT]
            mp.value = self.IMAGE_EFFECTS[value]
            self._image_effect_params = None
        except KeyError:
            raise PiCameraValueError(""Invalid image effect: %s"" % value)
        self._camera.control.params[mmal.MMAL_PARAMETER_IMAGE_EFFECT] = mp
    image_effect = property(_get_image_effect, _set_image_effect, doc=""""""\
        Retrieves or sets the current image effect applied by the camera.

        When queried, the :attr:`image_effect` property returns a string
        representing the effect the camera will apply to captured video. The
        possible values can be obtained from the ``PiCamera.IMAGE_EFFECTS``
        attribute, and are as follows:

        {values}

        When set, the property changes the effect applied by the camera.  The
        property can be set while recordings or previews are in progress, but
        only certain effects work while recording video (notably ``'negative'``
        and ``'solarize'``). The default value is ``'none'``.
        """""".format(values=docstring_values(IMAGE_EFFECTS)))

    def _get_image_effect_params(self):
        self._check_camera_open()
        return self._image_effect_params
    def _set_image_effect_params(self, value):
        self._check_camera_open()
        to_int = lambda x: int(x)
        to_byte = lambda x: max(0, min(255, int(x)))
        to_bool = lambda x: (0, 1)[bool(x)]
        to_8dot8 = lambda x: int(x * 256)
        valid_transforms = {
            'solarize': [
                (to_bool, to_byte, to_byte, to_byte, to_byte),
                (to_byte, to_byte, to_byte, to_byte),
                (to_bool,),
                ],
            'colorpoint': [
                (lambda x: max(0, min(3, int(x))),),
                ],
            'colorbalance': [
                (to_8dot8, to_8dot8, to_8dot8, to_8dot8, to_int, to_int),
                (to_8dot8, to_8dot8, to_8dot8, to_8dot8),
                (to_8dot8, to_8dot8, to_8dot8),
                ],
            'colorswap': [
                (to_bool,),
                ],
            'posterise': [
                (lambda x: max(2, min(31, int(x))),),
                ],
            'blur': [
                (lambda x: max(1, min(2, int(x))),),
                ],
            'film': [
                (to_byte, to_byte, to_byte),
                ],
            'watercolor': [
                (),
                (to_byte, to_byte),
                ]
            }
        # Ensure params is a tuple
        try:
            params = tuple(i for i in value)
        except TypeError:
            params = (value,)
        # Find the parameter combination for the current effect
        effect = self.image_effect
        param_transforms = [
            transforms for transforms in valid_transforms.get(effect, [])
            if len(transforms) == len(params)
            ]
        if not param_transforms:
            raise PiCameraValueError(
                'invalid set of parameters for effect ""%s""' % effect)
        param_transforms = param_transforms[0]
        params = tuple(
            transform(p)
            for (transform, p) in zip(param_transforms, params)
            )
        mp = mmal.MMAL_PARAMETER_IMAGEFX_PARAMETERS_T(
            mmal.MMAL_PARAMETER_HEADER_T(
                mmal.MMAL_PARAMETER_IMAGE_EFFECT_PARAMETERS,
                ct.sizeof(mmal.MMAL_PARAMETER_IMAGEFX_PARAMETERS_T)
                ),
            effect=self.IMAGE_EFFECTS[effect],
            num_effect_params=len(params),
            effect_parameter=params,
            )
        self._camera.control.params[mmal.MMAL_PARAMETER_IMAGE_EFFECT_PARAMETERS] = mp
        self._image_effect_params = value
    image_effect_params = property(
            _get_image_effect_params, _set_image_effect_params, doc=""""""\
        Retrieves or sets the parameters for the current :attr:`effect
        <image_effect>`.

        When queried, the :attr:`image_effect_params` property either returns
        ``None`` (for effects which have no configurable parameters, or if no
        parameters have been configured), or a tuple of numeric values up to
        six elements long.

        When set, the property changes the parameters of the current
        :attr:`effect <image_effect>` as a sequence of numbers, or a single
        number. Attempting to set parameters on an effect which does not
        support parameters, or providing an incompatible set of parameters for
        an effect will raise a :exc:`PiCameraValueError` exception.

        The effects which have parameters, and what combinations those
        parameters can take is as follows:

        .. tabularcolumns:: |p{30mm}|p{25mm}|p{75mm}|

        +--------------------+----------------+-----------------------------------------+
        | Effect             | Parameters     | Description                             |
        +====================+================+=========================================+
        | ``'solarize'``     | *yuv*,         | *yuv* controls whether data is          |
        |                    | *x0*, *y1*,    | processed as RGB (0) or YUV(1). Input   |
        |                    | *y2*, *y3*     | values from 0 to *x0* - 1 are remapped  |
        |                    |                | linearly onto the range 0 to *y0*.      |
        |                    |                | Values from *x0* to 255 are remapped    |
        |                    |                | linearly onto the range *y1* to *y2*.   |
        |                    +----------------+-----------------------------------------+
        |                    | *x0*, *y0*,    | Same as above, but *yuv* defaults to    |
        |                    | *y1*, *y2*     | 0 (process as RGB).                     |
        |                    +----------------+-----------------------------------------+
        |                    | *yuv*          | Same as above, but *x0*, *y0*, *y1*,    |
        |                    |                | *y2* default to 128, 128, 128, 0        |
        |                    |                | respectively.                           |
        +--------------------+----------------+-----------------------------------------+
        | ``'colorpoint'``   | *quadrant*     | *quadrant* specifies which quadrant     |
        |                    |                | of the U/V space to retain chroma       |
        |                    |                | from: 0=green, 1=red/yellow, 2=blue,    |
        |                    |                | 3=purple. There is no default; this     |
        |                    |                | effect does nothing until parameters    |
        |                    |                | are set.                                |
        +--------------------+----------------+-----------------------------------------+
        | ``'colorbalance'`` | *lens*,        | *lens* specifies the lens shading       |
        |                    | *r*, *g*, *b*, | strength (0.0 to 256.0, where 0.0       |
        |                    | *u*, *v*       | indicates lens shading has no effect).  |
        |                    |                | *r*, *g*, *b* are multipliers for their |
        |                    |                | respective color channels (0.0 to       |
        |                    |                | 256.0). *u* and *v* are offsets added   |
        |                    |                | to the U/V plane (0 to 255).            |
        |                    +----------------+-----------------------------------------+
        |                    | *lens*,        | Same as above but *u* are defaulted     |
        |                    | *r*, *g*, *b*  | to 0.                                   |
        |                    +----------------+-----------------------------------------+
        |                    | *lens*,        | Same as above but *g* also defaults to  |
        |                    | *r*, *b*       | to 1.0.                                 |
        +--------------------+----------------+-----------------------------------------+
        | ``'colorswap'``    | *dir*          | If *dir* is 0, swap RGB to BGR. If      |
        |                    |                | *dir* is 1, swap RGB to BRG.            |
        +--------------------+----------------+-----------------------------------------+
        | ``'posterise'``    | *steps*        | Control the quantization steps for the  |
        |                    |                | image. Valid values are 2 to 32, and    |
        |                    |                | the default is 4.                       |
        +--------------------+----------------+-----------------------------------------+
        | ``'blur'``         | *size*         | Specifies the size of the kernel. Valid |
        |                    |                | values are 1 or 2.                      |
        +--------------------+----------------+-----------------------------------------+
        | ``'film'``         | *strength*,    | *strength* specifies the strength of    |
        |                    | *u*, *v*       | effect. *u* and *v* are offsets added   |
        |                    |                | to the U/V plane (0 to 255).            |
        +--------------------+----------------+-----------------------------------------+
        | ``'watercolor'``   | *u*, *v*       | *u* and *v* specify offsets to add to   |
        |                    |                | the U/V plane (0 to 255).               |
        |                    +----------------+-----------------------------------------+
        |                    |                | No parameters indicates no U/V effect.  |
        +--------------------+----------------+-----------------------------------------+

        .. versionadded:: 1.8
        """""")

    def _get_color_effects(self):
        self._check_camera_open()
        mp = self._camera.control.params[mmal.MMAL_PARAMETER_COLOUR_EFFECT]
        if mp.enable != mmal.MMAL_FALSE:
            return (mp.u, mp.v)
        else:
            return None
    def _set_color_effects(self, value):
        self._check_camera_open()
        if value is None:
            enable = mmal.MMAL_FALSE
            u = v = 128
        else:
            enable = mmal.MMAL_TRUE
            try:
                u, v = value
            except (TypeError, ValueError) as e:
                raise PiCameraValueError(
                    ""Invalid color effect (u, v) tuple: %s"" % value)
            if not ((0 <= u <= 255) and (0 <= v <= 255)):
                raise PiCameraValueError(
                    ""(u, v) values must be between 0 and 255"")
        mp = mmal.MMAL_PARAMETER_COLOURFX_T(
            mmal.MMAL_PARAMETER_HEADER_T(
                mmal.MMAL_PARAMETER_COLOUR_EFFECT,
                ct.sizeof(mmal.MMAL_PARAMETER_COLOURFX_T)
                ),
            enable, u, v
            )
        self._camera.control.params[mmal.MMAL_PARAMETER_COLOUR_EFFECT] = mp
    color_effects = property(_get_color_effects, _set_color_effects, doc=""""""\
        Retrieves or sets the current color effect applied by the camera.

        When queried, the :attr:`color_effects` property either returns
        ``None`` which indicates that the camera is using normal color
        settings, or a ``(u, v)`` tuple where ``u`` and ``v`` are integer
        values between 0 and 255.

        When set, the property changes the color effect applied by the camera.
        The property can be set while recordings or previews are in progress.
        For example, to make the image black and white set the value to ``(128,
        128)``. The default value is ``None``.
        """""")

    def _get_rotation(self):
        self._check_camera_open()
        return self._camera.outputs[0].params[mmal.MMAL_PARAMETER_ROTATION]
    def _set_rotation(self, value):
        self._check_camera_open()
        try:
            value = ((int(value) % 360) // 90) * 90
        except ValueError:
            raise PiCameraValueError(""Invalid rotation angle: %s"" % value)
        for port in self._camera.outputs:
            port.params[mmal.MMAL_PARAMETER_ROTATION] = value
    rotation = property(_get_rotation, _set_rotation, doc=""""""\
        Retrieves or sets the current rotation of the camera's image.

        When queried, the :attr:`rotation` property returns the rotation
        applied to the image. Valid values are 0, 90, 180, and 270.

        When set, the property changes the rotation applied to the camera's
        input. The property can be set while recordings or previews are in
        progress. The default value is ``0``.
        """""")

    def _get_vflip(self):
        self._check_camera_open()
        return self._camera.outputs[0].params[mmal.MMAL_PARAMETER_MIRROR] in (
            mmal.MMAL_PARAM_MIRROR_VERTICAL, mmal.MMAL_PARAM_MIRROR_BOTH)
    def _set_vflip(self, value):
        self._check_camera_open()
        value = {
            (False, False): mmal.MMAL_PARAM_MIRROR_NONE,
            (True,  False): mmal.MMAL_PARAM_MIRROR_VERTICAL,
            (False, True):  mmal.MMAL_PARAM_MIRROR_HORIZONTAL,
            (True,  True):  mmal.MMAL_PARAM_MIRROR_BOTH,
            }[(bool(value), self.hflip)]
        for port in self._camera.outputs:
            port.params[mmal.MMAL_PARAMETER_MIRROR] = value
    vflip = property(_get_vflip, _set_vflip, doc=""""""\
        Retrieves or sets whether the camera's output is vertically flipped.

        When queried, the :attr:`vflip` property returns a boolean indicating
        whether or not the camera's output is vertically flipped. The property
        can be set while recordings or previews are in progress. The default
        value is ``False``.
        """""")

    def _get_hflip(self):
        self._check_camera_open()
        return self._camera.outputs[0].params[mmal.MMAL_PARAMETER_MIRROR] in (
            mmal.MMAL_PARAM_MIRROR_HORIZONTAL, mmal.MMAL_PARAM_MIRROR_BOTH)
    def _set_hflip(self, value):
        self._check_camera_open()
        value = {
            (False, False): mmal.MMAL_PARAM_MIRROR_NONE,
            (True,  False): mmal.MMAL_PARAM_MIRROR_VERTICAL,
            (False, True):  mmal.MMAL_PARAM_MIRROR_HORIZONTAL,
            (True,  True):  mmal.MMAL_PARAM_MIRROR_BOTH,
            }[(self.vflip, bool(value))]
        for port in self._camera.outputs:
            port.params[mmal.MMAL_PARAMETER_MIRROR] = value
    hflip = property(_get_hflip, _set_hflip, doc=""""""\
        Retrieves or sets whether the camera's output is horizontally flipped.

        When queried, the :attr:`hflip` property returns a boolean indicating
        whether or not the camera's output is horizontally flipped. The
        property can be set while recordings or previews are in progress. The
        default value is ``False``.
        """""")

    def _get_zoom(self):
        self._check_camera_open()
        mp = self._camera.control.params[mmal.MMAL_PARAMETER_INPUT_CROP]
        return (
            mp.rect.x / 65535.0,
            mp.rect.y / 65535.0,
            mp.rect.width / 65535.0,
            mp.rect.height / 65535.0,
            )
    def _set_zoom(self, value):
        self._check_camera_open()
        try:
            x, y, w, h = value
        except (TypeError, ValueError) as e:
            raise PiCameraValueError(
                ""Invalid zoom rectangle (x, y, w, h) tuple: %s"" % value)
        mp = mmal.MMAL_PARAMETER_INPUT_CROP_T(
            mmal.MMAL_PARAMETER_HEADER_T(
                mmal.MMAL_PARAMETER_INPUT_CROP,
                ct.sizeof(mmal.MMAL_PARAMETER_INPUT_CROP_T)
                ),
            mmal.MMAL_RECT_T(
                max(0, min(65535, int(65535 * x))),
                max(0, min(65535, int(65535 * y))),
                max(0, min(65535, int(65535 * w))),
                max(0, min(65535, int(65535 * h))),
                ),
            )
        self._camera.control.params[mmal.MMAL_PARAMETER_INPUT_CROP] = mp
    zoom = property(_get_zoom, _set_zoom, doc=""""""\
        Retrieves or sets the zoom applied to the camera's input.

        When queried, the :attr:`zoom` property returns a ``(x, y, w, h)``
        tuple of floating point values ranging from 0.0 to 1.0, indicating the
        proportion of the image to include in the output (this is also known as
        the ""Region of Interest"" or ROI). The default value is ``(0.0, 0.0,
        1.0, 1.0)`` which indicates that everything should be included. The
        property can be set while recordings or previews are in progress.

        The `zoom` is applied to the processed image, after rotation and rescale.
        If rotation has been used, zoom is composed of ``(y, x, h, w)`` instead.
        The values `w` and `h` can modify the aspect ratio of the image: use equal
        values for `w` and `h` if you want to keep the same the aspect ratio. 
        """""")

    def _get_crop(self):
        warnings.warn(
            PiCameraDeprecated(
                'PiCamera.crop is deprecated; use PiCamera.zoom instead'))
        return self.zoom
    def _set_crop(self, value):
        warnings.warn(
            PiCameraDeprecated(
                'PiCamera.crop is deprecated; use PiCamera.zoom instead'))
        self.zoom = value
    crop = property(_get_crop, _set_crop, doc=""""""
        Retrieves or sets the zoom applied to the camera's input.

        .. deprecated:: 1.8
            Please use the :attr:`zoom` attribute instead.
        """""")

    def _get_overlays(self):
        self._check_camera_open()
        return self._overlays
    overlays = property(_get_overlays, doc=""""""\
        Retrieves all active :class:`PiRenderer` overlays.

        If no overlays are current active, :attr:`overlays` will return an
        empty iterable. Otherwise, it will return an iterable of
        :class:`PiRenderer` instances which are currently acting as overlays.
        Note that the preview renderer is an exception to this: it is *not*
        included as an overlay despite being derived from :class:`PiRenderer`.

        .. versionadded:: 1.8
        """""")

    def _get_preview(self):
        self._check_camera_open()
        if isinstance(self._preview, PiPreviewRenderer):
            return self._preview
    preview = property(_get_preview, doc=""""""\
        Retrieves the :class:`PiRenderer` displaying the camera preview.

        If no preview is currently active, :attr:`preview` will return
        ``None``.  Otherwise, it will return the instance of
        :class:`PiRenderer` which is currently connected to the camera's
        preview port for rendering what the camera sees. You can use the
        attributes of the :class:`PiRenderer` class to configure the appearance
        of the preview. For example, to make the preview semi-transparent::

            import picamera

            with picamera.PiCamera() as camera:
                camera.start_preview()
                camera.preview.alpha = 128

        .. versionadded:: 1.8
        """""")

    def _get_preview_alpha(self):
        self._check_camera_open()
        warnings.warn(
            PiCameraDeprecated(
                'PiCamera.preview_alpha is deprecated; use '
                'PiCamera.preview.alpha instead'))
        if self.preview:
            return self.preview.alpha
        else:
            return self._preview_alpha
    def _set_preview_alpha(self, value):
        self._check_camera_open()
        warnings.warn(
            PiCameraDeprecated(
                'PiCamera.preview_alpha is deprecated; use '
                'PiCamera.preview.alpha instead'))
        if self.preview:
            self.preview.alpha = value
        else:
            self._preview_alpha = value
    preview_alpha = property(_get_preview_alpha, _set_preview_alpha, doc=""""""\
        Retrieves or sets the opacity of the preview window.

        .. deprecated:: 1.8
            Please use the :attr:`~PiRenderer.alpha` attribute of the
            :attr:`preview` object instead.
        """""")

    def _get_preview_layer(self):
        self._check_camera_open()
        warnings.warn(
            PiCameraDeprecated(
                'PiCamera.preview_layer is deprecated; '
                'use PiCamera.preview.layer instead'))
        if self.preview:
            return self.preview.layer
        else:
            return self._preview_layer
    def _set_preview_layer(self, value):
        self._check_camera_open()
        warnings.warn(
            PiCameraDeprecated(
                'PiCamera.preview_layer is deprecated; '
                'use PiCamera.preview.layer instead'))
        if self.preview:
            self.preview.layer = value
        else:
            self._preview_layer = value
    preview_layer = property(_get_preview_layer, _set_preview_layer, doc=""""""\
        Retrieves or sets the layer of the preview window.

        .. deprecated:: 1.8
            Please use the :attr:`~PiRenderer.layer` attribute of the
            :attr:`preview` object instead.
        """""")

    def _get_preview_fullscreen(self):
        self._check_camera_open()
        warnings.warn(
            PiCameraDeprecated(
                'PiCamera.preview_fullscreen is deprecated; '
                'use PiCamera.preview.fullscreen instead'))
        if self.preview:
            return self.preview.fullscreen
        else:
            return self._preview_fullscreen
    def _set_preview_fullscreen(self, value):
        self._check_camera_open()
        warnings.warn(
            PiCameraDeprecated(
                'PiCamera.preview_fullscreen is deprecated; '
                'use PiCamera.preview.fullscreen instead'))
        if self.preview:
            self.preview.fullscreen = value
        else:
            self._preview_fullscreen = value
    preview_fullscreen = property(
            _get_preview_fullscreen, _set_preview_fullscreen, doc=""""""\
        Retrieves or sets full-screen for the preview window.

        .. deprecated:: 1.8
            Please use the :attr:`~PiRenderer.fullscreen` attribute of the
            :attr:`preview` object instead.
        """""")

    def _get_preview_window(self):
        self._check_camera_open()
        warnings.warn(
            PiCameraDeprecated(
                'PiCamera.preview_window is deprecated; '
                'use PiCamera.preview.window instead'))
        if self.preview:
            return self.preview.window
        else:
            return self._preview_window
    def _set_preview_window(self, value):
        self._check_camera_open()
        warnings.warn(
            PiCameraDeprecated(
                'PiCamera.preview_window is deprecated; '
                'use PiCamera.preview.window instead'))
        if self.preview:
            self.preview.window = value
        else:
            self._preview_window = value
    preview_window = property(
        _get_preview_window, _set_preview_window, doc=""""""\
        Retrieves or sets the size of the preview window.

        .. deprecated:: 1.8
            Please use the :attr:`~PiRenderer.window` attribute of the
            :attr:`preview` object instead.
        """""")

    def _get_annotate_text(self):
        self._check_camera_open()
        mp = self._camera.control.params[mmal.MMAL_PARAMETER_ANNOTATE]
        if mp.enable:
            return mp.text.decode('ascii')
        else:
            return ''
    def _set_annotate_text(self, value):
        self._check_camera_open()
        mp = self._camera.control.params[mmal.MMAL_PARAMETER_ANNOTATE]
        mp.enable = bool(value or mp.show_frame_num)
        if mp.enable:
            try:
                mp.text = value.encode('ascii')
            except ValueError as e:
                raise PiCameraValueError(str(e))
        self._camera.control.params[mmal.MMAL_PARAMETER_ANNOTATE] = mp
    annotate_text = property(_get_annotate_text, _set_annotate_text, doc=""""""\
        Retrieves or sets a text annotation for all output.

        When queried, the :attr:`annotate_text` property returns the current
        annotation (if no annotation has been set, this is simply a blank
        string).

        When set, the property immediately applies the annotation to the
        preview (if it is running) and to any future captures or video
        recording. Strings longer than 255 characters, or strings containing
        non-ASCII characters will raise a :exc:`PiCameraValueError`. The
        default value is ``''``.

        .. versionchanged:: 1.8
            Text annotations can now be 255 characters long. The prior limit
            was 32 characters.
        """""")

    def _get_annotate_frame_num(self):
        self._check_camera_open()
        mp = self._camera.control.params[mmal.MMAL_PARAMETER_ANNOTATE]
        return mp.show_frame_num.value != mmal.MMAL_FALSE
    def _set_annotate_frame_num(self, value):
        self._check_camera_open()
        mp = self._camera.control.params[mmal.MMAL_PARAMETER_ANNOTATE]
        mp.enable = bool(value or mp.text)
        mp.show_frame_num = bool(value)
        self._camera.control.params[mmal.MMAL_PARAMETER_ANNOTATE] = mp
    annotate_frame_num = property(
        _get_annotate_frame_num, _set_annotate_frame_num, doc=""""""\
        Controls whether the current frame number is drawn as an annotation.

        The :attr:`annotate_frame_num` attribute is a bool indicating whether
        or not the current frame number is rendered as an annotation, similar
        to :attr:`annotate_text`. The default is ``False``.

        .. versionadded:: 1.8
        """""")

    def _get_annotate_text_size(self):
        self._check_camera_open()
        if self._camera.annotate_rev == 3:
            mp = self._camera.control.params[mmal.MMAL_PARAMETER_ANNOTATE]
            return mp.text_size or self.DEFAULT_ANNOTATE_SIZE
        else:
            return self.DEFAULT_ANNOTATE_SIZE
    def _set_annotate_text_size(self, value):
        self._check_camera_open()
        if not (6 <= value <= 160):
            raise PiCameraValueError(
                ""Invalid annotation text size: %d (valid range 6-160)"" % value)
        if self._camera.annotate_rev == 3:
            mp = self._camera.control.params[mmal.MMAL_PARAMETER_ANNOTATE]
            mp.text_size = value
            self._camera.control.params[mmal.MMAL_PARAMETER_ANNOTATE] = mp
        elif value != self.DEFAULT_ANNOTATE_SIZE:
            warnings.warn(
                PiCameraFallback(
                    ""Firmware does not support setting annotation text ""
                    ""size; using default (%d) instead"" % self.DEFAULT_ANNOTATE_SIZE))
    annotate_text_size = property(
        _get_annotate_text_size, _set_annotate_text_size, doc=""""""\
        Controls the size of the annotation text.

        The :attr:`annotate_text_size` attribute is an int which determines how
        large the annotation text will appear on the display. Valid values are
        in the range 6 to 160, inclusive. The default is {size}.

        .. versionadded:: 1.10
        """""".format(size=DEFAULT_ANNOTATE_SIZE))

    def _get_annotate_foreground(self):
        self._check_camera_open()
        mp = self._camera.control.params[mmal.MMAL_PARAMETER_ANNOTATE]
        if self._camera.annotate_rev == 3 and mp.custom_text_color:
            return Color.from_yuv_bytes(
                    mp.custom_text_Y,
                    mp.custom_text_U,
                    mp.custom_text_V)
        else:
            return Color('white')
    def _set_annotate_foreground(self, value):
        self._check_camera_open()
        if not isinstance(value, Color):
            raise PiCameraValueError(
                'annotate_foreground must be a Color')
        elif self._camera.annotate_rev < 3:
            if value.rgb_bytes != (255, 255, 255):
                warnings.warn(
                    PiCameraFallback(
                        ""Firmware does not support setting a custom foreground ""
                        ""annotation color; using white instead""))
            return
        mp = self._camera.control.params[mmal.MMAL_PARAMETER_ANNOTATE]
        mp.custom_text_color = True
        (
            mp.custom_text_Y,
            mp.custom_text_U,
            mp.custom_text_V,
            ) = value.yuv_bytes
        self._camera.control.params[mmal.MMAL_PARAMETER_ANNOTATE] = mp
    annotate_foreground = property(
        _get_annotate_foreground, _set_annotate_foreground, doc=""""""\
        Controls the color of the annotation text.

        The :attr:`annotate_foreground` attribute specifies, partially, the
        color of the annotation text. The value is specified as a
        :class:`Color`. The default is white.

        .. note::

            The underlying firmware does not directly support setting all
            components of the text color, only the Y' component of a `Y'UV`_
            tuple. This is roughly (but not precisely) analogous to the
            ""brightness"" of a color, so you may choose to think of this as
            setting how bright the annotation text will be relative to its
            background. In order to specify just the Y' component when setting
            this attribute, you may choose to construct the
            :class:`Color` instance as follows::

                camera.annotate_foreground = picamera.Color(y=0.2, u=0, v=0)

        .. _Y'UV: https://en.wikipedia.org/wiki/YUV

        .. versionadded:: 1.10
        """""")

    def _get_annotate_background(self):
        self._check_camera_open()
        mp = self._camera.control.params[mmal.MMAL_PARAMETER_ANNOTATE]
        if self._camera.annotate_rev == 3:
            if mp.enable_text_background:
                if mp.custom_background_color:
                    return Color.from_yuv_bytes(
                        mp.custom_background_Y,
                        mp.custom_background_U,
                        mp.custom_background_V)
                else:
                    return Color('black')
            else:
                return None
        else:
            if mp.black_text_background:
                return Color('black')
            else:
                return None
    def _set_annotate_background(self, value):
        self._check_camera_open()
        if value is True:
            warnings.warn(
                PiCameraDeprecated(
                    'Setting PiCamera.annotate_background to True is '
                    'deprecated; use PiCamera.color.Color(""black"") instead'))
            value = Color('black')
        elif value is False:
            warnings.warn(
                PiCameraDeprecated(
                    'Setting PiCamera.annotate_background to False is '
                    'deprecated; use None instead'))
            value = None
        elif value is None:
            pass
        elif not isinstance(value, Color):
            raise PiCameraValueError(
                'annotate_background must be a Color or None')
        elif self._camera.annotate_rev < 3 and value.rgb_bytes != (0, 0, 0):
            warnings.warn(
                PiCameraFallback(
                    ""Firmware does not support setting a custom background ""
                    ""annotation color; using black instead""))
        mp = self._camera.control.params[mmal.MMAL_PARAMETER_ANNOTATE]
        if self._camera.annotate_rev == 3:
            if value is None:
                mp.enable_text_background = False
            else:
                mp.enable_text_background = True
                mp.custom_background_color = True
                (
                    mp.custom_background_Y,
                    mp.custom_background_U,
                    mp.custom_background_V,
                    ) = value.yuv_bytes
        else:
            if value is None:
                mp.black_text_background = False
            else:
                mp.black_text_background = True
        self._camera.control.params[mmal.MMAL_PARAMETER_ANNOTATE] = mp
    annotate_background = property(
        _get_annotate_background, _set_annotate_background, doc=""""""\
        Controls what background is drawn behind the annotation.

        The :attr:`annotate_background` attribute specifies if a background
        will be drawn behind the :attr:`annotation text <annotate_text>` and,
        if so, what color it will be. The value is specified as a
        :class:`Color` or ``None`` if no background should be drawn. The
        default is ``None``.

        .. note::

            For backward compatibility purposes, the value ``False`` will be
            treated as ``None``, and the value ``True`` will be treated as the
            color black. The ""truthiness"" of the values returned by the
            attribute are backward compatible although the values themselves
            are not.

        .. versionadded:: 1.8

        .. versionchanged:: 1.10
            In prior versions this was a bool value with ``True`` representing
            a black background.
        """""")

"
269,550.0,USA,"The PM100D Handheld Optical Power and Energy Meter is designed to measure the
optical power of laser light or other monochromatic or near monochromatic light
sources and the energy of pulsed light sources.",PM 100 D,608.0,"Thorlabs, Inc. is an American privately held optical equipment company headquartered in Newton, New Jersey. The company was founded in 1989 by Alex Cable, who serves as its current president and CEO. As of 2018, Thorlabs has annual sales of approximately $500 million.
",Instrumental,Thorlabs,"[OrderedDict([('id', 'attRmyBjaipm6atue'), ('width', 295), ('height', 50), ('url', 'https://v5.airtableusercontent.com/v1/18/18/1689537600000/B34kPd8TDySPykIqGnAQbw/aiiK819WYExx3Ptf5wMW9occy5nw036iAflUYeUKd3P4oIOtDcjE-9n_aPuC9CIL6NqW9upoWnPw1MNj_dT272K3sQl6HrzLFUheR7vbgDA/McPpt4BKHE-5dfRkgnw9sy91ZT_2J2U6zhqPJLIpSMo'), ('filename', 'Thorlabs_Logo_Red.png'), ('size', 3614), ('type', 'image/png'), ('thumbnails', OrderedDict([('small', OrderedDict([('url', 'https://v5.airtableusercontent.com/v1/18/18/1689537600000/-IvxdwJZrNBEfbQllhNmjw/R2XVDzJm1eKjim2p3oPbkO0dTIqE8mJXIfagvk20zhFxQhu6OksXnXxFX3T7Js0XghjhysXg1U4hrYSY20fgqw/ePugtvTHT0veIoRZ7driznLOXT78vezhqu8vI9Qr9tw'), ('width', 212), ('height', 36)])), ('large', OrderedDict([('url', 'https://v5.airtableusercontent.com/v1/18/18/1689537600000/7MaR2gHuTGamIUDR6lpP_Q/h_TMBraNTFAgmrgYDRJVXnE24Tce3lEenhhkhf4x6E7N8W4ioeCEy-FmPSScWXQSKR9mNqK7eHTYLngjLCjurg/xbql1i-E8yIE38xfkhyh6Ea8xj3EnIOpH7RBtQWLpFs'), ('width', 295), ('height', 50)])), ('full', OrderedDict([('url', 'https://v5.airtableusercontent.com/v1/18/18/1689537600000/aM50-TKhPPqIT7mVMqCDNQ/NqK_Klj2IUd6BRR-rB9c_jzWWg3VIMr5gAf0IasH4eMUL0PkDhO0dpciWLQyN8Z40DMSviJxYrXeZh8qiz8g5Q/IrYPj2E85dh2q1sQ4SOYb_I8-A87BkF7ZS7m31_8sM0'), ('width', 3000), ('height', 3000)]))]))])]",https://www.thorlabs.com/,Write a Python script that uses Instrumental to connect to a PM100D Power Meters,https://www.hbm.com/en/9206/what-is-a-power-meter/,['Power Meters'],A Power Meter is one of the most useful and simple instruments to measure electrical power when no deeper analysis of the measured data is required. It measures the voltage (V) and current (A) and derives from these the most important power results,PM100D,https://www.thorlabs.com/_sd.cfm?fileName=17654-D02.pdf&partNumber=PM100D,"[OrderedDict([('id', 'attqAm3Kfn2xY6J7D'), ('width', 780), ('height', 780), ('url', 'https://v5.airtableusercontent.com/v1/18/18/1689537600000/aTjfKfVZDB9_cqDrNLaNVg/Dwu2ELpuTy56HhVPwMEu5ifQMxo9EX9aLUyB0sIKfHXS293EdrFmu6CwjKDvJOhg_Qg-Jv-gEV5Wp2HdsOC_S1rXZoas3LCYB1zFdrvMXLM/IpTgH7HU2WstkZzKJApvva3Gr-f90P0eRIc_-swb-DU'), ('filename', '17654-xl.jpg'), ('size', 109537), ('type', 'image/jpeg'), ('thumbnails', OrderedDict([('small', OrderedDict([('url', 'https://v5.airtableusercontent.com/v1/18/18/1689537600000/Zd3moHUjW1KJCM18fRgdUg/cHNH-a12oMnPsucbwXeRF9dpSW-aVJDjYTCtfVHjXTGFP0b-w5_23JxVbqXU9gL2JJNEld1BTL5RDzYsiB750Q/mlieXTycpJT_8qwBi-lTfhNP3ix0_vCI52gk4aT-bro'), ('width', 36), ('height', 36)])), ('large', OrderedDict([('url', 'https://v5.airtableusercontent.com/v1/18/18/1689537600000/2DZO72LnrhQvQhOL3buLhw/ppHomzT1sE3rSiJEux7QJbKCiBSIZopZsXPmvgIjfH2uJ5NnGMur5tpQ-dYWHkb5kMnbBpqSS1SrStL7jt-Lww/xhm4grnnJKhNuhrMy3KpYnSanGz3PF2rh92hVyU7DFE'), ('width', 512), ('height', 512)])), ('full', OrderedDict([('url', 'https://v5.airtableusercontent.com/v1/18/18/1689537600000/eqT2yc5WQWIhD5KHc8GW8w/Z4RArkZuhhMXDNWdam2t3Qd6nQQhcCXLwHnr6fz5C6I8SIl2Wiaei8QQunoS9gavUSd0fvdf-se0NqyMPySBEQ/AZ2jPwkNqd8-lcvDL_16ucL5tHI3hVrEEpGVpL8XCJ0'), ('width', 3000), ('height', 3000)]))]))])]",https://www.thorlabs.com/_sd.cfm?fileName=17654-D02.pdf&partNumber=PM100D,https://github.com/mabuchilab/Instrumental/blob/master/instrumental/drivers/powermeters/thorlabs.py,https://instrumental-lib.readthedocs.io/en/latest/thorlabs-powermeters.html,1245.48,,,,"# -*- coding: utf-8 -*-
# Copyright 2014-2019 Nate Bogdanowicz
""""""
Driver module for Thorlabs power meters. Supports:

* PM100D
""""""
import numpy
from . import PowerMeter
from .. import Facet, SCPI_Facet, VisaMixin, deprecated
from ... import Q_


class PM100D(PowerMeter, VisaMixin):
    """"""A Thorlabs PM100D series power meter""""""
    _INST_PARAMS_ = ['visa_address']
    _INST_VISA_INFO_ = ('Thorlabs', ['PM100D'])

    @deprecated('power')
    def get_power(self):
        return self.power

    @deprecated('range')
    def get_range(self):
        return self.range

    @deprecated('auto_range')
    def enable_auto_range(self, enable=True):
        self.auto_range = enable

    @deprecated('auto_range')
    def disable_auto_range(self):
        self.auto_range = False

    @deprecated('auto_range')
    def auto_range_enabled(self):
        return self.auto_range

    range = SCPI_Facet('power:dc:range', units='W', convert=float, readonly=True,
                       doc=""The current input range's max power"")

    @deprecated('wavelength')
    def get_wavelength(self):
        """"""Get the input signal wavelength setting

        Returns
        -------
        wavelength : Quantity
            the input signal wavelength in units of [length]
        """"""
        val = float(self._rsrc.query('sense:correction:wav?'))
        return Q_(val, 'nm')

    @deprecated('wavelength')
    def set_wavelength(self, wavelength):
        """"""Set the input signal wavelength setting

        Parameters
        ----------
        wavelength : Quantity
            the input signal wavelength in units of [length]
        """"""
        wav_nm = Q_(wavelength).to('nm').magnitude
        self.write('sense:correction:wav {}', wav_nm)

    @deprecated('num_averaged')
    def get_num_averaged(self):
        """"""Get the number of samples to average

        Returns
        -------
        num_averaged : int
            number of samples that are averaged
        """"""
        val = int(self._rsrc.query('sense:average:count?'))
        return val

    @deprecated('num_averaged')
    def set_num_averaged(self, num_averaged):
        """"""Set the number of samples to average

        Each sample takes approximately 3ms. Thus, averaging over 1000 samples
        would take about a second.

        Parameters
        ----------
        num_averaged : int
            number of samples to average
        """"""
        val = int(num_averaged)
        self.write('sense:average:count {}', val)

    auto_range = SCPI_Facet('power:dc:range:auto', convert=int, value={False:0, True:1},
                            doc=""Whether auto-ranging is enabled"")

    wavelength = SCPI_Facet('sense:corr:wav', units='nm', type=float,
                            doc=""Input signal wavelength"")

    num_averaged = SCPI_Facet('sense:average:count', type=int,
                              doc=""Number of samples to average"")

    def close(self):
        self._rsrc.control_ren(False)  # Disable remote mode

    # Tell list_instruments how to close this VISA resource properly
    @staticmethod
    def _close_resource(resource):
        resource.control_ren(False)  # Disable remote mode

    @Facet(units='W', cached=False)
    def power(self):
        """"""The measured optical power""""""
        self.write('power:dc:unit W')
        return float(self.query('measure:power?'))

    def measure(self, n_samples=100):
        """"""Make a multi-sample power measurement

        Parameters
        ----------
        n_samples : int
            Number of samples to take

        Returns
        -------
        pint.Measurement
            Measured power, with units and uncertainty, as a `pint.Measurement` object
        """"""
        n_avg = self.get_num_averaged()  # Save for later
        self.set_num_averaged(1)
        self.write('power:dc:unit W')

        raw_arr = numpy.empty((n_samples,), dtype='f')
        for i in range(n_samples):
            raw_arr[i] = float(self.query('measure:power?'))
        self.set_num_averaged(n_avg)

        return Q_(raw_arr.mean(), 'W').plus_minus(raw_arr.std())
"
275,23.0,"Beijing, China","The DP800 Series Power Supplies combine the ability to source, analyze, and coordinate over time on a powerful platform. The DP800 Series is a family of linear power supplies systems with 1, 2, or 3 outputs and 140 to 200 Watts in total. With one channel isolated users can reconfigure instruments into any number of systems or applications. Built in V, A, and W measurements make power monitoring easy, but additional wave tracking, timing, and analysis features in the advanced ""A"" models means there are even more ways to use the instruments. Digital triggering between instruments also makes it possible to reliably combine and connect supplies together. Intuitive to use for everything from education labs to the R & D bench, the DP800 family of power supplies provide incredible value for any application. Select the value models for best price performance or upgrade to the ""A"" model to improve resolution and add advanced monitoring, triggering, and programming capabilities.",Rigol,,"RIGOL Technologies, Inc. specializes in development and production of test and measuring equipment and is one of the fastest growing Chinese companies in this sphere.
RIGOL’s line of products includes [digital storage oscilloscopes](https://www.tmatlantic.com/e-store/index.php?SECTION_ID=227), [function/arbitrary waveform generators](https://www.tmatlantic.com/e-store/index.php?SECTION_ID=230), [digital multimeters](https://www.tmatlantic.com/e-store/index.php?SECTION_ID=233), PC-based devices compatible with LXI standard etc.
",Instrumental,Rigol,"[OrderedDict([('id', 'attivRtPF5u9Jt8zc'), ('width', 600), ('height', 400), ('url', 'https://v5.airtableusercontent.com/v1/18/18/1689537600000/dK_mPGZZbeZp5bcFtkzbxg/5lP9IsicLc-53c57pU09lJq8btShrsIP8oUdQD8g-Riyyl7CgjgCuLxljMo3u7mSmIXTKA_1rOHlzYVZ9sSzLxtBldRCizWNkSc1c3VGOCyAsmZ8HP6hOtPBhLsT_DGV/oOwHd5lFYWPNGu0Q17nAm8QwNC1tHLy_fE5qOLj7tSs'), ('filename', 'rigol-technologies-inc-logo-vector.png'), ('size', 2793), ('type', 'image/png'), ('thumbnails', OrderedDict([('small', OrderedDict([('url', 'https://v5.airtableusercontent.com/v1/18/18/1689537600000/zLg1x4J0supwCaJ83IL-4A/V-NesMLWPnrBKKPErwXZSINLiLFF25aXtZnMo_kMyeFuWjgTj5V5zEjv57r4fGSxq3VyIPN_4rjf69NIqz8Gzw/pZHIjN5-F-C70IjpD1k3NvdeUOvruDC8BQQZecQyYN8'), ('width', 54), ('height', 36)])), ('large', OrderedDict([('url', 'https://v5.airtableusercontent.com/v1/18/18/1689537600000/TtrY8vWkD6dH3FpegMsnZg/VDR5M5gZRQkV2BrFVjDxcbTNXGQhzU42l5WvrjM-ipcjmlF-7Oq-BO3myo_SOVCkT2wiVPQAjmpQh9cV_64hPQ/5dMGpy23APettmB4rgrzyyIGA_a3a69X1naJ8wxXyDA'), ('width', 600), ('height', 400)])), ('full', OrderedDict([('url', 'https://v5.airtableusercontent.com/v1/18/18/1689537600000/BLGNhQ0TAAvOaix0UZDcpQ/7lmilKMeJr2Ofwuq_NWkkqdKkoUC_-kSB7azAErsuAjnu-0VYL8ejL_P2ioQYZxF9PzGLPVMxaKtkDVN-tY0Sg/juOPtupWPWQ8JaIQ3AdUWz3xKt51fE8OTZzs7jTwlTk'), ('width', 3000), ('height', 3000)]))]))])]",https://www.rigol.com/,Write a Python script that uses Instrumental to connect to a ?? Power Supplies,https://en.wikipedia.org/wiki/Power_supply,['Power Supplies'],"A power supply is an electrical device that supplies electric power to an electrical load. The main purpose of a power supply is to convert electric current from a source to the correct voltage, current, and frequency to power the load. As a result, power supplies are sometimes referred to as electric power converters. Some power supplies are separate standalone pieces of equipment, while others are built into the load appliances that they power.",??,,,https://www.rigolna.com/products/dc-power-loads/dp800/,https://github.com/mabuchilab/Instrumental/blob/master/instrumental/drivers/powersupplies/rigol.py,not on docs,,,,,"# -*- coding: utf-8 -*-

from . import PowerSupply
from .. import VisaMixin, Facet, SCPI_Facet
from ... import u, Q_
from .. import ParamSet
from enum import Enum
from visa import ResourceManager
import pyvisa
import re

_INST_PARAMS_ = ['visa_address']
_INST_VISA_INFO_ = {
    'DP700': ('RIGOL TECHNOLOGIES', ['DP711', 'DP712']),
}

def list_instruments():
    """"""Get a list of all power supplies currently attached""""""
    paramsets = []
    search_string = ""ASRL?*""
    rm = ResourceManager()
    raw_spec_list = rm.list_resources(search_string)

    for spec in raw_spec_list:
        try:
            inst = rm.open_resource(spec, read_termination='\n', write_termination='\n')
            idn = inst.query(""*IDN?"")
            manufacturer, model, serial, version = idn.rstrip().split(',', 4)
            if re.match('DP7[0-9]{2}', model):
                paramsets.append(ParamSet(DP700, asrl=spec, manufacturer=manufacturer, serial=serial, model=model, version=version))
        except pyvisa.errors.VisaIOError as vio:
            # Ignore unknown serial devices
            pass

    return paramsets

class OnOffState(Enum):
    ON = True
    OFF = False

class RigolPowerSupply(PowerSupply, VisaMixin):
    def _initialize(self):
        self._rsrc.write_termination = '\n'
        self._rsrc.read_termination = '\n'

class DP700(RigolPowerSupply, VisaMixin):
    voltage = SCPI_Facet('SOURce:VOLTage:LEVel:IMMediate:AMPLitude', convert=float, units='V')
    current = SCPI_Facet('SOURce:CURRent:LEVel:IMMediate:AMPLitude', convert=float, units='A')
    current_protection = SCPI_Facet('SOURce:CURRent:PROTection', convert=float, units='A')
    current_protection_state = SCPI_Facet('SOURce:CURRent:PROTection:STATe', convert=OnOffState)
    output = SCPI_Facet('OUTPut:STATe', convert=OnOffState)
    beeper = SCPI_Facet('SYSTem:BEEPer', convert=OnOffState)

    @property
    def manufacturer(self):
        manufacturer, _, _, _ = self.query('*IDN?').rstrip().split(',', 4)
        return manufacturer

    @property
    def model(self):
        _, model, _, _ = self.query('*IDN?').rstrip().split(',', 4)
        return model

    @property
    def serial(self):
        _, _, serial, _ = self.query('*IDN?').rstrip().split(',', 4)
        return serial

    @property
    def version(self):
        _, _, _, version = self.query('*IDN?').rstrip().split(',', 4)
        return version

    def get_measured_voltage(self):
        return Q_(self.query(':MEASure:VOLTage?'), u.V)

    def get_measured_current(self):
        return Q_(self.query(':MEASure:CURRent?'), u.A)

    @current_protection_state.setter
    def current_protection_state(self, val):
        val = int(bool(val))
        self.write('SOURCE:CURRent:PROTection:STATe %s' % OnOffState(val).name)

    @output.setter
    def output(self, val):
        val = int(bool(val))
        self.write('OUTPut:STATe %s' % OnOffState(val).name)

    @beeper.setter
    def beeper(self, val):
        val = int(bool(val))
        self.write('SYSTem:BEEPer %s' % OnOffState(val).name)

    def local(self):
        self.write('SYSTem:LOCal')

    def remote(self):
        self.write('SYSTem:REMote')"
304,20.0,USA,"SR850 — 100 kHz DSP lock-in amplifier
",SR 850,,"Flexible rental terms for your short & long term projects. Choose from over 5000 models. Top quality electronic test equipment available for rent from over 80 top manufacturers
",Instrumental,Sr 850,"[OrderedDict([('id', 'attkoPQ5bTJDi4BdD'), ('width', 200), ('height', 200), ('url', 'https://v5.airtableusercontent.com/v1/18/18/1689537600000/PBXkdziTvFQsApTzC1bppA/wN8BgJr2CkvWL7LZbS0HbyvDzE5EQp-Hz7BdmzNQSg2w3LewwFxLxb-3sjNqNzgR9a6q74yhtElENgQp03z1qmyFuO9b9qKqLgLVgWHCggk/V74T-q70FdVUD25gveDNjAptPerVrRhV5PV-2QQfuzA'), ('filename', 'téléchargement (2).jpeg'), ('size', 4298), ('type', 'image/jpeg'), ('thumbnails', OrderedDict([('small', OrderedDict([('url', 'https://v5.airtableusercontent.com/v1/18/18/1689537600000/rnIrYmmVlHKQW1BbADN8fQ/cRZ6ZWcc2fZ6TfbGYfXzYTH5dZUNJVOFx7MGkp_qJUssbx9JOBrgTl7uCmmoClWJkbzYEzRNjArtM4l1B9CKSw/rGFVE-yacST7xYs31UfcFP-9CUxeK0OGNDXC2dxet_0'), ('width', 36), ('height', 36)])), ('large', OrderedDict([('url', 'https://v5.airtableusercontent.com/v1/18/18/1689537600000/jEk9jknenxypFOObBEqrxw/gujKa_7dauigKolcBNWitpJAJQzBsb66Gx6z3itxCLkTzmF0JnoS54_E5Glymy6NgmaW72StLYfVdwfg-0oqkg/b4rMGdEbyJbNuIJwmIML0Uu8XMCNmRKrEhYlKfeJFRk'), ('width', 200), ('height', 200)])), ('full', OrderedDict([('url', 'https://v5.airtableusercontent.com/v1/18/18/1689537600000/yjZxC7Ycv_-zqCsengk8jQ/1Aw3wsL37IP0SB2aXwT1c3EoM4mQg5AnLA1-ypQ8Vt01cFP8TvGcMbm4OsOwPCAR9Bjp6AJAcwa4ZTf-hBahhQ/V8d4pEzrtjrmGvmg_f72n3fw8IqeWFjb2dd3vLxgPEc'), ('width', 3000), ('height', 3000)]))]))])]",https://www.thinksrs.com,Write a Python script that uses Instrumental to connect to a {Device name} Lockin Amplifiers,https://en.wikipedia.org/wiki/Lock-in_amplifier,['Lockin Amplifiers'],"A lock-in amplifier is a type of amplifier that can extract a signal with a known carrier wave from an extremely noisy environment. Depending on the dynamic reserve of the instrument, signals up to a million times smaller than noise components, potentially fairly close by in frequency, can still be reliably detected. It is essentially a homodyne detector followed by low-pass filter that is often adjustable in cut-off frequency and filter order.",,https://www.thinksrs.com/downloads/pdfs/catalog/SR850c.pdf,"[OrderedDict([('id', 'att5RYW0QExpohlyt'), ('width', 3947), ('height', 1835), ('url', 'https://v5.airtableusercontent.com/v1/18/18/1689537600000/HhhMqP3FKHFqQ0xl8q4pyA/tQKsJW3CR0dw7d8kPdap1VAaGXhm9JVIfhYfzl1Qkb-ZTgfihvD6OpBw5TMVYb0fVPAtiVMHHdVduQxuai6DCjS1LlPpGHgDlKbS_646jX0/DnM85bM-UMqd1utONv7QPKzkMICD4KBXnGLhCtpkOXI'), ('filename', 'SR850_Main Pic.jpg'), ('size', 1473970), ('type', 'image/jpeg'), ('thumbnails', OrderedDict([('small', OrderedDict([('url', 'https://v5.airtableusercontent.com/v1/18/18/1689537600000/8nU-jH4aHWeZOqKurMQKlg/yvCZ48xI7qMfuKGI_sDcLMM9qjMOIdnIM4bcIeJjtjKAGo43Mu5SfMm33bx5CBLfgmmtPmQVdMnYTimbIZoV-Q/Fh2Sdb9j5jFKtfmeJKluzQZcpDxqanFECKSVXKzzC-E'), ('width', 77), ('height', 36)])), ('large', OrderedDict([('url', 'https://v5.airtableusercontent.com/v1/18/18/1689537600000/-F21eGSGkdva7cypUg8gpg/i8DsrJ-dvD0s5vzEn5SIGZyH2sxJrnhU4O5Twa-iMmnn6ytnKDh-UaEin1noBf2VZVlhG7eof0njnnqsQ-6xew/zf0SLGBzt3TdXGNKspq1Ihg30RvV59r3jBEL-7ug-A4'), ('width', 1101), ('height', 512)])), ('full', OrderedDict([('url', 'https://v5.airtableusercontent.com/v1/18/18/1689537600000/_scb0yfZslYWBrtkQ0s0fA/DmUXHk2qZ_Wn-d8JHbD-Q4KmnBwKA0l_IxIXenoLB5XvpzP8-A86iZwWrzHFhUS_utxF96PD_tR5oekk7FdUpg/9ZyAojWQgrlOI2wcmbyVqsya0D5y1nfbvBjmZSYY_kY'), ('width', 3000), ('height', 3000)]))]))])]",https://www.thinksrs.com/products/sr850.html,https://gist.github.com/amarvutha/8529502,,9500.0,,,,"# Talk to SR850 lockin amplifier
# version 1, 2014-01-20
# Amar

import visa
import numpy as np
from datetime import date
import time

class LIA(visa.SerialInstrument):
    # Lock-in amplifier, SR850

    def __init__(self,portName):
        visa.SerialInstrument.__init__(self,portName)
        self.timeout = 3
        self.id = self.ask(""*IDN?"")
        
    def characterize(self):
        warnings = """"
        print self.id
        print ""Sensitivity = "", self.ask(""SENS?"")
        print ""Reserve = "", self.ask(""RSRV?"")
        if self.ask(""RSRV?"")!=""0"": warnings+= ""** Warning! Dynamic reserve is not 0 dB ** \n""
        print ""Time constant = "", self.ask(""OFLT?"")    
        print ""Traces = "", self.ask(""TRCD?1""), self.ask(""TRCD?2""), self.ask(""TRCD?3""), self.ask(""TRCD?4"")

        FMOD = self.ask(""FMOD?"")
        if FMOD == ""0"": FMODString = ""internal""
        elif FMOD == ""1"": FMODString = ""internal sweep""
        elif FMOD == ""2"": FMODString = ""external""
        print ""Ref. source = "", FMODString

        print ""Ref. frequency = "", self.ask(""FREQ?"")    
        print ""Ref. phase = "", self.ask(""PHAS?"")    
        print ""Ref. sine level = "", self.ask(""SLVL?"")
        print 
        print warnings
        print
    def setf(self,fi):
        self.write(""FREQ "" + `fi`)
    def setphase(self,p):
        self.write(""PHAS "" +`p`)
            
    def getAverage(self,channel=1,N=400):
        # channel #s: 1 = X, 2 = Y, 3 = R, 4 = Theta
        S = []
        for i in range(N): S.append( float(self.ask(""OUTP?"" + `channel`)) )
        S_avg = np.average(S)
        S_std = np.std(S)
        return S_avg, S_std


    def scanP(self,f,channel=1,N=100,std=False):
        # channels: 1 = X, 2 = Y, 3 = R, 4 = Theta
        output, err = [], []
        for fi in f:
            self.setf(fi)
            print self.ask(""FREQ?""),
            
            x = []
            for i in range(N): x.append( float(self.ask(""OUTP?""+`channel`)) )
            x_avg = np.average(x)            
            output.append( x_avg )
            x_std = np.std(x)
            err.append( x_std )
            
        if std: return output, err
        else: return output

    def scanR(self,f,trace=1,N=100,std=False):
        # traces: 1,2,3,4
        output, err = [], []
        for fi in f:
            self.setf(fi)
            print self.ask(""FREQ?""),
            
            x = []
            for i in range(N): x.append( float(self.ask(""OUTR?""+`trace`)) )
            x_avg = np.average(x)            
            output.append( x_avg )
            x_std = np.std(x)
            err.append( x_std )
            
        if std: return output, err
        else: return output
"
321,550.0,USA,"BBD201 - 1-Channel Benchtop 3-Phase Brushless DC Servo Controller 
","Thorlabs Apt device package
",,"Thorlabs, Inc. is an American privately held optical equipment company headquartered in Newton, New Jersey. The company was founded in 1989 by Alex Cable, who serves as its current president and CEO. As of 2018, Thorlabs has annual sales of approximately $500 million.
",Instrumental,Thorlabs,"[OrderedDict([('id', 'attUZyCLkkmIuEgLg'), ('width', 2560), ('height', 398), ('url', 'https://v5.airtableusercontent.com/v1/18/18/1689537600000/BmNIeFHbhni_Wx2fsTQ7xA/7KGqzexbwgJQF4Glvxuce4xqbhQEHXktvcuNjxXhlvAhkK8CMrzKBeVv_-PHP7tmn-ApzOCstqQ78s4C7I55B6lFLXiYxxm-LRTgXLyuUC0/lrbUj5K4TTuMnassRPaSVDuk_3XZ5ajRvKcgWES7DOs'), ('filename', 'Image.png'), ('size', 70770), ('type', 'image/png'), ('thumbnails', OrderedDict([('small', OrderedDict([('url', 'https://v5.airtableusercontent.com/v1/18/18/1689537600000/ZZWxVhiPn1kMizuxLLk0Fg/0ebbYsrz7yPvC0KBBzNXH_YPxfCVcdt1ddf0aE3R7HCJoloiw8p80VHX4oB56xrqE7mAvyArGgfM18_lSXRRgQ/_9jZYvkMYMGOsykN6AlnWrE4VLLRcgITDBJiyCGI1Jk'), ('width', 232), ('height', 36)])), ('large', OrderedDict([('url', 'https://v5.airtableusercontent.com/v1/18/18/1689537600000/zIACzGEswy_VqAPI41RE0w/p9fZRVc7zXxmA-O88-PkMCrGiAgfV6ZhiPuJIdJges4pm8vK0kRS4RVlbvX3ISNKHhkpjNxGWTHFMS-Z1i6ltw/Q8t43BvydtWeEdRBXsOXk-0v2Fk3iEIno3tA1RqZfYA'), ('width', 2560), ('height', 398)])), ('full', OrderedDict([('url', 'https://v5.airtableusercontent.com/v1/18/18/1689537600000/g4og31dOk-TKjerxqVIoQg/QGIUp1sDPwfwfv7ZVi9tuQgRcWx7g3ITLMgEtKB1jNCQNc21T5YWxgXMcBFvV1ABa0s_2hn56K_GXiQvwAKfzg/1Q3db6fhAbr_EEtoThHWZoCVFkSliLdsG_PPrvp8Zeo'), ('width', 3000), ('height', 3000)]))]))])]",https://www.thorlabs.com/,Write a Python script that uses Instrumental to connect to a Thorlabs BBD201 (Example) Motion,https://en.wikipedia.org/wiki/Motion_control,['Motion'],"Motion controller calculates and controls the mechanical trajectories (motion profile) an actuator must follow (i.e., motion planning) and, in closed loop systems, employs feedback to make control corrections and thus implement closed-loop control.",Thorlabs BBD201 (Example),https://www.thorlabs.com/_sd.cfm?fileName=23964-D03.pdf&partNumber=BBD201,"[OrderedDict([('id', 'attITEf3GDXxe0cH5'), ('width', 200), ('height', 200), ('url', 'https://v5.airtableusercontent.com/v1/18/18/1689537600000/ZEx3Brw4QFm6mTk0UfMsfw/LiTqFUt9ge27JH9cU3KhuCFxU-joobD0-El-ylyd-_4aOMzaJ9Zw8yS5Oehw9wbmsDvnmmNPe3ufXppnc38B08OM__Ju0vJGiuPsO6WoRAo/N2TsK6s73r_lNbB0iDQsVI2KAmarB8LIw1F8vahFRoI'), ('filename', '23964-std.jpg'), ('size', 4813), ('type', 'image/jpeg'), ('thumbnails', OrderedDict([('small', OrderedDict([('url', 'https://v5.airtableusercontent.com/v1/18/18/1689537600000/9U5U881B6doG6bbZkyKI5w/wXaUmFqL8Z8OR0Ro7Ys8VKjGsVf9FmEmFLwcQNp1lMbwTWdEmoJZp8uGSOd19o8Y4VIB0q3dxQag5jZ6lvbeKQ/4A2lq4dQvw4U33P5nm7Tjs_GkoDpcA2P47Ja6OPsfT0'), ('width', 36), ('height', 36)])), ('large', OrderedDict([('url', 'https://v5.airtableusercontent.com/v1/18/18/1689537600000/F6pewv7mW7UQa_fq-_WxXg/4AYMjK_U9q_V6OazH9XyUt-QVN4GGRi7cDFh8zXPg2p9xdM96b-senUnn_lRgX-x4fzTHMdUYTj0CaGisLomTg/T-U-2AVyWlrhIR9IW3hOYP-QUz5AbBjqrYfQIbtEsO8'), ('width', 200), ('height', 200)])), ('full', OrderedDict([('url', 'https://v5.airtableusercontent.com/v1/18/18/1689537600000/1hep_sNYU92kGOpsTGo3UA/m9QlEdr7RvuegKljWO1FJ19judjnKLdJ02vjlOJnGifjtnge7usM-Ts7RzKsX0hUrrDJr4aslM3evmslhr9avg/gmi6NACVy7MpwkeWmbalRThmnAIPVtCIQHYgCMNFrBE'), ('width', 3000), ('height', 3000)]))]))])]",https://www.thorlabs.com/thorproduct.cfm?partnumber=BBD201,https://gitlab.com/ptapping/thorlabs-apt-device/-/blob/main/thorlabs_apt_device/devices/aptdevice.py,https://thorlabs-apt-device.readthedocs.io/en/latest/api/thorlabs_apt_device.html,,,True,,
324,12.0,USA,The NSC-A1 Series motion controller is a powerful single axis stepper motor control system which combines a microstepping driver with a programmable controller into a compact envelope,"Newmark - NSC-A1
",,"**Newmark Systems** is a world leader in precision rotary table technology designed for critical positioning applications
",Instrumental,Newmark,"[OrderedDict([('id', 'attM3smOnSeuTOvf4'), ('width', 400), ('height', 400), ('url', 'https://v5.airtableusercontent.com/v1/18/18/1689537600000/9VNmGrCtZkE2a5Iroi4OBA/oMJhHcai5ciDfGjAjE5idCxNvGx-cRC6PS-vCCOG_vvvQ_Eo9n41SzE09Y9gLDw3hUeKfic5RQez-eDvLXLg6YVlnyLV1NtkCCb72e3z5Dc6voSVz4m2gx5A0yMoSlMaZ0lmr2RwR-ty7J1trPMZfA/7grM1Qe8TnZm0hB3dywP6Zz1OHid3NAyFl9-9L8ZGK0'), ('filename', '9fa1c4833c1536f10e254a8bbe497e6c_400x400.png'), ('size', 60219), ('type', 'image/png'), ('thumbnails', OrderedDict([('small', OrderedDict([('url', 'https://v5.airtableusercontent.com/v1/18/18/1689537600000/m44yavMlubqvoFhI80eCgw/lxAfd8prZmlvwneRTFNPPhXOPW7ceS6YX4Gfp3Rl9xVCPJASgsxLbmoWIM0xLqNYOJAGVtE-Yftoo7EsFmqJiw/oC6bo-Om4ffl8uplGwJea4aw6OSbceV9AuTJFyiO2-k'), ('width', 36), ('height', 36)])), ('large', OrderedDict([('url', 'https://v5.airtableusercontent.com/v1/18/18/1689537600000/SqSwBEzKk1OakKdWbTYSlg/L5wO5fwRfUyNVsP6Dla3tgoVzK8de90XIZ6JihTm9pZo65rlRZgEswY0NknGpZbQd4kMq3deyETKQW1LOovwTQ/8aX957_rZCW-wxOkbeEpZ2TUwkB-aXhWWoKn-T4DAn0'), ('width', 400), ('height', 400)])), ('full', OrderedDict([('url', 'https://v5.airtableusercontent.com/v1/18/18/1689537600000/AgA8ZtAAjFh4uqruL0DOLg/h6kwawcqg8OGfftqapoQxP3P4jAqyDv5SlzInbiMvae9YWPP7HbdqdXMshjmVfbLV-d7d4y2AJAKhBzbIw_QCg/8j3DlDkc_x5RvmsCUFXRxO5XjHr6vpqSc-gPuQQ6AGY'), ('width', 3000), ('height', 3000)]))]))])]",https://www.newmarksystems.com/,Write a Python script that uses Instrumental to connect to a {Device name} Motion,https://en.wikipedia.org/wiki/Motion_control,['Motion'],"Motion controller calculates and controls the mechanical trajectories (motion profile) an actuator must follow (i.e., motion planning) and, in closed loop systems, employs feedback to make control corrections and thus implement closed-loop control.",,https://www.newmarksystems.com/downloads/software/NSC-A/NSC-A1/NSC-A1_Manual_Rev_1.3.0.pdf,"[OrderedDict([('id', 'attus90HqEIevMPuI'), ('width', 810), ('height', 460), ('url', 'https://v5.airtableusercontent.com/v1/18/18/1689537600000/TBUddFT2w-wob4Q0lnqb6A/pU5r2q1C5aXpYOZW5Xqy2_IhFxBGxqh1ixNgJPfOcOV9b04W_CQE2CuD3dPOWnXAc4KxPdrKve0OC0YjQZhlFJxU_FrOsL6pK7cTI2v-kljfVu3-F-cMimMutDN3ZS-9LQM_xdEtqsmqlX74uiC3Rg/v4DfCHYYuuXNp2nEf68Qtjigag_2Moc0hJXrV81cIn4'), ('filename', 'nsc-a1_stepper-motor-controller-main.webp'), ('size', 21528), ('type', 'image/webp'), ('thumbnails', OrderedDict([('small', OrderedDict([('url', 'https://v5.airtableusercontent.com/v1/18/18/1689537600000/U0htofGn0i30QeqNXNjecA/NrvYgPXn6PclZpQB-zCAAFbTG3v2obvWejGKlUeJHgRx_PAuZ1TSE8f1uBgGSWU3ogiypZe3FN5JlZIT-cxCZD5DfwJG8W2vBtxUqPaB9m4/mM3hx7ZAqUPgjfo-vVXaiXayuFZhBVVGGQYKY_iOW58'), ('width', 63), ('height', 36)])), ('large', OrderedDict([('url', 'https://v5.airtableusercontent.com/v1/18/18/1689537600000/dggJ5BJO86kyamDM4nIKNg/IqEpOupf9_194H3HUUZnUROQKQn6lYclihv88Pkau6uNVV2hzOCgr2CfHvP6fKY2yS18JSBAOwaf17TdwrV0TE1Qn_DPUi7uYcxT9EGGdAI/Ulc5pKDfoYFA9J9iSfIAF_baNMWxRh1pUlw3mc2FDB0'), ('width', 810), ('height', 460)])), ('full', OrderedDict([('url', 'https://v5.airtableusercontent.com/v1/18/18/1689537600000/3jOfvBlttNAaz-qGHGnl6w/gAu0NRKonSOBr-bQjfE4DxYytPCOUVERlEdTj0BpRA4mZP0690mOcCiypPcuav_4UeXUL205G4Tmg1b3bKCaghm0FesTudzDAt2WPfVxf5w/v7MEVuQCiL8b-1C7mpFAsX_o-M5eJiqJtZwkOgiK2XI'), ('width', 3000), ('height', 3000)]))]))])]",https://www.newmarksystems.com/motion-controllers/nsc-a1/,https://github.com/mabuchilab/Instrumental/blob/master/instrumental/drivers/motion/newmark.py,https://github.com/mabuchilab/Instrumental/tree/master,1300.0,,,,"# -*- coding: utf-8 -*-
""""""
Drive for controlling Newmark rotation stages
""""""

from . import Motion
from .. import ParamSet
from ...log import get_logger
from .. import VisaMixin
from ..util import check_units
from ... import u
import visa

log = get_logger(__name__)

__all__ = ['NSCA1']


# TODO: There should be a function wrapper
class ImplicitUnitsException(Exception):
    pass


class NSCA1(Motion):
    """""" Class for controlling Newmark NSC AI.

    """"""
    _INST_PARAMS_ = ['serial']

    def _initialize(self):
        self.serial = self._paramset['serial']
        # I only ever use channel 1, but maybe there are needs for more.
        self.channel = 1
        self._rsrc = visa.ResourceManager().open_resource(
            self.serial,
            read_termination='\r',
            write_termination='\r')

    def _cmd(self, cmd):
        self._rsrc.write('@%02d%s' % (self.channel, cmd))
        ret = self._rsrc.read()
        return ret

    def close(self):
        self._rsrc.close()

    @check_units(val='deg')
    def _get_ticks(self, val):
        if val.u not in [u.rad, u.deg]:
            raise ImplicitUnitsException('Mark as degrees or radians')
        return val.to('deg').m * 1e4

    @property
    def angle(self):
        angle = float(self._cmd('PX'))/1e4
        return angle * u.deg

    @angle.setter
    @check_units(angle='deg')
    def angle(self, angle):
        if angle.u not in [u.rad, u.deg]:
            raise ImplicitUnitsException('Mark as degrees or radians')
        self._cmd('ABS')
        self._cmd('X%i' % self._get_ticks(angle))
        self.wait_until_motor_is_idle()

    @check_units(angle='deg')
    def cw(self, angle, background=False):
        """"""
        Rotate clockwise through a specified angle

        Args:
            angle (Quantity): The amount to rotate
            background (bool, optional): If true, the rotation will occur in
                the background to allow other commands to happen while the
                stage rotates
        """"""
        if angle.u not in [u.rad, u.deg]:
            raise ImplicitUnitsException('Mark as degrees or radians')
        val = self._get_ticks(angle)
        #if not self._is_safe(self.x + val):
        #    return

        self._cmd('INC')
        self._cmd('X{}'.format(val))

        if background:
            self.wait_until_motor_is_idle()

    @check_units(angle='deg')
    def ccw(self, angle, background=False):
        """"""
        Rotate counter-clockwise through a specified angle

        Args:
            angle (Quantity): The amount to rotate
            background (bool, optional): If true, the rotation will occur in
                the background to allow other commands to happen while the
                stage rotates
        """"""
        self.cw(-angle, background)

    def wait_until_motor_is_idle(self):
        try:
            while self.is_moving():
                pass
        except KeyboardInterrupt:
            self._cmd('STOP')
            raise KeyboardInterrupt

    def is_stationary(self):
        return int(self._cmd('MST')) == 0

    def is_moving(self):
        return not self.is_stationary()

    @property
    def velocity(self):
        return int(self._cmd('HSPD')) / 1e4 * u.deg / u.s

    @velocity.setter
    @check_units(velocity='deg/s')
    def velocity(self, velocity):
        ticks_per_second = self._get_ticks((velocity * u.s).to('deg'))
        self._cmd('HSPD=%i' % (ticks_per_second))
"
329,23.0,"Beijing, China","RIGOL DG1000 series function/arbitrary waveform generators use Direct Digital Synthesis (DDS) technology. They can generate accurate, stable, clean, low distortion signals.  Each channel can create sine, square, ramp, pulse,or noise signals and coordinate with the other channel. These instruments each have 2 channels with the 2nd channel having a lower voltage output. Each DG1000 can output 4055 point arbitrary waves.",Rigol,,"RIGOL Technologies, Inc. specializes in development and production of test and measuring equipment and is one of the fastest growing Chinese companies in this sphere.
RIGOL’s line of products includes [digital storage oscilloscopes](https://www.tmatlantic.com/e-store/index.php?SECTION_ID=227), [function/arbitrary waveform generators](https://www.tmatlantic.com/e-store/index.php?SECTION_ID=230), [digital multimeters](https://www.tmatlantic.com/e-store/index.php?SECTION_ID=233), PC-based devices compatible with LXI standard etc.
",Instrumental,Rigol,"[OrderedDict([('id', 'attivRtPF5u9Jt8zc'), ('width', 600), ('height', 400), ('url', 'https://v5.airtableusercontent.com/v1/18/18/1689537600000/dK_mPGZZbeZp5bcFtkzbxg/5lP9IsicLc-53c57pU09lJq8btShrsIP8oUdQD8g-Riyyl7CgjgCuLxljMo3u7mSmIXTKA_1rOHlzYVZ9sSzLxtBldRCizWNkSc1c3VGOCyAsmZ8HP6hOtPBhLsT_DGV/oOwHd5lFYWPNGu0Q17nAm8QwNC1tHLy_fE5qOLj7tSs'), ('filename', 'rigol-technologies-inc-logo-vector.png'), ('size', 2793), ('type', 'image/png'), ('thumbnails', OrderedDict([('small', OrderedDict([('url', 'https://v5.airtableusercontent.com/v1/18/18/1689537600000/zLg1x4J0supwCaJ83IL-4A/V-NesMLWPnrBKKPErwXZSINLiLFF25aXtZnMo_kMyeFuWjgTj5V5zEjv57r4fGSxq3VyIPN_4rjf69NIqz8Gzw/pZHIjN5-F-C70IjpD1k3NvdeUOvruDC8BQQZecQyYN8'), ('width', 54), ('height', 36)])), ('large', OrderedDict([('url', 'https://v5.airtableusercontent.com/v1/18/18/1689537600000/TtrY8vWkD6dH3FpegMsnZg/VDR5M5gZRQkV2BrFVjDxcbTNXGQhzU42l5WvrjM-ipcjmlF-7Oq-BO3myo_SOVCkT2wiVPQAjmpQh9cV_64hPQ/5dMGpy23APettmB4rgrzyyIGA_a3a69X1naJ8wxXyDA'), ('width', 600), ('height', 400)])), ('full', OrderedDict([('url', 'https://v5.airtableusercontent.com/v1/18/18/1689537600000/BLGNhQ0TAAvOaix0UZDcpQ/7lmilKMeJr2Ofwuq_NWkkqdKkoUC_-kSB7azAErsuAjnu-0VYL8ejL_P2ioQYZxF9PzGLPVMxaKtkDVN-tY0Sg/juOPtupWPWQ8JaIQ3AdUWz3xKt51fE8OTZzs7jTwlTk'), ('width', 3000), ('height', 3000)]))]))])]",https://www.rigol.com/,Write a Python script that uses Instrumental to connect to a ?? Function Generators,https://en.wikipedia.org/wiki/Function_generator,['Function Generators'],"In electrical engineering, a function generator is usually a piece of electronic test equipment or software used to generate different types of electrical waveforms over a wide range of frequencies. Some of the most common waveforms produced by the function generator are the sine wave, square wave, triangular wave and sawtooth shapes. These waveforms can be either repetitive or single-shot (which requires an internal or external trigger source).[1] Another feature included on many function generators is the ability to add a DC offset. Integrated circuits used to generate waveforms may also be described as function generator ICs.",??,,,https://www.rigolna.com/products/waveform-generators/dg1000/,https://github.com/mabuchilab/Instrumental/blob/master/instrumental/drivers/funcgenerators/rigol.py,not on docs,,,True,,"# -*- coding: utf-8 -*-
# Copyright 2019 Jonathan Wheeler, Christopher Zee
""""""
Driver module for Rigol signal generators.
""""""
from enum import Enum, auto
from . import FunctionGenerator
from .. import VisaMixin, SCPI_Facet
from .. import ParamSet
from visa import ResourceManager

_INST_PARAMS = ['visa_address']
_INST_VISA_INFO = {
    'DG800': ('Rigol Technologies', ['DG811', 'DG812']),
}

MANUFACTURER_ID = 0x1AB1


class SpecTypes(Enum):
    DG811 = 0x0643


class Waveform(Enum):
    PULSe = auto()


def list_instruments():
    """"""Get a list of all spectrometers currently attached""""""
    paramsets = []
    model_string = '|'.join('{:04X}'.format(spec.value) for spec in SpecTypes)
    search_string = ""USB[0-9]*::0x{:04X}::0x({})"".format(MANUFACTURER_ID, model_string)
    rm = ResourceManager()

    try:
        raw_spec_list = rm.list_resources(search_string)
    except:
        return paramsets

    for spec in raw_spec_list:
        _, _, model, serial, _ = spec.split('::', 4)
        model = SpecTypes(int(model, 0))
        paramsets.append(ParamSet(DG800, usb=spec, serial=serial, model=model))

    return paramsets


class RigolFunctionGenerator(FunctionGenerator, VisaMixin):
    def _initialize(self):
        self._rsrc.read_termination = '\n'


class OnOffState(Enum):
    ON = True
    OFF = False


class DG800(RigolFunctionGenerator, VisaMixin):
    frequency1 = SCPI_Facet('SOURce1:FREQuency', convert=float, units='Hz')
    frequency2 = SCPI_Facet('SOURce2:FREQuency', convert=float, units='Hz')
    amplitude1 = SCPI_Facet('SOURce1:VOLTage:AMPlitude', convert=float, units='V')
    amplitude2 = SCPI_Facet('SOURce2:VOLTage:AMPlitude', convert=float, units='V')
    offset1 = SCPI_Facet('SOURce1:VOLTage:OFFSet', convert=float, units='V')
    offset2 = SCPI_Facet('SOURce2:VOLTage:OFFSet', convert=float, units='V')
    phase1 = SCPI_Facet('SOURce1:PHASe', convert=float, units='deg')
    phase2 = SCPI_Facet('SOURce2:PHASe', convert=float, units='deg')
    width1 = SCPI_Facet('SOURce1:FUNCtion:PULSe:WIDTh', convert=float, units='s')
    width2 = SCPI_Facet('SOURce2:FUNCtion:PULSe:WIDTh', convert=float, units='s')
    waveform1 = SCPI_Facet('SOURce1:FUNCtion')
    waveform2 = SCPI_Facet('SOURce2:FUNCtion')
    duty_cycle1 = SCPI_Facet('SOURce1:FUNCtion:PULSe:DCYC', convert=float)
    duty_cycle2 = SCPI_Facet('SOURce2:FUNCtion:PULSe:DCYC', convert=float)
    output1 = SCPI_Facet('OUTPut1:STATe', convert=OnOffState)
    output2 = SCPI_Facet('OUTPut2:STATe', convert=OnOffState)

    @property
    def manufacturer(self):
        manufacturer, _, _, _ = self.query('*IDN?').rstrip().split(',', 4)
        return manufacturer

    @property
    def model(self):
        _, model, _, _ = self.query('*IDN?').rstrip().split(',', 4)
        return model

    @property
    def serial(self):
        _, _, serial, _ = self.query('*IDN?').rstrip().split(',', 4)
        return serial

    @property
    def version(self):
        _, _, _, version = self.query('*IDN?').rstrip().split(',', 4)
        return version

    @property
    def output1(self):
        val = self.query('OUTPut1:STATe?')
        return OnOffState[val].value

    @output1.setter
    def output1(self, val):
        val = int(bool(val))
        self.write('OUTPut1:STATe %s' % OnOffState(val).name)

    @property
    def output2(self):
        val = self.query('OUTPut2:STATe?')
        return OnOffState[val].value

    @output2.setter
    def output2(self, val):
        val = int(bool(val))
        self.write('OUTPut2:STATe %s' % OnOffState(val).name)

    def align(self):
        # /*Executes an align phase operation on CH1.*/
        self._rsrc.write(':SOUR1:PHAS:INIT')

        # /*Executes an align phase operation on CH2.*/
        # :SOUR2:PHAS:SYNC

    def apply1(self, waveform, frequency=None, amplitude=None, offset=None, phase=None):
        self.write('SOURce1:APPLy:{} {},{},{},{}'.format(Waveform(waveform).name, frequency, amplitude, offset, phase))

    def local(self):
        self.write('SYSTem:LOCal')

    def remote(self):
        self.write('SYSTem:REMote')
"
359,7.1,USA,The 771 Series Laser Spectrum Analyzer combines proven Michelson interferometer technology with fast Fourier transform analysis resulting in a unique instrument that operates as both a high-resolution spectrum analyzer and a high-accuracy wavelength meter.,Bristol,,"**Bristol Instruments**' Wavelength Meters Are For Scientists And Engineers. Precise laser wavelength measurement and complete spectral analysis. Reliable accuracy.

",Instrumental,Bristol,"[OrderedDict([('id', 'attEnd6xRSoAmFpME'), ('url', 'https://v5.airtableusercontent.com/v1/18/18/1689537600000/omMEvjtPmIxKH599oLkE6Q/gCTkLpTwBlDi7ahrfFyv7TLKCIzUZIZwvoi6AB8BZWdDPmF1HXX6vnKiYTuLfwdg6tF5WxrPsfr65ScxE6sLWw2uCEK-nfUGWxd3zuJBxbvuRoCHwqIBTmQpxjBeMNJ5asXYrGILlJxqSR3SIoK8HQ/LIMLCza8MxSrI4ZthcClbOwBaj_YwRRT6xHLvslgjss'), ('filename', 'Bristol_logo_BRIGHT_official_2x.5df1145a1c08b.avif'), ('size', 5617), ('type', 'image/avif')])]",https://www.bristol-inst.com/,Write a Python script that uses Instrumental to connect to a Bristol 771  Spectrum Analyzers,https://en.wikipedia.org/wiki/Spectrum_analyzer,['Spectrum Analyzers'],"A spectrum analyzer measures the magnitude of an input signal versus frequency within the full frequency range of the instrument
",Bristol 771 ,https://bristolinst.wpenginepowered.com//wp-content/uploads/2020/01/Bristol-771-Series-Laser-Spectrum-Analyzer-Specifications.pdf,"[OrderedDict([('id', 'attibm1oQTNYNoysV'), ('width', 590), ('height', 400), ('url', 'https://v5.airtableusercontent.com/v1/18/18/1689537600000/1kyzMMIt5-n4Ynev38Rufw/tmkdJsZptoLeyVQFOuXF0k7wz0YBGrLDIcj-N1p8uE1G-E6KF413pMvkB61UXGKu06cl8ndrRjLj3Ymw9a6pmdR5lX009bxDppM2YlvBE1k/Vxpe77HM_UEyrb1M0t47Nm2di2EibOmaa2zVQkvutGI'), ('filename', 'thumbnail_63095.png'), ('size', 92197), ('type', 'image/png'), ('thumbnails', OrderedDict([('small', OrderedDict([('url', 'https://v5.airtableusercontent.com/v1/18/18/1689537600000/iSONlknGrDVSqBq93NKjQQ/jwfIiOOvJEMgxQ2aYUNoUWRxLoOtNNvuro6vIaqKiy03dRidarAkhEzaipYr_nwNb0z1BjCOiifA0OI3AsH81A/ggAQf0AetX6ntY17dmHsaGd_ElpegjOsompTbM8IkVU'), ('width', 53), ('height', 36)])), ('large', OrderedDict([('url', 'https://v5.airtableusercontent.com/v1/18/18/1689537600000/F9dRuHFuDNFYKm8BdzjDwQ/hX7LwiAF1bVNp0EubgVUus21pGGPP1Swr4lSY1Fkjtd6aC1sUxYVmrj5mdhB9LeMXS2wjsbZaTEypcxJuVDhQA/JIEgoe466Tbbo5_Jg72FYeDj9B3f0NFex8FoSfhyTmk'), ('width', 590), ('height', 400)])), ('full', OrderedDict([('url', 'https://v5.airtableusercontent.com/v1/18/18/1689537600000/3qotE3dsKbAmvcvOmwp4Gg/LsApkD2-GPqjz9UjAHm-4wo5ZAuM1Xly5Igk47kqS9buzMJpYbn4dBLr0dTw2cWc4ZBnWFbR2udGxRi_BUzi_Q/pOm7wrK1zRk6m4_b7tnvxOPoD5ER9vrKhwcxIMkHQT0'), ('width', 3000), ('height', 3000)]))]))])]",https://www.bristol-inst.com/bristol-instruments-products/spectrum-analyzers/771-series-laser-spectrum-analyzer/,https://github.com/mabuchilab/Instrumental/blob/master/instrumental/drivers/spectrometers/bristol.py,https://instrumental-lib.readthedocs.io/en/stable/spectrometers-bristol.html,,,,,"# -*- coding: utf-8 -*-
# Copyright 2014-2017 Nikolas Tezak, Nate Bogdanowicz
""""""
Driver for Bristol 721 spectrum analyzers.
""""""
import os
import sys
from contextlib import contextmanager
from functools import wraps
import ctypes
import numpy as np
from numpy.ctypeslib import ndpointer

from . import Spectrometer
from .. import ParamSet
from ... import u

_INST_PARAMS = ['port']
_INST_CLASSES = ['Bristol_721']

bristol_dll = ctypes.WinDLL(""BristolFFT.dll"", use_errno=True, use_last_error=True)
bristol_lv_dll = ctypes.WinDLL(""BristolLV.dll"", use_errno=True, use_last_error=True)

# Set DLL function signatures
find_comm_port = bristol_lv_dll.CLFindBristolCommPort
find_comm_port.restype = ctypes.c_int

open_device = bristol_dll.CLOpenUSBSerialDevice
open_device.argtypes = [ctypes.c_int]
open_device.restype = ctypes.c_int

get_lambda = bristol_dll.CLGetLambdaReading
get_lambda.argtypes = [ctypes.c_int]
get_lambda.restype = ctypes.c_double

bristol_dll.CLGetPowerReading.argtypes = [ctypes.c_int]
bristol_dll.CLGetPowerReading.restype = ctypes.c_float

bristol_lv_dll.CLGetNextSpectrum.argtypes = [
    ctypes.c_int,
    ndpointer(np.float32, ndim=1, flags='C_CONTIGUOUS'),
    ndpointer(np.float32, ndim=1, flags='C_CONTIGUOUS'),
    ctypes.c_int
]


# Adapted from <http://stackoverflow.com/a/17954769>
@contextmanager
def stderr_redirected(to=os.devnull):
    fd = sys.stderr.fileno()

    def _redirect_stderr(to):
        sys.stderr.close()
        os.dup2(to.fileno(), fd)
        sys.stderr = os.fdopen(fd, 'w')

    with os.fdopen(os.dup(fd), 'w') as old_stderr:
        with open(to, 'w') as file:
            _redirect_stderr(to=file)
        try:
            yield
        finally:
            _redirect_stderr(to=old_stderr)


# Decorator for redirecting stderr to /dev/null for the function's duration
def ignore_stderr(f):
    @wraps(f)
    def wrapper(*args, **kwargs):
        with stderr_redirected():
            return f(*args, **kwargs)
    return wrapper


# Table obtained from Bristol technical support
FFT_INFO = {
    ('VIS', 0): (131072, 1, 3, 350, 421),
    ('VIS', 1): (131072, 1, 2, 423, 632),
    ('VIS', 2): (131072, 1, 1, 632, 1100),
    ('NIR', 0): (131072, 1, 2, 500, 632),
    ('NIR', 1): (131072, 1, 1, 633, 1265),
    ('NIR', 2): (32768,  4, 3, 1266, 1685),
    ('IR',  0): (65536,  1, 0, 1266, 5000),
    ('MIR', 0): (32768,  5, 1, 4000, 6329),
    ('MIR', 1): (16384,  9, 1, 5700, 11000),
    ('XIR', 0): (131072, 1, 0, 1500, 12000),
}


def bin_index(fft_size, dec, fold, lamb):
    if fold % 2 == 0:
        bin = ((2*dec*632.9914)/lamb - fold) * fft_size
    else:
        bin = (fold + 1 - (2*dec*632.9914)/lamb) * fft_size
    return bin


class Bristol_721(Spectrometer):
    def _initialize(self):
        self._com_port = self._paramset.get('port') or find_comm_port()
        self._dll = bristol_dll
        self._lv = bristol_lv_dll
        self._handle = open_device(self._com_port)

        if self._handle < 0:
            raise Exception(""Failed to open connection to spectrum analyzer"")
        elif self._handle == 0:
            raise Exception(""You must have already opened the port"")

        self._lv.CLRegister(self._handle)

        # Hard-coded for now, ideally we'd auto-detect this via a model byte or something
        self._model = 'IR'
        self._range_num = 0

    def _fft_range(self):
        fft_size, dec, fold, low_wav, hi_wav = FFT_INFO[(self._model, self._range_num)]
        low_index = bin_index(fft_size, dec, fold, low_wav)
        high_index = bin_index(fft_size, dec, fold, hi_wav)
        if low_index > high_index:
            low_index, high_index = high_index, low_index
        return int(low_index+1), int(high_index-1)  # Exclude edge of the range to be safe

    def get_wavelength(self):
        """"""Get the vacuum wavelength of the tallest peak in the spectrum

        Returns
        -------
        wavelength : Quantity
            The peak wavelength, in nm
        """"""
        return self._dll.CLGetLambdaReading(self._handle) * u.nm

    def get_spectrum(self):
        """"""Get a spectrum.

        Returns
        -------
        x : array Quantity
            Wavelength of the bins, in nm
        y : array
            Power in each bin, in arbitrary units
        """"""
        start, stop = self._fft_range()
        size = stop-start
        self._dll.CLGetFullSpectrum(self._handle, start, stop)

        wavenumber = np.zeros(size, np.float32)
        power = np.zeros(size, np.float32)

        ret = 0
        while not ret:
            ret = self._lv.CLGetNextSpectrum(self._handle, power, wavenumber, size)

        # For some reason, x is not sorted already...
        indices = np.argsort(wavenumber)
        return 1e10/wavenumber[indices] * u.nm, power[indices]  # Is this scaling right?

    def close(self):
        self._dll.CLCloseDevice(self._handle)


def list_instruments():
    com_ports = [find_comm_port()]  # TODO: Add support for multiple connected devices
    return [ParamSet(Bristol_721, port=port) for port in com_ports]
"
369,15.0,USA,"The MCS2 is SmarAct’s most versatile and modular control system.

Its stringent modular design approach makes it the perfect choice for all field of applications
","Smaract (MCS and SDC
)",,"In our [Motion](https://www.smaract.com/en/motion) business unit, we develop and produce high-precision, compact products for nano-positioning that meet the highest demands while being easy to handle

",Instrumental,Smaract,"[OrderedDict([('id', 'att0tQm23j3IZTujq'), ('width', 455), ('height', 111), ('url', 'https://v5.airtableusercontent.com/v1/18/18/1689537600000/Ja1TPO7_jYrnIe7OWuimNQ/FBfBOkXXnHy60fBj_Vvr8CSTM6h3EV16jXkJdmBMoUX5e_0_ZrRHDUmB6Lvk80MmjbsM7WvSrXBvfuURYvod4ZvKa4PAWHIDO-J502WlS6E/po8OaEG2ZZ3Ke_OwtrhSzVdUbcs24x4xujyWCY2oGaE'), ('filename', 'images (1).png'), ('size', 5259), ('type', 'image/png'), ('thumbnails', OrderedDict([('small', OrderedDict([('url', 'https://v5.airtableusercontent.com/v1/18/18/1689537600000/3gEbpISJjnQdfqiVEMwU4g/QK3hzhk8D-JSvrnUHNBEjv5qFvV68h4So11uikDZEs0-kzvvqtDC2nWWH880Nb6mozuoIW8An4Hh8DUdYnNgCQ/Wmca2oOlz6l8xVDcAADYIaK5rS6staGlnPEsQcx4c4U'), ('width', 148), ('height', 36)])), ('large', OrderedDict([('url', 'https://v5.airtableusercontent.com/v1/18/18/1689537600000/DVUXg9jJG2heS-cYLeN4dg/mcXH8-HuCZufVl0Wmi8FtzN_BF2E4ID9iKiH2RckO8NhcOaJImUId2hvhtN68R-c7lumbCHZdtLiz8aBcELSZA/2Erffk_nqXsoZD9U5Y2yjw8DV73gLGP4nVpoi8jbAR4'), ('width', 455), ('height', 111)])), ('full', OrderedDict([('url', 'https://v5.airtableusercontent.com/v1/18/18/1689537600000/xTEAOW3l0J3PvvYSmggW2Q/z5gLADYuJONi0s57mHvQRoZlbeX6tumH7DHUWj1tLzfpA07SEHm7PevXSM8R1qA2hv4CJ9I2pfzpCcSMkHmgrg/XJO9kCQIy5m58zTsjQ6dUDjhSKi92vsUde5-tg3_HNg'), ('width', 3000), ('height', 3000)]))]))])]",https://www.smaract.com/en/,Write a Python script that uses Instrumental to connect to a Smaract - MCS2 Motion,https://en.wikipedia.org/wiki/Motion_control,['Motion'],"Motion controller calculates and controls the mechanical trajectories (motion profile) an actuator must follow (i.e., motion planning) and, in closed loop systems, employs feedback to make control corrections and thus implement closed-loop control.",Smaract - MCS2,https://www.smaract.com/en/control-systems-and-software/product/mcs2?download=files/media/files/MCS2_Modular_Concept.pdf,"[OrderedDict([('id', 'attfpiboz0V74uoiM'), ('width', 275), ('height', 183), ('url', 'https://v5.airtableusercontent.com/v1/18/18/1689537600000/u8539uBN46E9OAAJFeg4Fw/ahUDDuGTft7b0SP9uTG7IGx5wVmDs8moQLwCBA-wbivEl1lZgipXUVvCXBD55wd_vyNWY9kpNH5pyiNjOV11JDIJDe6bPHYJgaElsD66EYU/xzFfxR1fEtJrRgmWxr2YLe_bqQojyBt8fPv-8U19QG0'), ('filename', 'téléchargement (3).jpeg'), ('size', 4112), ('type', 'image/jpeg'), ('thumbnails', OrderedDict([('small', OrderedDict([('url', 'https://v5.airtableusercontent.com/v1/18/18/1689537600000/E5UeHm3PT7lTBRUukHWRLg/a4Nmja7ALyIZkmVOkdHZSDwuPqogehGIVBMWocUvw5h5Xf3zlIyd63GVpQdn7nKx0tQAbqXsbzv1ukkeI4XzFw/hjJB9BrrLyI_trkOcJ5oozagK5aRrAEpYvQCHqZNsCc'), ('width', 54), ('height', 36)])), ('large', OrderedDict([('url', 'https://v5.airtableusercontent.com/v1/18/18/1689537600000/nDgGILSXPlrkPKD4dnF2bw/6ZTHk2fhKoOXVBfrOMSFfHJ4dakaVecApqHKU2U7owBjtZC2GsN66pvHn73gqeQ5Da6bqfdMLOKHp6ke8yjcOQ/FIFEwlLCY-fBDOLmMIQY7kYGWjHjo22j5LLbxNuvbUg'), ('width', 275), ('height', 183)])), ('full', OrderedDict([('url', 'https://v5.airtableusercontent.com/v1/18/18/1689537600000/hzSoYKtU4yNO4daR2NhqRQ/YDeueAUVdNeUMO8buL7W52zcNJMi7gSdVecfIYeZb7UaXMVvmXcR1LXNf-n_wEJTosq1IIA4KaCeGNoJyxdPyw/Il-CqxlvtdiXclCNeDKbjUL0tw4ZvXk_Ay5c9wuKcMY'), ('width', 3000), ('height', 3000)]))]))])]",https://www.smaract.com/en/control-systems-and-software/product/mcs2#description,https://github.com/ALBA-Synchrotron/smaract/blob/master/smaract/controller.py,https://github.com/ALBA-Synchrotron/smaract/tree/master,,,,,"# ------------------------------------------------------------------------------
# This file is part of smaract (https://github.com/ALBA-Synchrotron/smaract)
#
# Copyright 2008-2017 CELLS / ALBA Synchrotron, Bellaterra, Spain
#
# Distributed under the terms of the GNU General Public License,
# either version 3 of the License, or (at your option) any later version.
# See LICENSE.txt for more info.
# ------------------------------------------------------------------------------


from .constants import *
from .axis import SmaractSDCAxis, SmaractMCSAngularAxis, SmaractMCSLinearAxis
from .communication import SmaractCommunication


class SmaractBaseController(list):
    """"""
    Smaract Controller Base class. Contains the common Smaract ASCii API for any
    Smaract motor controller. The methods here implemented correspond to those
    at the controller level. It also implements a generic send_cmd function to
    automatically handles the error codes. This send_cmd functions is based on
    the communication base class, which provides an abstraction from the
    hardware layer communication.
    """"""
    ERROR_CODES = {0: 'No Error',
                   1: 'Syntax Error',
                   2: 'Invalid Command Error',
                   3: 'Overflow Error',
                   4: 'Parse Error',
                   5: 'Too Few Parameters Error',
                   6: 'Too Many Parameters Error',
                   7: 'Invalid Parameter Error',
                   8: 'Wrong Mode Error',
                   129: 'No Sensor Present Error',
                   140: 'Sensor Disabled Error',
                   141: 'Command Overridden Error',
                   142: 'End Stop Reached Error',
                   143: 'Wrong Sensor Type Error',
                   144: 'Could Not Find Reference Mark Error',
                   145: 'Wrong End Effector Type Error',
                   146: 'Movement Locked Error',
                   147: 'Range Limit Reached Error',
                   148: 'Physical Position Unknown Error',
                   150: 'Command Not Processable Error',
                   151: 'Waiting For Trigger Error',
                   152: 'Command Not Triggerable Error',
                   153: 'Command Queue Full Error',
                   154: 'Invalid Component Error',
                   155: 'Invalid Sub Component Error',
                   156: 'Invalid Property Error',
                   157: 'Permission Denied Error',
                   159: 'Power Amplifier Disabled Error'}

    SENSOR_CODE = {1: 'S',
                   2: 'SR',
                   3: 'ML',
                   4: 'MR',
                   5: 'SP',
                   6: 'SC',
                   7: 'M25',
                   8: 'SR20',
                   9: 'M',
                   10: 'GC',
                   11: 'GD',
                   12: 'GE',
                   13: 'RA',
                   14: 'GF',
                   15: 'RB',
                   16: 'G605S',
                   17: 'G775S',
                   18: 'SC500',
                   19: 'G955S',
                   20: 'SR77',
                   21: 'SD',
                   22: 'R20ME',
                   23: 'SR2',
                   24: 'SCD',
                   25: 'SRC',
                   26: 'SR36M',
                   27: 'SR36ME',
                   28: 'SR50M',
                   29: 'SR50ME',
                   30: 'G1045S',
                   31: 'G1395S',
                   32: 'MD',
                   33: 'G935M',
                   34: 'SHL20',
                   35: 'SCT'}

    LINEAR_SENSORS = [1, 5, 6, 9, 18, 21, 24, 32, 35]

    ROTARY_SENSORS = [2, 8, 14, 20, 22, 23, 25, 26, 27, 28, 29]

    def __init__(self, comm_type, *args):
        """"""
        Class constructor. Requires an axis or list of axes from class
        SmaractBase axis (or derived classes).

        :param axes: axis or list of axes.
        """"""
        list.__init__(self)
        self._comm = SmaractCommunication(comm_type, *args)

    def send_cmd(self, cmd):
        """"""
        Communication function used to send any command to the smaract
        controller.
        :param cmd: string command following the Smaract ASCii Programming
        Interface.
        :return:
        """"""
        ans = self._comm.send_cmd(cmd)
        flg_error = ans[0] == 'E' and ans[1] != 'S'
        if flg_error:
            error_code = int(ans.rsplit(',', 1)[1])
            if error_code != 0:
                if error_code in self.ERROR_CODES:
                    error_msg = self.ERROR_CODES[error_code]
                else:
                    error_msg = 'There is not message for this error on ' + \
                        'the documentation'
                msg = ('Error %d: %s' % (error_code, error_msg))
                                                   
                raise RuntimeError(msg)
        return ans

    @property
    def comm_type(self):
        """"""
        Get the communication type for this controller.

        :return: communication type
        """"""
        return self._comm.get_comm_type()

    # 3.1 - Initialization commands
    # -------------------------------------------------------------------------
    @property
    def version(self):
        """"""
        Get the interface version of the system.

        :return: String representing the current interface version.
        """"""
        cmd = 'GIV'
        ans = self.send_cmd(cmd)
        return 'Version: %s' % '.'.join(ans[2:].split(','))

    @property
    def nchannels(self):
        """"""
        Get the number of channels available (does not represent the number of
        currently connected positioners and effectors). The channels indexed are
        zero based.

        :return: the number of channels configured.
        """"""
        cmd = 'GNC'
        ans = self.send_cmd(cmd)
        return int(ans[1:])

    @property
    def id(self):
        """"""
        Identify the controller with a unique ID.

        :return: system ID.
        """"""
        cmd = 'GSI'
        ans = self.send_cmd(cmd)
        return ans


class SmaractSDCController(SmaractBaseController):
    """"""
    Specific Smaract motor controller class for a Step and Direction Controller
    (SDC). This class extends the base class with the ASCII commands specific
    for the SDC motion controller.
    """"""
    def __init__(self, comm_type, *args):
        SmaractBaseController.__init__(self, comm_type, *args)
        axis = SmaractSDCAxis(self)
        self.append(axis)


class SmaractMCSController(SmaractBaseController):
    """"""
    Specific Smaract motor controller class for a Modular Controller System
    (MCS). This class extends the base class with the ASCII commands specific
    for the MCS motion controller.
    """"""

    def __init__(self, comm_type, *args):
        SmaractBaseController.__init__(self, comm_type, *args)

        # Configure communication mode to synchronous
        # The communication library work with acknowledge

        mode = CommunicationMode.SYNC
        cmd = 'SCM%d' % mode
        self.send_cmd(cmd)
        self._create_axes()
        
    def _create_axes(self):
        # We need to remake the complete axis list
        try:
            while True:
                self.pop()
        except IndexError:
            pass

        for axis_nr in range(self.nchannels):
            ans = self.send_cmd('GST%d' % axis_nr)
            sensor_code = int(ans.rsplit(',', 1)[1])
            if sensor_code in self.LINEAR_SENSORS:
                axis = SmaractMCSLinearAxis(self, axis_nr)
                self.append(axis)
            elif sensor_code in self.ROTARY_SENSORS:
                axis = SmaractMCSAngularAxis(self, axis_nr)
                self.append(axis)
            else:
                msg = ""Failed to create axis %s\n"" % axis_nr
                msg += 'There is not axis class for sensor code %d' % sensor_code
                raise RuntimeError()

    # 3.1 - Initialization commands
    # -------------------------------------------------------------------------
    @property
    def communication_mode(self):
        """"""
        Gets the type of communication with the controller.
        0: synchronous communication (SYNC).
        1: asynchronous communication (ASYNC).

        :return: current communication mode.
        """"""
        ans = self.send_cmd('GCM')
        return int(ans[-1])

    # The communication class is based on synchronous communication, for that
    #  reason it is not possible to change the type of communication.
    # @communication_mode.setter
    # def communication_mode(self, mode):
    #     """"""
    #     Sets the type of communication with the controller.
    #
    #     :param mode: 0 (SYNC) or 1 (ASYMC)
    #     :return: None
    #     """"""
    #     cmd = 'SCM%d' % mode
    #     self.send_cmd(cmd)

    def reset(self):
        """"""
        Performs a system reset, equivalent to a power up/down cycle.

        :return: None
        """"""
        ans = self.send_cmd('R')
        return float(ans.split(',')[1])

    def set_hcm_enabled(self, mode):
        """"""
        Sets the Hand Control Module operation mode:
        0: Disabled
        1: Enabled
        2: Read-Only

        :param mode: integer representing the operation mode.
        :return: None
        """"""
        cmd = 'SHE%d' % mode
        self.send_cmd(cmd)

    # 3.2 - Configuration commands
    # -------------------------------------------------------------------------
    @property
    def sensor_enabled(self):
        """"""
        Gets the current sensor operation mode.

        0: Disabled (DISABLED)
        1: Enabled (ENABLED)
        2: Power save (POWER_SAVE)

        :return: integer representing the sensor operation mode.
        """"""
        cmd = 'GSE'
        ans = self.send_cmd(cmd)
        return str(ans[-1])

    @sensor_enabled.setter
    def sensor_enabled(self, enabled):
        """"""
        Sets the current sensor operation mode.

        :param enabled: operation mode value (0,1 or 2)
        :return: None
        """"""
        cmd = 'SSE%d' % enabled
        self.send_cmd(cmd)

    def trigger_command(self, trigger_idx=0):
        """"""
        Triggers the commands that were loaded into the command queue. Each
        is loaded with a given trigger index, grouping the commands loaded.
        There are 256 trigger index available, from 0 to 255, which correspond
        to a code range between 1792 and 2047.

        :param trigger_idx: trigger index of the command(s).
        :return: None
        """"""
        is_trigger_in_range(int(trigger_idx))
        cmd = 'TC%d' % (trigger_idx + TRIGGER_INDEX_0)
        self.send_cmd(cmd)

    # 3.5 - Miscellaneous commands
    # -------------------------------------------------------------------------
    def configure_baudrate(self, baudrate):
        """"""
        Sets the baudrate of the RS-232 interface.
        IMPORTANT: NOT AVAILABLE for network interface.

        :param baudrate: valid baudrate value
        :return: applied baudrate value.
        """"""
        # TODO: Restrict only to serial communication layer.
        is_baudrate_in_range(baudrate)
        cmd = 'BR%d' % baudrate
        ans = self.send_cmd(cmd)
        return int(ans[2:])

    def keep_alive(self, delay=0):
        """"""
        Timeout mechanism to stop all positioners immediately if the system does
        not receive a command in a certain interval. If delay=0 disables this
        feature.

        :param delay: timeout in ms.
        :return: None
        """"""
        cmd = 'K%d' % delay
        self.send_cmd(cmd)
"
383,550.0,USA,"KDC101 - K-Cube Brushed DC Servo Motor Controller (Power Supply Not Included) 
","Thorlabs - KDC101
",,"Thorlabs, Inc. is an American privately held optical equipment company headquartered in Newton, New Jersey. The company was founded in 1989 by Alex Cable, who serves as its current president and CEO. As of 2018, Thorlabs has annual sales of approximately $500 million.
",Instrumental,Thorlabs,"[OrderedDict([('id', 'attUZyCLkkmIuEgLg'), ('width', 2560), ('height', 398), ('url', 'https://v5.airtableusercontent.com/v1/18/18/1689537600000/BmNIeFHbhni_Wx2fsTQ7xA/7KGqzexbwgJQF4Glvxuce4xqbhQEHXktvcuNjxXhlvAhkK8CMrzKBeVv_-PHP7tmn-ApzOCstqQ78s4C7I55B6lFLXiYxxm-LRTgXLyuUC0/lrbUj5K4TTuMnassRPaSVDuk_3XZ5ajRvKcgWES7DOs'), ('filename', 'Image.png'), ('size', 70770), ('type', 'image/png'), ('thumbnails', OrderedDict([('small', OrderedDict([('url', 'https://v5.airtableusercontent.com/v1/18/18/1689537600000/ZZWxVhiPn1kMizuxLLk0Fg/0ebbYsrz7yPvC0KBBzNXH_YPxfCVcdt1ddf0aE3R7HCJoloiw8p80VHX4oB56xrqE7mAvyArGgfM18_lSXRRgQ/_9jZYvkMYMGOsykN6AlnWrE4VLLRcgITDBJiyCGI1Jk'), ('width', 232), ('height', 36)])), ('large', OrderedDict([('url', 'https://v5.airtableusercontent.com/v1/18/18/1689537600000/zIACzGEswy_VqAPI41RE0w/p9fZRVc7zXxmA-O88-PkMCrGiAgfV6ZhiPuJIdJges4pm8vK0kRS4RVlbvX3ISNKHhkpjNxGWTHFMS-Z1i6ltw/Q8t43BvydtWeEdRBXsOXk-0v2Fk3iEIno3tA1RqZfYA'), ('width', 2560), ('height', 398)])), ('full', OrderedDict([('url', 'https://v5.airtableusercontent.com/v1/18/18/1689537600000/g4og31dOk-TKjerxqVIoQg/QGIUp1sDPwfwfv7ZVi9tuQgRcWx7g3ITLMgEtKB1jNCQNc21T5YWxgXMcBFvV1ABa0s_2hn56K_GXiQvwAKfzg/1Q3db6fhAbr_EEtoThHWZoCVFkSliLdsG_PPrvp8Zeo'), ('width', 3000), ('height', 3000)]))]))])]",https://www.thorlabs.com/,Write a Python script that uses Instrumental to connect to a Thorlabs - KDC101 Motion,https://en.wikipedia.org/wiki/Motion_control,['Motion'],"Motion controller calculates and controls the mechanical trajectories (motion profile) an actuator must follow (i.e., motion planning) and, in closed loop systems, employs feedback to make control corrections and thus implement closed-loop control.",Thorlabs - KDC101,https://www.thorlabs.com/_sd.cfm?fileName=ETN017655-D02.pdf&partNumber=KDC101,"[OrderedDict([('id', 'attPIz9PYvkWq7hsV'), ('width', 300), ('height', 300), ('url', 'https://v5.airtableusercontent.com/v1/18/18/1689537600000/DGZH2aUdX_s4IjDqzofRog/GfuA7zGQ63IMSQKDiLEHUEcFxo9ITIO9Rjbs7urVQVDY7SddZZA5LhlZsDYh1Osd6aQcqmZTBc1CFQzsd9EILjMArWDtxvdMJ5pVRMtPfF0/d9bL9pL-Pgv_PvTGVJduh4YrKe-PvTjNbS0wz4uGg98'), ('filename', 'ETN017655-lrg.jpg'), ('size', 51070), ('type', 'image/jpeg'), ('thumbnails', OrderedDict([('small', OrderedDict([('url', 'https://v5.airtableusercontent.com/v1/18/18/1689537600000/Dkj14jBR0uTx9hOnCZsFjg/0QZvAJqsOOYzZbxj0HNKLhyQcIw_eOBR93RnoWwNA7UsPZA0WrM5wuvV6pPDlhvB94Z2F2RROLUOopbBG9vRww/nDclsBDvWVRED_CpLJp_L9HwPait79-95-4GIefdcTM'), ('width', 36), ('height', 36)])), ('large', OrderedDict([('url', 'https://v5.airtableusercontent.com/v1/18/18/1689537600000/TDHQrc6brPZwQJOsacF3jQ/8qfoOyUoMnsSV9zbTxqWNq3cbmwJmaehrhGZxzbPk9jwc79ckHRzjnmC5I5nZnL_DHqc00lBAu60-FnT-a0NjA/U5d5OgUXSZF-fRvZ3VZy1KMjcHwK6EXwoa_fZ31LlFs'), ('width', 300), ('height', 300)])), ('full', OrderedDict([('url', 'https://v5.airtableusercontent.com/v1/18/18/1689537600000/QJxJ1JkFWu9Q6bVN6dRaRw/Ni0vSDg-TrJXW8dtfnpNMHPM9AvC830Uvh5imBY4Z8C4OW9eVe2JLA9mpLIxUqyAi4cNkgtbw4OFxlUW-p0PbA/XiAN72E2lqOfzwBssF102bhDh4XqqFVm5-LgRpQwJ9o'), ('width', 3000), ('height', 3000)]))]))])]",https://www.thorlabs.com/thorproduct.cfm?partnumber=KDC101,https://gitlab.com/ptapping/thorlabs-apt-device/-/blob/main/thorlabs_apt_device/devices/kdc101.py,https://thorlabs-apt-device.readthedocs.io/en/latest/api/thorlabs_apt_device.html,760.0,,,,
413,550.0,USA,"TDC001 - T-Cube DC Servo Motor Controller (Power Supply Not Included) 
","Thorlabs - TDC001
",,"**Thorlabs** specializes in the building blocks for laser and fiber optic systems.

",Instrumental,Thorlabs,"[OrderedDict([('id', 'attUZyCLkkmIuEgLg'), ('width', 2560), ('height', 398), ('url', 'https://v5.airtableusercontent.com/v1/18/18/1689537600000/BmNIeFHbhni_Wx2fsTQ7xA/7KGqzexbwgJQF4Glvxuce4xqbhQEHXktvcuNjxXhlvAhkK8CMrzKBeVv_-PHP7tmn-ApzOCstqQ78s4C7I55B6lFLXiYxxm-LRTgXLyuUC0/lrbUj5K4TTuMnassRPaSVDuk_3XZ5ajRvKcgWES7DOs'), ('filename', 'Image.png'), ('size', 70770), ('type', 'image/png'), ('thumbnails', OrderedDict([('small', OrderedDict([('url', 'https://v5.airtableusercontent.com/v1/18/18/1689537600000/ZZWxVhiPn1kMizuxLLk0Fg/0ebbYsrz7yPvC0KBBzNXH_YPxfCVcdt1ddf0aE3R7HCJoloiw8p80VHX4oB56xrqE7mAvyArGgfM18_lSXRRgQ/_9jZYvkMYMGOsykN6AlnWrE4VLLRcgITDBJiyCGI1Jk'), ('width', 232), ('height', 36)])), ('large', OrderedDict([('url', 'https://v5.airtableusercontent.com/v1/18/18/1689537600000/zIACzGEswy_VqAPI41RE0w/p9fZRVc7zXxmA-O88-PkMCrGiAgfV6ZhiPuJIdJges4pm8vK0kRS4RVlbvX3ISNKHhkpjNxGWTHFMS-Z1i6ltw/Q8t43BvydtWeEdRBXsOXk-0v2Fk3iEIno3tA1RqZfYA'), ('width', 2560), ('height', 398)])), ('full', OrderedDict([('url', 'https://v5.airtableusercontent.com/v1/18/18/1689537600000/g4og31dOk-TKjerxqVIoQg/QGIUp1sDPwfwfv7ZVi9tuQgRcWx7g3ITLMgEtKB1jNCQNc21T5YWxgXMcBFvV1ABa0s_2hn56K_GXiQvwAKfzg/1Q3db6fhAbr_EEtoThHWZoCVFkSliLdsG_PPrvp8Zeo'), ('width', 3000), ('height', 3000)]))]))])]",https://www.thorlabs.com/,Write a Python script that uses Instrumental to connect to a Thorlabs - TDC001 Motion,https://en.wikipedia.org/wiki/Motion_control,['Motion'],"Motion controller calculates and controls the mechanical trajectories (motion profile) an actuator must follow (i.e., motion planning) and, in closed loop systems, employs feedback to make control corrections and thus implement closed-loop control.",Thorlabs - TDC001,https://www.thorlabs.com/_sd.cfm?fileName=15797-D01.pdf&partNumber=TDC001,"[OrderedDict([('id', 'attowkxTzWAFmbDft'), ('width', 200), ('height', 200), ('url', 'https://v5.airtableusercontent.com/v1/18/18/1689537600000/ZDpbiB1Niu2oCMUXSJauLg/UTyUNokBqQySLHisV-EY-QzBCF6l9XYu4GHFKHO1534jaWrzmEM_1YE5dlXO9jm_XLMZuoJxpOtRuQm-hjejErw0cuVzMa2Lu31VbPS6NZo/jxY5XEFffHo6-lSId_cr0izzDIYtTB-RCu6pBftjy6U'), ('filename', '15797-std.jpg'), ('size', 6517), ('type', 'image/jpeg'), ('thumbnails', OrderedDict([('small', OrderedDict([('url', 'https://v5.airtableusercontent.com/v1/18/18/1689537600000/WD-p_pJ4ZzjvSzWU6tI02w/oNSh9_yyk0GvJdOgue5rr72O2woMts0IBma73s6VkE-wjEbBF1JkMt6aS9eMotIaeGqRJzrGQOtjn8EykWSQ9A/Rrkq7qPscEbbuvMy7xk_aEQsTKuxiaSRYuvxK465AeA'), ('width', 36), ('height', 36)])), ('large', OrderedDict([('url', 'https://v5.airtableusercontent.com/v1/18/18/1689537600000/DFsGU_X1-BlQLIvB1-Syig/88MJ_q1tQo--k0nwjQVrys3s7Ow4BWXhAIUDALn3X6SJrHq5NGN3LSfOrUdCPKr3IZse6MRGJhe0yz0-Tjsqrg/PIE4wUclak1coh7mJIkZBttCTnFILxlU0pANxdnWgMo'), ('width', 200), ('height', 200)])), ('full', OrderedDict([('url', 'https://v5.airtableusercontent.com/v1/18/18/1689537600000/hVPV0MZWTfz-G2hZlQbUxg/9mrN5z-mVhEmlKc3xx38LV68kCCYQJ7QTpIc8cozQe2hlNc_lS0acDpGa0nv2dRURXW6N9DgHJ8EJseBCbI7PA/m05BDiNv77wRTBY8g7DO4UWqTgsQlcV9X7D1Pyxxrp8'), ('width', 3000), ('height', 3000)]))]))])]",https://www.thorlabs.com/thorproduct.cfm?partnumber=TDC001,https://gitlab.com/ptapping/thorlabs-apt-device/-/blob/main/thorlabs_apt_device/devices/tdc001.py,https://thorlabs-apt-device.readthedocs.io/en/latest/api/thorlabs_apt_device.html,,,True,,
